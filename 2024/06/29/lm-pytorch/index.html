<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="keywords" content="Hexo Theme Redefine">
    
    <meta name="author" content="YIWEI WU">
    <!-- preconnect -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>

    
    <!--- Seo Part-->
    
    <link rel="canonical" href="https://sonder730.github.io/2024/06/29/lm-pytorch/"/>
    <meta name="robots" content="index,follow">
    <meta name="googlebot" content="index,follow">
    <meta name="revisit-after" content="1 days">
    
        <meta name="description" content="线性神经网络向量链式法则 标量链式法则    拓展到向量 需要注意维数的变化 下图三种情况分别对应：  y为标量，x为向量 y为标量，x为矩阵 y、x为矩阵       例1（标量对向量求导） 这里应该是用分子布局，所以是X转置    例2（涉及到矩阵的情况） X是mxn的矩阵,w为n维向量，y为m维向量； z对Xw-y做L2 norm,为标量； 过程与例一大体一致；      由于在神经网络动辄">
<meta property="og:type" content="article">
<meta property="og:title" content="lm_pytorch">
<meta property="og:url" content="https://sonder730.github.io/2024/06/29/lm-pytorch/index.html">
<meta property="og:site_name" content="RUN FOR WHERE">
<meta property="og:description" content="线性神经网络向量链式法则 标量链式法则    拓展到向量 需要注意维数的变化 下图三种情况分别对应：  y为标量，x为向量 y为标量，x为矩阵 y、x为矩阵       例1（标量对向量求导） 这里应该是用分子布局，所以是X转置    例2（涉及到矩阵的情况） X是mxn的矩阵,w为n维向量，y为m维向量； z对Xw-y做L2 norm,为标量； 过程与例一大体一致；      由于在神经网络动辄">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/07/image-01">
<meta property="og:image" content="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/07/image-02">
<meta property="og:image" content="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/07/image-03">
<meta property="og:image" content="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/07/image-04">
<meta property="og:image" content="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/07/image-05">
<meta property="og:image" content="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/07/image-06">
<meta property="og:image" content="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/07/image-07">
<meta property="og:image" content="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/07/image-08">
<meta property="og:image" content="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/07/image-09">
<meta property="og:image" content="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/07/image-10">
<meta property="og:image" content="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/07/image-11">
<meta property="og:image" content="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/07/image-12">
<meta property="og:image" content="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/07/image-13">
<meta property="og:image" content="c:/Users/Y9000p/AppData/Roaming/Typora/typora-user-images/image-20240701223140819.png">
<meta property="og:image" content="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/09/09-01.png">
<meta property="og:image" content="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/09/09-02.png">
<meta property="og:image" content="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/09/09-03.png">
<meta property="og:image" content="c:/Users/Y9000p/AppData/Roaming/Typora/typora-user-images/image-20240702004009211.png">
<meta property="og:image" content="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/09/09-04.png">
<meta property="og:image" content="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/09/09-05.png">
<meta property="og:image" content="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/09/09-06.png">
<meta property="og:image" content="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/09/09-07.png">
<meta property="og:image" content="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/09/09-08.png">
<meta property="og:image" content="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/09/09-09.png">
<meta property="og:image" content="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/10/%E6%84%9F%E7%9F%A5%E6%9C%BA.png">
<meta property="og:image" content="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/10/%E7%BA%BF%E6%80%A7%E5%88%92%E5%88%86.png">
<meta property="og:image" content="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/10/XOR%E9%97%AE%E9%A2%98.png">
<meta property="og:image" content="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/10/%E5%A4%9A%E5%B1%82%E5%88%86%E7%B1%BBXOR1.png">
<meta property="og:image" content="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/10/%E5%A4%9A%E5%B1%82%E5%88%86%E7%B1%BBXOR2.png">
<meta property="og:image" content="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/10/XOR%E4%BF%A1%E6%81%AF%E5%A4%9A%E5%B1%82%E6%AC%A1%E4%BC%A0%E9%80%92.png">
<meta property="og:image" content="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/10/%E5%8D%95%E9%9A%90%E8%97%8F%E5%B1%82.png">
<meta property="og:image" content="c:/Users/Y9000p/AppData/Roaming/Typora/typora-user-images/image-20240702223819681.png">
<meta property="og:image" content="c:/Users/Y9000p/AppData/Roaming/Typora/typora-user-images/image-20240702224134621.png">
<meta property="og:image" content="c:/Users/Y9000p/AppData/Roaming/Typora/typora-user-images/image-20240702225202168.png">
<meta property="og:image" content="https://camo.githubusercontent.com/c268c1ccd2378c3872e8678310498cca00c9fd8f26bfdb0ce3cf4e983d7a1d56/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e6c617465783f6d696e2535437370616365253543656c6c28253543626f6c64253742772537442c6229">
<meta property="og:image" content="https://camo.githubusercontent.com/ff9b452cab8da34e38960130e11bdb43c17681b4d715f35495b631be82f83c1d/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e6c617465783f253543626f6c6425374277253744">
<meta property="og:image" content="https://camo.githubusercontent.com/60135f218d6eff45f9d2a2b226b09f667b1e5862c12b77709fe9fe000ce42fdf/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e6c617465783f253743253743253543626f6c6425374277253744253743253743253545322535436c6571736c616e742535437468657461">
<meta property="og:image" content="https://camo.githubusercontent.com/2ee9f22d00819650d1da57b8e92428653b1baaeb0a5efbd4f95ee745612fc9a7/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e6c617465783f2535437468657461">
<meta property="og:image" content="https://camo.githubusercontent.com/2ee9f22d00819650d1da57b8e92428653b1baaeb0a5efbd4f95ee745612fc9a7/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e6c617465783f2535437468657461">
<meta property="og:image" content="https://camo.githubusercontent.com/ff9b452cab8da34e38960130e11bdb43c17681b4d715f35495b631be82f83c1d/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e6c617465783f253543626f6c6425374277253744">
<meta property="og:image" content="https://camo.githubusercontent.com/2ee9f22d00819650d1da57b8e92428653b1baaeb0a5efbd4f95ee745612fc9a7/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e6c617465783f2535437468657461">
<meta property="og:image" content="https://camo.githubusercontent.com/2ee9f22d00819650d1da57b8e92428653b1baaeb0a5efbd4f95ee745612fc9a7/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e6c617465783f2535437468657461">
<meta property="og:image" content="https://camo.githubusercontent.com/ff9b452cab8da34e38960130e11bdb43c17681b4d715f35495b631be82f83c1d/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e6c617465783f253543626f6c6425374277253744">
<meta property="og:image" content="https://camo.githubusercontent.com/aca2480d2e63e33df193ee69d1869f501c3cc3e4123bee06c3510ca892890098/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e6c617465783f6d696e2535437370616365253543656c6c28253543626f6c64253742772537442c62292b253543667261632537422535436c616d62646125374425374232253744253743253743253543626f6c642537427725374425374325374325354532">
<meta property="og:image" content="https://camo.githubusercontent.com/2ee9f22d00819650d1da57b8e92428653b1baaeb0a5efbd4f95ee745612fc9a7/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e6c617465783f2535437468657461">
<meta property="og:image" content="https://camo.githubusercontent.com/2ec5723e26bb86479707648ec0b1b1f9183674b99e31bcce5cbe2835d1e7f9e1/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e6c617465783f2535436c616d626461">
<meta property="og:image" content="https://camo.githubusercontent.com/db89c02a9c53b868fba4f1c0c9e4e7d986adeb498d5324dcb072189d24ad3904/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e6c617465783f253543667261632537422535436c616d62646125374425374232253744253743253743253543626f6c642537427725374425374325374325354532">
<meta property="og:image" content="https://camo.githubusercontent.com/2ec5723e26bb86479707648ec0b1b1f9183674b99e31bcce5cbe2835d1e7f9e1/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e6c617465783f2535436c616d626461">
<meta property="og:image" content="https://camo.githubusercontent.com/bc056ceedf61585d3b6a3f773ef4edf48c1311f60f335d60d30ba6060270c40c/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e6c617465783f2535436c616d6264613d30">
<meta property="og:image" content="https://camo.githubusercontent.com/61cf8e5887264e92d223f8e3044131588ab79b6276c3c73935362d73f6fa9cc3/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e6c617465783f2535436c616d62646125354372696768746172726f77253543696e667479">
<meta property="og:image" content="https://camo.githubusercontent.com/5deec64969e5a58e2d4259232575940bfbbdf093a0472cea94e2a5305d3739ba/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e6c617465783f253543626f6c64253742772537442535452a25354372696768746172726f7730">
<meta property="og:image" content="https://camo.githubusercontent.com/2ec5723e26bb86479707648ec0b1b1f9183674b99e31bcce5cbe2835d1e7f9e1/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e6c617465783f2535436c616d626461">
<meta property="og:image" content="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/12/12-01.JPG">
<meta property="og:image" content="https://camo.githubusercontent.com/ff9b452cab8da34e38960130e11bdb43c17681b4d715f35495b631be82f83c1d/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e6c617465783f253543626f6c6425374277253744">
<meta property="og:image" content="https://camo.githubusercontent.com/816e5c55c47c5414a4ddafeae7feaccbd6446371d3719dcd4bb12b53196747da/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e6c617465783f253543667261632537422535437061727469616c2537422537442537442537422535437061727469616c253742253543626f6c642537427725374425374425374428253543656c6c28253543626f6c64253742772537442c62292b253543667261632537422535436c616d62646125374425374232253744253743253743253543626f6c642537427725374425374325374325354532293d253543667261632537422535437061727469616c253742253543656c6c28253543626f6c64253742772537442c62292537442537442537422537422535437061727469616c253742253543626f6c64253742772537442537442537442537442b2535436c616d626461253543626f6c6425374277253744">
<meta property="og:image" content="https://camo.githubusercontent.com/221e250a978f50f162533c2a1f5bc34e70bfd1751255e95b1c11eaf4bfdec1a1/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e6c617465783f253543626f6c64253742772537445f253742742b312537443d28312d2535436574612535436c616d62646129253543626f6c64253742772537445f253742742537442d253543657461253543667261632537422535437061727469616c253742253543656c6c28253543626f6c64253742772537445f742c625f74292537442537442537422537422535437061727469616c253742253543626f6c64253742772537445f25374274253744253744253744253744">
<meta property="og:image" content="https://camo.githubusercontent.com/779be9c1d70105fed3cfa9549c24c591ab988cf098abfce4af42c0d069c64c18/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e6c617465783f253543626f6c64253742772537445f25374274253744">
<meta property="og:image" content="https://camo.githubusercontent.com/36e26287fd68d846aa72c6befe07fbaf4130e11a2a7a5a45c8eb4fe552993054/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e6c617465783f28312d2535436574612535436c616d62646129">
<meta property="og:image" content="https://camo.githubusercontent.com/4c3bff3b6dcd9ffc7a7122b114f60f2f3687b910cebc2f00f751b30a4071f11d/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e6c617465783f2535436574612535436c616d62646125334331">
<meta property="og:image" content="https://camo.githubusercontent.com/2ec5723e26bb86479707648ec0b1b1f9183674b99e31bcce5cbe2835d1e7f9e1/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e6c617465783f2535436c616d626461">
<meta property="og:image" content="https://camo.githubusercontent.com/2ec5723e26bb86479707648ec0b1b1f9183674b99e31bcce5cbe2835d1e7f9e1/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e6c617465783f2535436c616d626461">
<meta property="og:image" content="c:/Users/Y9000p/AppData/Roaming/Typora/typora-user-images/image-20240703012706882.png">
<meta property="og:image" content="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/13/13-02.jpg">
<meta property="og:image" content="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/13/13-03.jpg">
<meta property="og:image" content="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/13/13-01.png">
<meta property="og:image" content="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/13/13-04.png">
<meta property="og:image" content="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/13/13-05.png">
<meta property="og:image" content="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/13/13-06.png">
<meta property="og:image" content="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/13/13-07.png">
<meta property="og:image" content="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/14/14-01.png">
<meta property="og:image" content="https://camo.githubusercontent.com/a71cebb102ebd87361c6309af0486f84cf349a20dec37eb3f2cae5bc0b90b22a/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e696d6167653f685e7b742d317d">
<meta property="og:image" content="https://camo.githubusercontent.com/ea7eda19f58ecfb0aa08ee4f8aa706c973834f1d814cfd6cc862932921b2ea4b/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e696d6167653f665f7b747d">
<meta property="og:image" content="https://camo.githubusercontent.com/cd456da509cd823ff05c8a8147686bebe7f70b0479d17469c343f39a9cc2a8d7/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e696d6167653f575f7b747d2673706163653b">
<meta property="og:image" content="https://camo.githubusercontent.com/cb03a67eca0aab939a881ca6e61c07bfa0c7a014080a832c6ef22dba43c45436/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e696d6167653f345c74696d65732673706163653b31305e7b31377d">
<meta property="og:image" content="https://camo.githubusercontent.com/a231a5a2891622e8dc7f0d3a45b8e4c5fc159338ba446402166557ea948d16e4/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e696d6167653f325c74696d657331305e7b2d31307d">
<meta property="og:image" content="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/14/14-02.png">
<meta property="og:image" content="https://camo.githubusercontent.com/be807542e1881c650372a6b6555cb672468be104bb488370e96db8ac22417766/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e696d6167653f685e7b747d">
<meta property="og:image" content="https://camo.githubusercontent.com/a71cebb102ebd87361c6309af0486f84cf349a20dec37eb3f2cae5bc0b90b22a/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e696d6167653f685e7b742d317d">
<meta property="og:image" content="https://camo.githubusercontent.com/ea7eda19f58ecfb0aa08ee4f8aa706c973834f1d814cfd6cc862932921b2ea4b/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e696d6167653f665f7b747d">
<meta property="og:image" content="https://camo.githubusercontent.com/a71cebb102ebd87361c6309af0486f84cf349a20dec37eb3f2cae5bc0b90b22a/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e696d6167653f685e7b742d317d">
<meta property="og:image" content="https://camo.githubusercontent.com/ebf39e7aad643a5167823309290a6aafec495508d69747797eb052e48131fded/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e696d6167653f5c7369676d612673706163653b">
<meta property="og:image" content="https://camo.githubusercontent.com/be807542e1881c650372a6b6555cb672468be104bb488370e96db8ac22417766/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e696d6167653f685e7b747d">
<meta property="og:image" content="https://camo.githubusercontent.com/ebf39e7aad643a5167823309290a6aafec495508d69747797eb052e48131fded/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e696d6167653f5c7369676d612673706163653b">
<meta property="og:image" content="https://camo.githubusercontent.com/ebf39e7aad643a5167823309290a6aafec495508d69747797eb052e48131fded/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e696d6167653f5c7369676d612673706163653b">
<meta property="og:image" content="https://camo.githubusercontent.com/2e6701f04d89ef9b5335964cbec64476b9ddadb5b6ca6bc64d73b2b8009f06c9/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e696d6167653f575f7b747d685f7b742d317d">
<meta property="og:image" content="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/14/14-03.png">
<meta property="og:image" content="https://camo.githubusercontent.com/71fa9f16c839f40f6fe979e6f72bc88ddf24c3d304e576cf6463c3ad09e6a010/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e696d6167653f5c667261637b5c7061727469616c2673706163653b575e7b747d685e7b742d317d7d7b5c7061727469616c2673706163653b685e7b742d317d7d3d2673706163653b575e7b747d">
<meta property="og:image" content="https://camo.githubusercontent.com/83dc04c534c36c75a02d9516553d3555aef6df231859509223a9953a04942948/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e696d6167653f5c6c6566742673706163653b28575e7b747d2673706163653b2673706163653b5c72696768742673706163653b295e7b547d">
<meta property="og:image" content="https://camo.githubusercontent.com/90fa60edb85fc018d18ff5c3e21e28f3c075a514b1c31cba204dba5995760657/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e696d6167653f575e7b747d">
<meta property="og:image" content="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/14/14-04.png">
<meta property="og:image" content="https://camo.githubusercontent.com/b672ed6c4894aead93fb0506d22544a527353e71b36e572f7dd042825883dbb3/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e696d6167653f575e7b697d">
<meta property="og:image" content="https://camo.githubusercontent.com/4eb293144f80b51159f7c69a62a047aad97e5d9bbe61e83f4fe6234323f63360/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e696d6167653f5c70726f645c6c6566742673706163653b282673706163653b575e7b697d2673706163653b5c72696768742673706163653b292673706163653b">
<meta property="og:image" content="https://camo.githubusercontent.com/b672ed6c4894aead93fb0506d22544a527353e71b36e572f7dd042825883dbb3/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e696d6167653f575e7b697d">
<meta property="og:image" content="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/14/14-05.png">
<meta property="og:image" content="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/14/14-06.png">
<meta property="og:image" content="https://camo.githubusercontent.com/ed117f76d7cda611a8d81fcae67f6bf383bce937de9b079c5ccaaf30bdb6c5b8/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e696d6167653f5c70726f642673706163653b646961675c6c6566742673706163653b282673706163653b5c7369676d612673706163653b5e7b277d5c6c6566742673706163653b282673706163653b575e7b697d685e7b692d317d2673706163653b5c72696768742673706163653b292673706163653b5c72696768742673706163653b292673706163653b">
<meta property="og:image" content="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/14/14-07.png">
<meta property="og:image" content="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/14/14-08.png">
<meta property="og:image" content="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/14/14-09.png">
<meta property="og:image" content="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/14/14-10.png">
<meta property="og:image" content="https://camo.githubusercontent.com/f78bf82d006b08b156283f8f36616c89d1f31532215546b92819e2cc772c773a/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e696d6167653f2673706163653b6e5f7b742d317d5c67616d6d612673706163653b5f7b747d3d312673706163653b">
<meta property="og:image" content="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/14/14-11.png">
<meta property="og:image" content="https://camo.githubusercontent.com/9b5cf0aa698dc23314756d854eac930e1469588685dffa3b9fbefeec54b963a6/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e696d6167653f2673706163653b6e5f7b747d5c67616d6d612673706163653b5f7b747d3d312673706163653b">
<meta property="og:image" content="https://camo.githubusercontent.com/f78bf82d006b08b156283f8f36616c89d1f31532215546b92819e2cc772c773a/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e696d6167653f2673706163653b6e5f7b742d317d5c67616d6d612673706163653b5f7b747d3d312673706163653b">
<meta property="og:image" content="https://camo.githubusercontent.com/9b5cf0aa698dc23314756d854eac930e1469588685dffa3b9fbefeec54b963a6/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e696d6167653f2673706163653b6e5f7b747d5c67616d6d612673706163653b5f7b747d3d312673706163653b">
<meta property="og:image" content="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/14/14-12.png">
<meta property="og:image" content="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/14/14-13.png">
<meta property="og:image" content="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/14/14-14.png">
<meta property="og:image" content="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/14/14-15.png">
<meta property="og:image" content="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/14/14-16.png">
<meta property="og:image" content="c:/Users/Y9000p/AppData/Roaming/Typora/typora-user-images/image-20240703193142034.png">
<meta property="og:image" content="c:/Users/Y9000p/AppData/Roaming/Typora/typora-user-images/image-20240703195033844.png">
<meta property="og:image" content="c:/Users/Y9000p/AppData/Roaming/Typora/typora-user-images/image-20240704152743711.png">
<meta property="og:image" content="c:/Users/Y9000p/AppData/Roaming/Typora/typora-user-images/image-20240704182222900.png">
<meta property="og:image" content="c:/Users/Y9000p/AppData/Roaming/Typora/typora-user-images/image-20240704183058348.png">
<meta property="og:image" content="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/20/20-01.png">
<meta property="og:image" content="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/20/20-02.png">
<meta property="og:image" content="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/20/20-03.png">
<meta property="og:image" content="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/20/20-04.png">
<meta property="og:image" content="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/20/20-05.png">
<meta property="og:image" content="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/21/21-01.png">
<meta property="og:image" content="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/21/21-02.png">
<meta property="og:image" content="https://camo.githubusercontent.com/ede4ec360206a8830f9e4238a35c436d5881462fce077e695ae3bfd683ccbe56/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e696d6167653f635f7b697d5c74696d65732673706163653b6e5f7b687d5c74696d65732673706163653b6e5f7b777d">
<meta property="og:image" content="https://camo.githubusercontent.com/e4217dc5acd1775e3830925e71415d8059621f1b5616a2ae8e53d9264303f872/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e696d6167653f635f7b697d5c74696d65732673706163653b6b5f7b687d5c74696d65732673706163653b6b5f7b777d">
<meta property="og:image" content="https://camo.githubusercontent.com/0704054ba0d6b123fe7d3508371099665253aecec2c758f3cba4005d24305367/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e696d6167653f6d5f7b687d5c74696d65732673706163653b6d5f7b777d">
<meta property="og:image" content="https://camo.githubusercontent.com/11eddda729f3b5b08fb8c797e2fe70e66dcd6a4758c676ad943dc41a773fc910/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e696d6167653f593d5c73756d2673706163653b5f7b693d307d5e7b635f7b697d7d585f7b692c3a2c3a7d5c626967737461722673706163653b575f7b692c3a2c3a7d">
<meta property="og:image" content="https://camo.githubusercontent.com/e4217dc5acd1775e3830925e71415d8059621f1b5616a2ae8e53d9264303f872/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e696d6167653f635f7b697d5c74696d65732673706163653b6b5f7b687d5c74696d65732673706163653b6b5f7b777d">
<meta property="og:image" content="https://camo.githubusercontent.com/db922e672cf0c939ff058271b413d0b0c81b0da02a111bf6714bea10c20ea491/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e696d6167653f635f7b6f7d5c74696d65732673706163653b635f7b697d5c74696d65732673706163653b6b5f7b687d5c74696d65732673706163653b6b5f7b777d">
<meta property="og:image" content="https://camo.githubusercontent.com/d7f7c4abac6c1ae3b751fc15b6f42d2592d0ebe0146a2c088f305eade884b6d2/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e696d6167653f635f7b6f7d5c74696d65732673706163653b6d5f7b687d5c74696d65732673706163653b6d5f7b777d">
<meta property="og:image" content="https://camo.githubusercontent.com/ccb530df0e5be9267635cc5817021485ff677fd9b6be34b18bd5a249489a7da1/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e696d6167653f595f7b692c3a2c3a7d3d585c626967737461722673706163653b575f7b692c3a2c3a7d5c71717561642673706163653b666f722673706163653b5c717561642673706163653b693d312c2e2e2e2c635f7b6f7d">
<meta property="og:image" content="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/21/21-03.png">
<meta property="og:image" content="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/21/21-04.png">
<meta property="og:image" content="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/21/21-05.png">
<meta property="og:image" content="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/23/23-01.png">
<meta property="og:image" content="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/23/23-02.png">
<meta property="og:image" content="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/23/23-03.png">
<meta property="og:image" content="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/24/24-01.png">
<meta property="og:image" content="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/24/24-02.png">
<meta property="og:image" content="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/24/24-03.png">
<meta property="og:image" content="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/24/24-04.png">
<meta property="og:image" content="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/24/24-05.png">
<meta property="og:image" content="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/24/24-06.png">
<meta property="og:image" content="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/24/24-07.png">
<meta property="og:image" content="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/24/24-08.png">
<meta property="og:image" content="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/25/25-01.PNG">
<meta property="og:image" content="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/25/25-02.PNG">
<meta property="og:image" content="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/26/26-01.png">
<meta property="og:image" content="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/26/26-02.png">
<meta property="og:image" content="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/26/26-03.png">
<meta property="og:image" content="https://github.com/kinza99/DeepLearning-MuLi-Notes/raw/main/imgs/27/27-1.png">
<meta property="og:image" content="https://github.com/kinza99/DeepLearning-MuLi-Notes/raw/main/imgs/27/27-2.png">
<meta property="article:published_time" content="2024-06-29T13:14:37.000Z">
<meta property="article:modified_time" content="2024-07-05T09:53:13.267Z">
<meta property="article:author" content="YIWEI WU">
<meta property="article:tag" content="DEEP LEARNING">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/07/image-01">
    
    
    <!--- Icon Part-->
    <link rel="icon" type="image/png" href="/images/favicon.png" sizes="192x192">
    <link rel="apple-touch-icon" sizes="180x180" href="/images/favicon.png">
    <meta name="theme-color" content="#A31F34">
    <link rel="shortcut icon" href="/images/favicon.png">
    <!--- Page Info-->
    
    <title>
        
            lm_pytorch -
        
        RUN FOR WHERE
    </title>

    
<link rel="stylesheet" href="/fonts/Chillax/chillax.css">


    <!--- Inject Part-->
    

    
<link rel="stylesheet" href="/css/style.css">


    
        
<link rel="stylesheet" href="/assets/build/styles.css">

    

    
<link rel="stylesheet" href="/fonts/GeistMono/geist-mono.css">

    
<link rel="stylesheet" href="/fonts/Geist/geist.css">

    <!--- Font Part-->
    
    
    
    

    
        
<script src="/js/libs/anime.min.js"></script>

    

    <script id="hexo-configurations">
    window.config = {"hostname":"sonder730.github.io","root":"/","language":"en"};
    window.theme = {"articles":{"style":{"font_size":"16px","line_height":1.5,"image_border_radius":"14px","image_alignment":"center","image_caption":false,"link_icon":true,"title_alignment":"left","headings_top_spacing":{"h1":"3.2rem","h2":"2.4rem","h3":"1.9rem","h4":"1.6rem","h5":"1.4rem","h6":"1.3rem"}},"word_count":{"enable":true,"count":true,"min2read":true},"author_label":{"enable":true,"auto":false,"list":[]},"code_block":{"copy":true,"style":"mac","font":{"enable":false,"family":null,"url":null}},"toc":{"enable":true,"max_depth":3,"number":true,"expand":true,"init_open":true},"copyright":{"enable":true,"default":"cc_by_nc_sa"},"lazyload":true,"recommendation":{"enable":false,"title":"推荐阅读","limit":3,"mobile_limit":2,"placeholder":"/images/wallhaven-wqery6-light.webp","skip_dirs":[]}},"colors":{"primary":"#A31F34","secondary":null,"default_mode":"light"},"global":{"fonts":{"chinese":{"enable":false,"family":null,"url":null},"english":{"enable":false,"family":null,"url":null}},"content_max_width":"1000px","sidebar_width":"210px","hover":{"shadow":true,"scale":false},"scroll_progress":{"bar":false,"percentage":true},"website_counter":{"url":"https://cn.vercount.one/js","enable":true,"site_pv":true,"site_uv":true,"post_pv":true},"single_page":true,"preloader":true,"open_graph":true,"google_analytics":{"enable":false,"id":null}},"home_banner":{"enable":true,"style":"fixed","image":{"light":"/images/wallhaven-wqery6-light.webp","dark":"/images/wallhaven-wqery6-dark.webp"},"title":"RUN FOR WHERE","subtitle":{"text":["Gloria!Euphoria!"],"hitokoto":{"enable":true,"api":"https://v1.hitokoto.cn"},"typing_speed":100,"backing_speed":80,"starting_delay":500,"backing_delay":1500,"loop":true,"smart_backspace":true},"text_color":{"light":"#fff","dark":"#d1d1b6"},"text_style":{"title_size":"2.8rem","subtitle_size":"1.5rem","line_height":1.2},"custom_font":{"enable":false,"family":null,"url":null},"social_links":{"enable":true,"style":"default","links":{"github":"https://github.com/SONDER730","instagram":null,"zhihu":null,"twitter":null,"email":null},"qrs":{"weixin":null}}},"plugins":{"feed":{"enable":false},"aplayer":{"enable":false,"type":"fixed","audios":[{"name":null,"artist":null,"url":null,"cover":null,"lrc":null}]},"mermaid":{"enable":false,"version":"9.3.0"}},"version":"2.6.4","navbar":{"auto_hide":false,"color":{"left":"#f78736","right":"#367df7","transparency":35},"width":{"home":"1200px","pages":"1000px"},"links":{"Home":{"path":"/","icon":"fa-regular fa-house"},"Archives":{"path":"/archives","icon":"fa-regular fa-archive"},"About":{"icon":"fa-regular fa-user","submenus":{"Github":"https://github.com/SONDER730"}}},"search":{"enable":false,"preload":true}},"page_templates":{"friends_column":2,"tags_style":"blur","categories_style":"list"},"home":{"sidebar":{"enable":true,"position":"left","first_item":"menu","announcement":null,"show_on_mobile":true,"links":{"Archives":{"path":"/archives","icon":"fa-regular fa-archive"},"Tags":{"path":"/tags","icon":"fa-regular fa-tags"},"Categories":{"path":"/categories","icon":"fa-regular fa-folder"}}},"article_date_format":"auto","categories":{"enable":true,"limit":3},"tags":{"enable":true,"limit":3}},"footerStart":"2024/6/23 18:06:14"};
    window.lang_ago = {"second":"%s seconds ago","minute":"%s minutes ago","hour":"%s hours ago","day":"%s days ago","week":"%s weeks ago","month":"%s months ago","year":"%s years ago"};
    window.data = {"masonry":false};
  </script>
    
    <!--- Fontawesome Part-->
    
<link rel="stylesheet" href="/fontawesome/fontawesome.min.css">

    
<link rel="stylesheet" href="/fontawesome/brands.min.css">

    
<link rel="stylesheet" href="/fontawesome/solid.min.css">

    
<link rel="stylesheet" href="/fontawesome/regular.min.css">

    
    
    
    
<meta name="generator" content="Hexo 7.2.0"></head>


<body>
<div class="progress-bar-container">
    

    
        <span class="pjax-progress-bar"></span>
<!--        <span class="swup-progress-icon">-->
<!--            <i class="fa-solid fa-circle-notch fa-spin"></i>-->
<!--        </span>-->
    
</div>



    <style>
    :root {
        --preloader-background-color: #fff;
        --preloader-text-color: #000;
    }

    @media (prefers-color-scheme: dark) {
        :root {
            --preloader-background-color: #202124;
            --preloader-text-color: #fff;
        }
    }

    @media (prefers-color-scheme: light) {
        :root {
            --preloader-background-color: #fff;
            --preloader-text-color: #000;
        }
    }

    @media (max-width: 600px) {
        .ml13 {
            font-size: 2.6rem !important; /* Adjust this value as needed */
        }
    }

    .preloader {
        display: flex;
        flex-direction: column;
        gap: 1rem; /* Tailwind 'gap-4' is 1rem */
        align-items: center;
        justify-content: center;
        position: fixed;
        padding: 12px;
        top: 0;
        right: 0;
        bottom: 0;
        left: 0;
        width: 100vw;
        height: 100vh; /* 'h-screen' is 100% of the viewport height */
        background-color: var(--preloader-background-color);
        z-index: 1100; /* 'z-[1100]' sets the z-index */
        transition: opacity 0.2s ease-in-out;
    }

    .ml13 {
        font-size: 3.2rem;
        /* text-transform: uppercase; */
        color: var(--preloader-text-color);
        letter-spacing: -1px;
        font-weight: 500;
        font-family: 'Chillax-Variable', sans-serif;
        text-align: center;
    }

    .ml13 .word {
        display: inline-flex;
        flex-wrap: wrap;
        white-space: nowrap;
    }

    .ml13 .letter {
        display: inline-block;
        line-height: 1em;
    }
</style>

<div class="preloader">
    <h2 class="ml13">
        RUN FOR WHERE
    </h2>
    <script>
        var textWrapper = document.querySelector('.ml13');
        // Split text into words
        var words = textWrapper.textContent.trim().split(' ');

        // Clear the existing content
        textWrapper.innerHTML = '';

        // Wrap each word and its letters in spans
        words.forEach(function(word) {
            var wordSpan = document.createElement('span');
            wordSpan.classList.add('word');
            wordSpan.innerHTML = word.replace(/\S/g, "<span class='letter'>$&</span>");
            textWrapper.appendChild(wordSpan);
            textWrapper.appendChild(document.createTextNode(' ')); // Add space between words
        });

        var animation = anime.timeline({loop: true})
            .add({
                targets: '.ml13 .letter',
                translateY: [40,0],
                translateZ: 0,
                opacity: [0,1],
                filter: ['blur(5px)', 'blur(0px)'], // Starting from blurred to unblurred
                easing: "easeOutExpo",
                duration: 1400,
                delay: (el, i) => 300 + 30 * i,
            }).add({
                targets: '.ml13 .letter',
                translateY: [0,-40],
                opacity: [1,0],
                filter: ['blur(0px)', 'blur(5px)'], // Ending from unblurred to blurred
                easing: "easeInExpo",
                duration: 1200,
                delay: (el, i) => 100 + 30 * i,
                complete: function() {
                    hidePreloader(); // Call hidePreloader after the animation completes
                }
            });

        let themeStatus = JSON.parse(localStorage.getItem('REDEFINE-THEME-STATUS'))?.isDark;

        // If the theme status is not found in local storage, check the preferred color scheme
        if (themeStatus === undefined || themeStatus === null) {
            if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches) {
                themeStatus = 'dark';
            } else {
                themeStatus = 'light';
            }
        }

        // Now you can use the themeStatus variable in your code
        if (themeStatus) {
            document.documentElement.style.setProperty('--preloader-background-color', '#202124');
            document.documentElement.style.setProperty('--preloader-text-color', '#fff');
        } else {
            document.documentElement.style.setProperty('--preloader-background-color', '#fff');
            document.documentElement.style.setProperty('--preloader-text-color', '#000');
        }

        window.addEventListener('load', function () {
            setTimeout(hidePreloader, 5000); // Call hidePreloader after 5000 milliseconds if not already called by animation
        });

        function hidePreloader() {
            var preloader = document.querySelector('.preloader');
            preloader.style.opacity = '0';
            setTimeout(function () {
                preloader.style.display = 'none';
            }, 200);
        }
    </script>
</div>

<main class="page-container" id="swup">

    

    <div class="main-content-container">


        <div class="main-content-header">
            <header class="navbar-container px-6 md:px-12">

    <div class="navbar-content ">
        <div class="left">
            
                <a class="logo-image" href="/">
                    <img src="/images/favicon.png">
                </a>
            
            <a class="logo-title" href="/">
                
                RUN FOR WHERE
                
            </a>
        </div>

        <div class="right">
            <!-- PC -->
            <div class="desktop">
                <ul class="navbar-list">
                    
                        
                            

                            <li class="navbar-item">
                                <!-- Menu -->
                                <a class=""
                                   href="/"
                                        >
                                    <i class="fa-regular fa-house fa-fw"></i>
                                    HOME
                                    
                                </a>

                                <!-- Submenu -->
                                
                            </li>
                    
                        
                            

                            <li class="navbar-item">
                                <!-- Menu -->
                                <a class=""
                                   href="/archives"
                                        >
                                    <i class="fa-regular fa-archive fa-fw"></i>
                                    ARCHIVES
                                    
                                </a>

                                <!-- Submenu -->
                                
                            </li>
                    
                        
                            

                            <li class="navbar-item">
                                <!-- Menu -->
                                <a class="has-dropdown"
                                   href="#"
                                        onClick=&#34;return false;&#34;>
                                    <i class="fa-regular fa-user fa-fw"></i>
                                    ABOUT
                                    <i class="fa-solid fa-chevron-down fa-fw"></i>
                                </a>

                                <!-- Submenu -->
                                
                                    <ul class="sub-menu">
                                        
                                            <li>
                                                <a target="_blank" rel="noopener" href="https://github.com/SONDER730">
                                                    GITHUB
                                                </a>
                                            </li>
                                        
                                    </ul>
                                
                            </li>
                    
                    
                </ul>
            </div>
            <!-- Mobile -->
            <div class="mobile">
                
                <div class="icon-item navbar-bar">
                    <div class="navbar-bar-middle"></div>
                </div>
            </div>
        </div>
    </div>

    <!-- Mobile sheet -->
    <div class="navbar-drawer h-screen w-full absolute top-0 left-0 bg-background-color flex flex-col justify-between">
        <ul class="drawer-navbar-list flex flex-col px-4 justify-center items-start">
            
                
                    

                    <li class="drawer-navbar-item text-base my-1.5 flex flex-col w-full">
                        
                        <a class="py-1.5 px-2 flex flex-row items-center justify-between gap-1 hover:!text-primary active:!text-primary text-2xl font-semibold group border-b border-border-color hover:border-primary w-full "
                           href="/"
                        >
                            <span>
                                HOME
                            </span>
                            
                                <i class="fa-regular fa-house fa-sm fa-fw"></i>
                            
                        </a>
                        

                        
                    </li>
            
                
                    

                    <li class="drawer-navbar-item text-base my-1.5 flex flex-col w-full">
                        
                        <a class="py-1.5 px-2 flex flex-row items-center justify-between gap-1 hover:!text-primary active:!text-primary text-2xl font-semibold group border-b border-border-color hover:border-primary w-full "
                           href="/archives"
                        >
                            <span>
                                ARCHIVES
                            </span>
                            
                                <i class="fa-regular fa-archive fa-sm fa-fw"></i>
                            
                        </a>
                        

                        
                    </li>
            
                
                    

                    <li class="drawer-navbar-item-sub text-base my-1.5 flex flex-col w-full">
                        
                        <div class="py-1.5 px-2 flex flex-row items-center justify-between gap-1 hover:!text-primary active:!text-primary cursor-pointer text-2xl font-semibold group border-b border-border-color hover:border-primary w-full "
                             navbar-data-toggle="submenu-About"
                        >
                            <span>
                                ABOUT
                            </span>
                            
                                <i class="fa-solid fa-chevron-right fa-sm fa-fw transition-all"></i>
                            
                        </div>
                        

                        
                            <div class="flex-col items-start px-2 py-2 hidden" data-target="submenu-About">
                                
                                    <div class="drawer-navbar-item text-base flex flex-col justify-center items-start hover:underline active:underline hover:underline-offset-1 rounded-3xl">
                                        <a class=" text-third-text-color text-xl"
                                           target="_blank" rel="noopener" href="https://github.com/SONDER730">GITHUB</a>
                                    </div>
                                
                            </div>
                        
                    </li>
            

            
            
                
                    
                    
                    <li class="drawer-navbar-item text-base my-1.5 flex flex-col w-full">
                        <a class="py-1.5 px-2 flex flex-row items-center justify-between gap-1 hover:!text-primary active:!text-primary text-2xl font-semibold group border-b border-border-color hover:border-primary w-full active"
                           href="/tags"
                        >
                            <span>Tags</span>
                            <i class="fa-regular fa-tags fa-sm fa-fw"></i>
                        </a>
                    </li>
                
                    
                    <li class="drawer-navbar-item text-base my-1.5 flex flex-col w-full">
                        <a class="py-1.5 px-2 flex flex-row items-center justify-between gap-1 hover:!text-primary active:!text-primary text-2xl font-semibold group border-b border-border-color hover:border-primary w-full active"
                           href="/categories"
                        >
                            <span>Categories</span>
                            <i class="fa-regular fa-folder fa-sm fa-fw"></i>
                        </a>
                    </li>
                
            
        </ul>

        <div class="statistics flex justify-around my-2.5">
    <a class="item tag-count-item flex flex-col justify-center items-center w-20" href="/tags">
        <div class="number text-2xl sm:text-xl text-second-text-color font-semibold">2</div>
        <div class="label text-third-text-color text-sm">Tags</div>
    </a>
    <a class="item tag-count-item flex flex-col justify-center items-center w-20" href="/categories">
        <div class="number text-2xl sm:text-xl text-second-text-color font-semibold">1</div>
        <div class="label text-third-text-color text-sm">Categories</div>
    </a>
    <a class="item tag-count-item flex flex-col justify-center items-center w-20" href="/archives">
        <div class="number text-2xl sm:text-xl text-second-text-color font-semibold">3</div>
        <div class="label text-third-text-color text-sm">Posts</div>
    </a>
</div>
    </div>

    <div class="window-mask"></div>

</header>


        </div>

        <div class="main-content-body">

            

            <div class="main-content">

                
                    <div class="post-page-container flex relative justify-between box-border w-full h-full">
    <div class="article-content-container">

        <div class="article-title relative w-full">
            
                <div class="w-full flex items-center pt-6 justify-start">
                    <h1 class="article-title-regular text-second-text-color tracking-tight text-4xl md:text-6xl font-semibold px-2 sm:px-6 md:px-8 py-3">lm_pytorch</h1>
                </div>
            
            </div>

        
            <div class="article-header flex flex-row gap-2 items-center px-2 sm:px-6 md:px-8">
                <div class="avatar w-[46px] h-[46px] flex-shrink-0 rounded-medium border border-border-color p-[1px]">
                    <img src="/images/favicon.png">
                </div>
                <div class="info flex flex-col justify-between">
                    <div class="author flex items-center">
                        <span class="name text-default-text-color text-lg font-semibold">YIWEI WU</span>
                        
                            <span class="author-label ml-1.5 text-xs px-2 py-0.5 rounded-small text-third-text-color border border-shadow-color-1">Lv1</span>
                        
                    </div>
                    <div class="meta-info">
                        <div class="article-meta-info">
    <span class="article-date article-meta-item">
        <i class="fa-regular fa-pen-fancy"></i>&nbsp;
        <span class="desktop">2024-06-29 21:14:37</span>
        <span class="mobile">2024-06-29 21:14:37</span>
        <span class="hover-info">Created</span>
    </span>
    
        <span class="article-date article-meta-item">
            <i class="fa-regular fa-wrench"></i>&nbsp;
            <span class="desktop">2024-07-05 17:53:13</span>
            <span class="mobile">2024-07-05 17:53:13</span>
            <span class="hover-info">Updated</span>
        </span>
    

    
    
        <span class="article-tags article-meta-item">
            <i class="fa-regular fa-tags"></i>&nbsp;
            <ul>
                
                    <li>
                        <a href="/tags/DEEP-LEARNING/">DEEP LEARNING</a>&nbsp;
                    </li>
                
            </ul>
        </span>
    

    
    
    
    
        <span class="article-pv article-meta-item">
            <i class="fa-regular fa-eye"></i>&nbsp;<span id="busuanzi_value_page_pv"></span>
        </span>
    
</div>

                    </div>
                </div>
            </div>
        

        


        <div class="article-content markdown-body px-2 sm:px-6 md:px-8 pb-8">
            <h1 id="线性神经网络"><a href="#线性神经网络" class="headerlink" title="线性神经网络"></a>线性神经网络</h1><h2 id="向量链式法则"><a href="#向量链式法则" class="headerlink" title="向量链式法则"></a>向量链式法则</h2><ul>
<li><h3 id="标量链式法则"><a href="#标量链式法则" class="headerlink" title="标量链式法则"></a>标量链式法则</h3></li>
</ul>
<p><a target="_blank" rel="noopener" href="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/blob/main/imgs/07/image-01"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/07/image-01"
                      alt="image-20220107231957396"
                ></a></p>
<ul>
<li><h3 id="拓展到向量"><a href="#拓展到向量" class="headerlink" title="拓展到向量"></a>拓展到向量</h3><blockquote>
<p>需要注意维数的变化</p>
<p>下图三种情况分别对应：</p>
<ol>
<li>y为标量，x为向量</li>
<li>y为标量，x为矩阵</li>
<li>y、x为矩阵</li>
</ol>
</blockquote>
</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/blob/main/imgs/07/image-02"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/07/image-02"
                      alt="image-20220107231931153"
                ></a></p>
<hr>
<h5 id="例1（标量对向量求导）"><a href="#例1（标量对向量求导）" class="headerlink" title="例1（标量对向量求导）"></a>例1（标量对向量求导）</h5><blockquote>
<p>这里应该是用分子布局，所以是X转置</p>
</blockquote>
<p> <a target="_blank" rel="noopener" href="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/blob/main/imgs/07/image-03"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/07/image-03"
                      alt="image-20220107233527373"
                ></a></p>
<h5 id="例2（涉及到矩阵的情况）"><a href="#例2（涉及到矩阵的情况）" class="headerlink" title="例2（涉及到矩阵的情况）"></a>例2（涉及到矩阵的情况）</h5><blockquote>
<p>X是mxn的矩阵,w为n维向量，y为m维向量； z对Xw-y做L2 norm,为标量； 过程与例一大体一致；</p>
</blockquote>
<p> <a target="_blank" rel="noopener" href="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/blob/main/imgs/07/image-04"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/07/image-04"
                      alt="image-20220107233753066"
                ></a></p>
<hr>
<blockquote>
<p>由于在神经网络动辄几百层，手动进行链式求导是很困难的，因此我们需要借助自动求导</p>
</blockquote>
<hr>
<h3 id="自动求导"><a href="#自动求导" class="headerlink" title="自动求导"></a>自动求导</h3><ul>
<li><p>含义：计算一个函数在指定值上的导数</p>
</li>
<li><p>自动求导有别于</p>
<ul>
<li><p>符号求导</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/07/image-05"
                      alt="image-20220107235547201"
                ></p>
</li>
<li><p>数值求导</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/07/image-06"
                      alt="image-20220107235611767"
                ></p>
</li>
</ul>
</li>
</ul>
<p>为了更好地理解自动求导，下面引入计算图的概念</p>
<h4 id="计算图"><a href="#计算图" class="headerlink" title="计算图"></a>计算图</h4><ul>
<li><p>将代码分解成操作子</p>
</li>
<li><p>将计算表示成一个<strong>无环图</strong></p>
<blockquote>
<p>下图自底向上其实就类似于链式求导过程</p>
</blockquote>
</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/blob/main/imgs/07/image-07"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/07/image-07"
                      alt="image-20220107235918270"
                ></a></p>
<ul>
<li><p>计算图有两种构造方式</p>
<ul>
<li><p>显示构造</p>
<blockquote>
<p>可以理解为先定义公式再代值</p>
<p>Tensorflow&#x2F;Theano&#x2F;MXNet</p>
</blockquote>
<p> <a target="_blank" rel="noopener" href="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/blob/main/imgs/07/image-08"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/07/image-08"
                      alt="image-20220108000740736"
                ></a></p>
</li>
<li><p>隐式构造</p>
<blockquote>
<p>系统将所有的计算记录下来</p>
<p>Pytorch&#x2F;MXNet</p>
</blockquote>
<p> <a target="_blank" rel="noopener" href="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/blob/main/imgs/07/image-09"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/07/image-09"
                      alt="image-20220108001154208"
                ></a></p>
</li>
</ul>
</li>
</ul>
<h4 id="自动求导的两种模式"><a href="#自动求导的两种模式" class="headerlink" title="自动求导的两种模式"></a>自动求导的两种模式</h4><ul>
<li><p>正向累积</p>
<p> <a target="_blank" rel="noopener" href="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/blob/main/imgs/07/image-10"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/07/image-10"
                      alt="image-20220108001506326"
                ></a></p>
</li>
<li><p>反向累积（反向传递back propagation）</p>
<p> <a target="_blank" rel="noopener" href="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/blob/main/imgs/07/image-11"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/07/image-11"
                      alt="image-20220108001517029"
                ></a></p>
</li>
</ul>
<p> <strong>反向累积计算过程</strong></p>
<p><a target="_blank" rel="noopener" href="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/blob/main/imgs/07/image-12"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/07/image-12"
                      alt="image-20220108001847175"
                ></a></p>
<blockquote>
<p>反向累积的正向过程：自底向上，需要存储中间结果</p>
<p>反向累积的反向过程：自顶向下，可以去除不需要的枝（图中的x应为w）</p>
<p> <a target="_blank" rel="noopener" href="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/blob/main/imgs/07/image-13"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/07/image-13"
                      alt="image-20220108002228166"
                ></a></p>
</blockquote>
<h4 id="复杂度比较"><a href="#复杂度比较" class="headerlink" title="复杂度比较"></a>复杂度比较</h4><ul>
<li><p>反向累积</p>
<ul>
<li>时间复杂度：O(n),n是操作子数<ul>
<li>通常正向和反向的代价类似</li>
</ul>
</li>
<li>空间复杂度：O(n)<ul>
<li>存储正向过程所有的中间结果</li>
</ul>
</li>
</ul>
</li>
<li><p>正向累积</p>
<blockquote>
<p>每次计算一个变量的梯度时都需要将所有节点扫一遍</p>
</blockquote>
<ul>
<li>时间复杂度：O(n)</li>
<li>空间复杂度：O(1)</li>
</ul>
</li>
</ul>
<hr>
<h3 id="代码部分"><a href="#代码部分" class="headerlink" title="代码部分"></a>代码部分</h3><div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#对y = x.Tx关于列向量x求导</span><br><span class="line">import torch</span><br><span class="line"></span><br><span class="line">x = torch.arange(4.0)</span><br><span class="line">x</span><br></pre></td></tr></table></figure></div>

<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([0., 1., 2., 3.])</span><br></pre></td></tr></table></figure></div>



<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#存储梯度</span><br><span class="line">x.requires_grad_(True)#等价于x = torch.arange(4.0,requires_grad=True)</span><br><span class="line">x.grad#默认值是None</span><br></pre></td></tr></table></figure></div>

<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">y = torch.dot(x,x)</span><br><span class="line">y</span><br><span class="line">#PyTorch隐式地构造计算图，grad_fn用于记录梯度计算</span><br></pre></td></tr></table></figure></div>

<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor(14., grad_fn=&lt;DotBackward0&gt;)</span><br></pre></td></tr></table></figure></div>

<p><strong>通过调用反向传播函数来自动计算y关于x每个分量的梯度</strong></p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y.backward()</span><br><span class="line">x.grad</span><br></pre></td></tr></table></figure></div>

<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([0., 2., 4., 6.])</span><br></pre></td></tr></table></figure></div>



<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x.grad==2*x#验证</span><br></pre></td></tr></table></figure></div>

<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([True, True, True, True])</span><br></pre></td></tr></table></figure></div>



<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 在默认情况下，PyTorch会累积梯度，我们需要清除之前的值</span><br><span class="line">x.grad.zero_()#如果没有这一步结果就会加累上之前的梯度值，变为[1,5,9,13]</span><br><span class="line">y = x.sum()</span><br><span class="line">y.backward()</span><br><span class="line">x.grad</span><br></pre></td></tr></table></figure></div>

<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([1., 1., 1., 1.])</span><br></pre></td></tr></table></figure></div>



<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">x.grad.zero_()</span><br><span class="line">y=x*x#哈达玛积，对应元素相乘</span><br><span class="line"></span><br><span class="line">#在深度学习中我们一般不计算微分矩阵</span><br><span class="line">#而是计算批量中每个样本单独计算的偏导数之和</span><br><span class="line"></span><br><span class="line">y.sum().backward()#等价于y.backword(torch.ones(len(x)))</span><br><span class="line">x.grad</span><br></pre></td></tr></table></figure></div>

<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([0., 2., 4., 6.])</span><br></pre></td></tr></table></figure></div>



<p><strong>将某些计算移动到记录的计算图之外</strong></p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># 后可用于用于将神经网络的一些参数固定住</span><br><span class="line">x.grad.zero_()</span><br><span class="line">y = x*x</span><br><span class="line">u = y.detach()#把y当作常数</span><br><span class="line">z = u*x</span><br><span class="line"></span><br><span class="line">z.sum().backward()</span><br><span class="line">x.grad == u</span><br></pre></td></tr></table></figure></div>

<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([True, True, True, True])</span><br></pre></td></tr></table></figure></div>



<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x.grad.zero_()</span><br><span class="line">y.sum().backward()</span><br><span class="line">x.grad == 2*x</span><br></pre></td></tr></table></figure></div>

<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([True, True, True, True])</span><br></pre></td></tr></table></figure></div>

<p><strong>即使构建函数的计算图需要用过Python控制流，仍然可以计算得到的变量的梯度</strong></p>
<p><strong>这也是隐式构造的优势，因为它会存储梯度计算的计算图，再次计算时执行反向过程就可以</strong></p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">def f(a):</span><br><span class="line">    b = a * 2</span><br><span class="line">    while b.norm()&lt;1000:</span><br><span class="line">        b = b * 2</span><br><span class="line">    if b.sum() &gt; 0:</span><br><span class="line">        c = b</span><br><span class="line">    else:</span><br><span class="line">        c = 100 * b</span><br><span class="line">    return c</span><br><span class="line"></span><br><span class="line">a = torch.randn(size=(),requires_grad=True)</span><br><span class="line">d = f(a)</span><br><span class="line">d.backward()</span><br><span class="line"></span><br><span class="line">a.grad == d / a</span><br></pre></td></tr></table></figure></div>

<h2 id="线性回归-基础优化算法"><a href="#线性回归-基础优化算法" class="headerlink" title="线性回归+基础优化算法"></a>线性回归+基础优化算法</h2><h3 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h3><ul>
<li><p><strong>线性模型</strong></p>
<ul>
<li><p>输入：$x&#x3D;[x_1,x_2,…,x_n]^T$</p>
</li>
<li><p>线性模型需要确定一个n维权重和一个标量偏差$\omega&#x3D;[\omega_1,\omega_2,…,\omega_n]^T,b$</p>
</li>
<li><p>输出 ：$y&#x3D;\omega_1x_1+\omega_2x_2+…+\omega_nx_n+b$，</p>
<p>向量版本的是 𝑦&#x3D;&lt;𝜔,𝑥&gt;+𝑏</p>
</li>
<li><p>线性模型可以看作是单层神经网络</p>
<blockquote>
<ul>
<li>神经网络源于神经科学</li>
<li>最早的神经网络是源自神经科学的，但是时至今日，很多神经网络已经远远高于神经科学，可解释性也不是很强，不必纠结</li>
</ul>
</blockquote>
</li>
</ul>
</li>
<li><p>衡量估计质量</p>
<ul>
<li><p>我们需要估计模型的预估值和真实值之间的差距，例如房屋售价和股价</p>
</li>
<li><p>假设$y$是真实值，$\tilde{y}$是估计值，我们可以比较</p>
<p>𝑙(𝑦,$\tilde{y}$)&#x3D;(1&#x2F;2)<em>(𝑦−$\tilde{y}$)2，这个叫做*<em>平方损失</em></em></p>
</li>
</ul>
</li>
<li><p><strong>训练数据</strong></p>
<ul>
<li><p>收集一些数据点来决定参数值（权重$\omega$和偏差$b$），例如6个月内被卖掉的房子。</p>
</li>
<li><p>这被称之为训练数据</p>
</li>
<li><p>通常越多越好。需要注意的是，现实世界的数据都是有限的，但是为了训练出精确的参数往往需要训练数据越多越好，当训练数据不足的时候，我们还需要进行额外处理。</p>
</li>
<li><p>假设我们有n个样本，记为</p>
<p>𝑋&#x3D;[𝑥1,𝑥2,…,𝑥𝑛]𝑇,𝑦&#x3D;[𝑦1,𝑦2,…𝑦𝑛]𝑇</p>
<p>𝑋的每一行是一个样本，$y$的每一行是一个输出的实数值。</p>
</li>
</ul>
</li>
<li><p><strong>参数学习</strong></p>
<ul>
<li><p><strong>训练损失</strong>。但我们训练参数的时候，需要定义一个损失函数来衡量参数的好坏，应用前文提过的平方损失有公式：</p>
</li>
<li><p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="C:/Users/Y9000p/AppData/Roaming/Typora/typora-user-images/image-20240701223140819.png"
                      alt="image-20240701223140819"
                ></p>
<p> 𝑙(𝑋,𝑥,𝜔,𝑏)&#x3D;(1&#x2F;2𝑛)<em>∑(𝑦𝑖−$\tilde{y}$)^2&#x3D;(1&#x2F;2𝑛)</em>||𝑦i−𝑋𝜔−𝑏||^2</p>
</li>
<li><p><strong>最小化损失来学习参数</strong>。训练参数的目的就是使损失函数的值尽可能小（这意味着预估值和真实值更接近）。最后求得的参数值可表示为：</p>
<p>$\omega^*,b^*&#x3D;argmin_{\omega,b}l(X,x,\omega,b)$</p>
</li>
</ul>
</li>
<li><p><strong>显示解</strong> </p>
<ul>
<li><p>线性回归有显示解，即可以直接矩阵数学运算，得到参数w和b的最优解，而不是用梯度下降，牛顿法等参数优化方式一点点逼近最优解。</p>
</li>
<li><p><strong>推导过程</strong>：</p>
<ul>
<li><p>为了方便矩阵表示和计算，将偏差加入权重，$X\gets[X,1],\omega\gets[\omega,b]$</p>
</li>
<li><p>损失函数是凸函数，最优解满足导数为0，可解出显示解</p>
<p>令$\frac{\partial}{\partial\omega} l(X,y,\omega)&#x3D;0$</p>
<p>有$\frac{1}{n}(y-X\omega)^TX&#x3D;0$</p>
<p>解得$\omega^*&#x3D;(X^TX)^{-1}X^Ty$</p>
</li>
</ul>
</li>
</ul>
</li>
<li><p>总结</p>
<ul>
<li>线性回归是对n维输入的加权，外加偏差</li>
<li>使用<strong>平方损失</strong>来衡量预测值和真实值之间的误差</li>
<li><strong>线性回归有显示解</strong></li>
<li>线性回归可以看作单层神经网络</li>
</ul>
</li>
</ul>
<h3 id="2-基础优化算法"><a href="#2-基础优化算法" class="headerlink" title="2.基础优化算法"></a>2.基础优化算法</h3><ul>
<li><p><strong>梯度下降</strong></p>
<ul>
<li>当模型没有显示解的时候，应用梯度下降法逼近最优解。</li>
<li>梯度下降法的具体步骤：<ul>
<li>挑选一个初始值$\omega_0$</li>
<li>重复迭代参数，迭代公式为：$\omega_t&#x3D;\omega_{t-1}-\lambda\frac{\partial l}{\partial\omega_{t-1} } $<ul>
<li><strong>−𝜕𝑙𝜕𝜔𝑡−1为函数值下降最快的方向，学习率$\lambda$为学习步长。</strong></li>
</ul>
</li>
</ul>
</li>
<li>选择学习率<ul>
<li>学习率$\lambda$为学习步长，代表了沿负梯度方向走了多远，这是超参数（人为指定的的值，不是训练得到的）</li>
<li>学习率不能太大，也不能太小，需要选取适当。</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>小批量随机梯度下降</strong></p>
<ul>
<li><p>在整个训练集上算梯度太贵了</p>
<ul>
<li>在实际应用中，很少直接应用梯度下降法，这是因为每次更新都需要计算训练集上所有的样本，耗费时间太长。一个深度神经网络模型，迭代一次可能需要数分钟甚至数小时。</li>
</ul>
</li>
<li><p>为了减少运算代价，我们可以&#x3D;&#x3D;随机采样&#x3D;&#x3D;b个样本$i_1,i_2,…,i_b$来近似损失，损失函数为：</p>
<p> 1𝑏∑𝑖∈𝐼𝑏𝑙(𝑥𝑖,𝑦𝑖,𝜔) ,</p>
<p>其中<strong>b是批量大小(batch size)，也是超参数</strong></p>
</li>
<li><p><strong>选择批量大小</strong></p>
<ul>
<li>b也不能太大：内存消耗增加；浪费计算资源，一个极端的情况是可能会重复选取很多差不多的样本，浪费计算资源</li>
<li>b也不能太小：每次计算量太小，很难以并行，不能最大限度利用GPU资源</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>总结</strong></p>
<ul>
<li>梯度下降通过不断<strong>沿着负梯度方向</strong>更新参数求解</li>
<li>小批量随机梯度下降是深度学习默认的求解算法（简单，稳定）</li>
<li><strong>两个重要的超参数：批量大小（batch size），学习率（lr）</strong></li>
</ul>
</li>
</ul>
<h3 id="3-线性回归的从零开始实现"><a href="#3-线性回归的从零开始实现" class="headerlink" title="3.线性回归的从零开始实现"></a>3.线性回归的从零开始实现</h3><div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">import random</span><br><span class="line">import torch</span><br><span class="line">from d2l import torch as d2l</span><br><span class="line"></span><br><span class="line">def synthetic_data(w, b, num_examples):</span><br><span class="line">    # 生成特征矩阵 X，形状为 (num_examples, len(w))</span><br><span class="line">    X = torch.normal(0, 1, (num_examples, len(w)))</span><br><span class="line">    # 生成标签 y</span><br><span class="line">    y = torch.matmul(X, w) + b</span><br><span class="line">    # 向标签添加噪声</span><br><span class="line">    y += torch.normal(0, 0.01, y.shape)</span><br><span class="line">    # 返回特征和标签，标签的形状调整为 (-1, 1)</span><br><span class="line">    return X, y.reshape((-1, 1))</span><br><span class="line"></span><br><span class="line"># 定义真实的权重和偏置</span><br><span class="line">true_w = torch.tensor([2, -3.4])</span><br><span class="line">true_b = 4.2</span><br><span class="line"># 生成特征和标签</span><br><span class="line">features, labels = synthetic_data(true_w, true_b, 1000)</span><br><span class="line"></span><br><span class="line"># 打印前五个特征和标签</span><br><span class="line">print(&quot;Features:&quot;, features[:5])</span><br><span class="line">print(&quot;Labels:&quot;, labels[:5])</span><br><span class="line"></span><br><span class="line">d2l.set_figsize()</span><br><span class="line">d2l.plt.scatter(features[:,1].detach().numpy(),labels.detach().numpy(),1)</span><br></pre></td></tr></table></figure></div>

<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">def data_iter(batch_size, features, labels):</span><br><span class="line">    num_examples = len(features)</span><br><span class="line">    indices = list(range(num_examples))</span><br><span class="line">    random.shuffle(indices)</span><br><span class="line">    for i in range(0, num_examples, batch_size):</span><br><span class="line">        batch_indices = torch.tensor(</span><br><span class="line">            indices[i:min(i + batch_size, num_examples)]</span><br><span class="line">        )</span><br><span class="line">        yield features[batch_indices], labels[batch_indices]</span><br><span class="line"></span><br><span class="line"># 测试数据迭代器</span><br><span class="line">batch_size = 10</span><br><span class="line">for X, y in data_iter(batch_size, features, labels):</span><br><span class="line">    print(X, &#x27;\n&#x27;, y)</span><br><span class="line">    break</span><br></pre></td></tr></table></figure></div>

<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">#初始化参数</span><br><span class="line">w =torch.normal(0,0.01,(2,1),requires_grad=true)</span><br><span class="line">b = torch.zeros(1,requires_grad=true)</span><br><span class="line"></span><br><span class="line">#定义模型</span><br><span class="line">def linreg(x,w,b):</span><br><span class="line">	return torch.matmul(x,w)+b </span><br><span class="line">	</span><br><span class="line">#定义损失函数</span><br><span class="line">def squared_loss(y_hat,y):</span><br><span class="line">	return (y_hat-y.reshape(y_hat.shape))**2/2</span><br><span class="line"></span><br><span class="line">#定义优化算法</span><br><span class="line">def sgd(params,lr,batch_size):</span><br><span class="line">	with torch.no_grad():</span><br><span class="line">		for param in params:</span><br><span class="line">			param-=lr*param.grad/batch_size</span><br><span class="line">			param.grad.zeros_()</span><br></pre></td></tr></table></figure></div>



<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">## 训练过程</span><br><span class="line">lr = 0.03</span><br><span class="line">num_epoches = 3</span><br><span class="line">net = linreg</span><br><span class="line">loss = squared_loss</span><br><span class="line">for epoch in range(num_epochs):</span><br><span class="line">    for X, y in data_iter(batch_size, features, labels):</span><br><span class="line">        l = loss(net(X, w, b), y)  # 计算损失</span><br><span class="line">        l.sum().backward()  # 反向传播计算梯度</span><br><span class="line">        sgd([w, b], lr, batch_size)  # 更新参数</span><br><span class="line">    with torch.no_grad():  # 禁止自动计算梯度</span><br><span class="line">        train_l = loss(net(features, w, b), labels)</span><br><span class="line">        print(f&#x27;epoch &#123;epoch + 1&#125;, loss &#123;float(train_l.mean()):f&#125;&#x27;)  # 打印每个 epoch 的损失</span><br><span class="line"></span><br></pre></td></tr></table></figure></div>

<h3 id="4-新型回归的简洁实现"><a href="#4-新型回归的简洁实现" class="headerlink" title="4.新型回归的简洁实现"></a>4.新型回归的简洁实现</h3><div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import torch</span><br><span class="line">from torch.utils import data</span><br><span class="line">from d2l import torch as d2l</span><br><span class="line"></span><br><span class="line">true_w = torch.tensor([2, -3.4])</span><br><span class="line">true_b = 4.2</span><br><span class="line"># 生成特征和标签</span><br><span class="line">features, labels = synthetic_data(true_w, true_b, 1000)</span><br></pre></td></tr></table></figure></div>

<p>调用框架中现有的API来读取数据</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torch.utils.data import TensorDataset, DataLoader</span><br><span class="line"></span><br><span class="line">def load_array(data_arrays, batch_size, is_train=True):</span><br><span class="line">    dataset = TensorDataset(*data_arrays)</span><br><span class="line">    return DataLoader(dataset, batch_size=batch_size, shuffle=is_train)</span><br><span class="line"></span><br><span class="line">batch_size = 10</span><br><span class="line">data_iter = load_array((features, labels), batch_size)</span><br></pre></td></tr></table></figure></div>

<p>使用框架的预定义好的层</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">from torch import nn</span><br><span class="line">net = nn.Sequential(nn.Linear(2,1))</span><br><span class="line"></span><br><span class="line">net[0].weight.data.normal(0,0.01)</span><br><span class="line">net[0].bias.data.fill_(0)</span><br></pre></td></tr></table></figure></div>

<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 计算均方误差使用MSELoss类</span><br><span class="line">loss = nn.MSELoss()</span><br><span class="line"></span><br><span class="line"># 实例化SGD</span><br><span class="line">trainer = torch.optim.SGD(net.parameters(),lr=0.03)</span><br></pre></td></tr></table></figure></div>

<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">num_epochs=3</span><br><span class="line">for epoch in range(num_epochs):</span><br><span class="line">	for X ,y in data_iter:</span><br><span class="line">		l = loss(net(X),y)</span><br><span class="line">		trainer.zero_grad()</span><br><span class="line">		l.backward()</span><br><span class="line">		trainer.step()</span><br><span class="line">	l = loss(net(features),labels)</span><br><span class="line">	print(f&#x27;epoch &#123;epoch + 1&#125;,loss &#123;l:f&#125;&#x27;)</span><br></pre></td></tr></table></figure></div>



<h2 id="Softmax-回归-损失函数-图片分类数据集"><a href="#Softmax-回归-损失函数-图片分类数据集" class="headerlink" title="Softmax 回归 + 损失函数 + 图片分类数据集"></a>Softmax 回归 + 损失函数 + 图片分类数据集</h2><h3 id="回归VS分类："><a href="#回归VS分类：" class="headerlink" title="回归VS分类："></a>回归VS分类：</h3><ul>
<li>回归估计一个连续值</li>
<li>分类预测一个离散类别</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/blob/main/imgs/09/09-01.png"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/09/09-01.png"
                      alt="image"
                ></a></p>
<h4 id="1-1-从回归到多类分类："><a href="#1-1-从回归到多类分类：" class="headerlink" title="1.1 从回归到多类分类："></a>1.1 从回归到多类分类：</h4><h5 id="回归："><a href="#回归：" class="headerlink" title="回归："></a>回归：</h5><ul>
<li>单连续数值输出</li>
<li>自然区间R</li>
<li>跟真实值的区别作为损失</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/blob/main/imgs/09/09-02.png"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/09/09-02.png"
                      alt="image"
                ></a></p>
<h4 id="分类："><a href="#分类：" class="headerlink" title="分类："></a>分类：</h4><ul>
<li><p>通常多个输出</p>
</li>
<li><p>输出i是预测为第i类的置信度</p>
<p><a target="_blank" rel="noopener" href="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/blob/main/imgs/09/09-03.png"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/09/09-03.png"
                      alt="image"
                ></a></p>
</li>
</ul>
<h5 id="均方损失："><a href="#均方损失：" class="headerlink" title="均方损失："></a>均方损失：</h5><ul>
<li><p>对类别进行一位有效编码</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="C:/Users/Y9000p/AppData/Roaming/Typora/typora-user-images/image-20240702004009211.png"
                      alt="image-20240702004009211"
                ></p>
</li>
<li><p>使用均方损失训练</p>
</li>
<li><p>最大值为预测 ![](<a class="link"   target="_blank" rel="noopener" href="http://latex.codecogs.com/gif.latex?%5C%5C" >http://latex.codecogs.com/gif.latex?\\ <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a> \hat{y}&#x3D;\underset {i}{argmax}\quad o^{i} )</p>
</li>
</ul>
<h5 id="无校验比例"><a href="#无校验比例" class="headerlink" title="无校验比例"></a>无校验比例</h5><ul>
<li>对类别进行一位有效编码</li>
<li>最大值为预测 ![](<a class="link"   target="_blank" rel="noopener" href="http://latex.codecogs.com/gif.latex?%5C%5C" >http://latex.codecogs.com/gif.latex?\\ <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a> \hat{y}&#x3D;\underset {i}{argmax}\quad o^{i} )</li>
<li>需要更置信的识别正确类（大余量） ![](<a class="link"   target="_blank" rel="noopener" href="http://latex.codecogs.com/gif.latex?%5C%5C" >http://latex.codecogs.com/gif.latex?\\ <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a> o_y-o_i\geq\Delta(y,i) )</li>
</ul>
<h5 id="校验比例"><a href="#校验比例" class="headerlink" title="校验比例"></a>校验比例</h5><ul>
<li><p>输出匹配概率（非负，和为1） ![](<a class="link"   target="_blank" rel="noopener" href="http://latex.codecogs.com/gif.latex?%5C%5C" >http://latex.codecogs.com/gif.latex?\\ <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a> \hat{y}&#x3D;softmax(o) )</p>
<p>![](<a class="link"   target="_blank" rel="noopener" href="http://latex.codecogs.com/gif.latex?%5C%5C" >http://latex.codecogs.com/gif.latex?\\ <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a> \hat{y_i}&#x3D;\frac{exp(o_i)}{\sum_{k} exp(o_k)} )</p>
</li>
<li><p>概率y和$\hat{y}$的区别作为损失</p>
</li>
</ul>
<h3 id="Softmax和交叉熵损失"><a href="#Softmax和交叉熵损失" class="headerlink" title="Softmax和交叉熵损失"></a>Softmax和交叉熵损失</h3><ul>
<li>交叉熵用来衡量两个概率的区别$H(p,q)&#x3D;\sum_{i} -p_{i}log(q_i)$</li>
<li>将它作为损失 ![](<a class="link"   target="_blank" rel="noopener" href="http://latex.codecogs.com/gif.latex?%5C%5C" >http://latex.codecogs.com/gif.latex?\\ <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a> l(y,\hat{y})&#x3D;-\sum_{i}y_{i}log\hat{y_{i}}&#x3D;-log\hat{y_y} )</li>
<li>其梯度是真实概率和预测概率的区别 ![](<a class="link"   target="_blank" rel="noopener" href="http://latex.codecogs.com/gif.latex?%5C%5C" >http://latex.codecogs.com/gif.latex?\\ <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a> \partial_{o_{i}}l(y,\hat{y})&#x3D;softmax(o)<em>{i}-y</em>{i} )</li>
</ul>
<blockquote>
<p>Softmax回归是一个多类分类模型</p>
<p>使用Softmax操作子得到每个类的预测置信度</p>
<p>使用交叉熵来衡量和预测标号的区别</p>
</blockquote>
<h3 id="2-损失函数"><a href="#2-损失函数" class="headerlink" title="2.损失函数"></a>2.损失函数</h3><p><a target="_blank" rel="noopener" href="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/blob/main/imgs/09/09-04.png"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/09/09-04.png"
                      alt="image"
                ></a></p>
<h4 id="L2-Loss"><a href="#L2-Loss" class="headerlink" title="L2 Loss"></a>L2 Loss</h4><p>![](<a class="link"   target="_blank" rel="noopener" href="http://latex.codecogs.com/gif.latex?%5C%5C" >http://latex.codecogs.com/gif.latex?\\ <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a> l(y,y^{‘})&#x3D;\frac{1}{2}(y-y^{‘})^2 )</p>
<p><a target="_blank" rel="noopener" href="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/blob/main/imgs/09/09-05.png"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/09/09-05.png"
                      alt="image"
                ></a></p>
<blockquote>
<p>梯度会随着结果逼近而下降</p>
</blockquote>
<h4 id="L1-Loss"><a href="#L1-Loss" class="headerlink" title="L1 Loss"></a>L1 Loss</h4><p>![](<a class="link"   target="_blank" rel="noopener" href="http://latex.codecogs.com/gif.latex?%5C%5C" >http://latex.codecogs.com/gif.latex?\\ <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a> l(y,y^{‘})&#x3D;\lvert y-y^{‘}\rvert )</p>
<p><a target="_blank" rel="noopener" href="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/blob/main/imgs/09/09-06.png"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/09/09-06.png"
                      alt="image"
                ></a></p>
<blockquote>
<p>梯度保持不变，但在0处梯度随机</p>
</blockquote>
<h4 id="Huber’s-Robust-Loss"><a href="#Huber’s-Robust-Loss" class="headerlink" title="Huber’s Robust Loss"></a>Huber’s Robust Loss</h4><p><a target="_blank" rel="noopener" href="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/blob/main/imgs/09/09-07.png"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/09/09-07.png"
                      alt="image"
                ></a></p>
<blockquote>
<p>结合L1 Loss和L2 Loss的优点</p>
</blockquote>
<h3 id="图片分类数据集"><a href="#图片分类数据集" class="headerlink" title="图片分类数据集"></a>图片分类数据集</h3><h4 id="Fashion-MNIST数据集："><a href="#Fashion-MNIST数据集：" class="headerlink" title="Fashion-MNIST数据集："></a>Fashion-MNIST数据集：</h4><div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import torchvision</span><br><span class="line">import torch</span><br><span class="line">from torch.utils import data</span><br><span class="line">from torchvision import transforms</span><br><span class="line">from d2l import torch as d2l </span><br><span class="line"></span><br><span class="line">d2l.use_svg_display()</span><br></pre></td></tr></table></figure></div>

<ul>
<li><p>读取数据集</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 图片类型转变</span><br><span class="line">trans=transforms.ToTensor()</span><br><span class="line"></span><br><span class="line">mnist_train=torchvision.datasets.FashionMNIST(root=&quot;../data&quot;,train=True,                                              transform=trans,download=True)</span><br><span class="line">mnist_test=torchvision.datasets.FashionMNIST(root=&quot;../data&quot;,train=False,                                             transform=trans,download=True)</span><br></pre></td></tr></table></figure></div>
</li>
<li><p>数据集内图片大小</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mnist_train[0][0].shape</span><br><span class="line">torch.Size([1, 28, 28])</span><br></pre></td></tr></table></figure></div>

<p>表示图片为单通道（黑白）的28X28的图片</p>
</li>
<li><p>显示数据集图像</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X,y = next(iter(data.DataLoader(mnist_train,batch_size=18)))</span><br><span class="line">show_images(X.reshape(18,28,28),2,9,titles=get_fashion_mnist_labels(y))</span><br></pre></td></tr></table></figure></div>



<p><a target="_blank" rel="noopener" href="https://github.com/MLNLP-World/DeepLearning-MuLi-[Notes/blob/main/imgs/09/09-08.png"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/09/09-08.png"
                      alt="image"
                ></a></p>
<p>读取一小批数据</p>
</li>
</ul>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">256</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_dataloader_workers</span>():</span><br><span class="line">	<span class="keyword">return</span> <span class="number">4</span></span><br><span class="line">train_iter = data.Dataloader(mnist_train,batch_size,shuffle=<span class="literal">True</span>,num_workers=get_dataLoader_workers())</span><br><span class="line"></span><br><span class="line">timer = d2l.Timer()</span><br><span class="line"><span class="keyword">for</span> X,y <span class="keyword">in</span> train_iter:</span><br><span class="line">	<span class="keyword">continue</span></span><br><span class="line"><span class="string">f&#x27;<span class="subst">&#123;timer.stop():<span class="number">.2</span>f&#125;</span> sec&#x27;</span></span><br></pre></td></tr></table></figure></div>

<h3 id="从零实现softmax回归"><a href="#从零实现softmax回归" class="headerlink" title="从零实现softmax回归"></a>从零实现softmax回归</h3><h4 id="softmax"><a href="#softmax" class="headerlink" title="softmax:"></a>softmax:</h4><p>$$ softmax(X)<em>{ij}&#x3D;\frac{exp(X</em>{ij})}{\sum_{k} exp(X_{ik})} $$</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from IPthon import display</span><br><span class="line">from d2l import torch as d2l </span><br><span class="line"></span><br><span class="line">batch_size = 256</span><br><span class="line">train_iter,test_iter = d2l.load_data_fasion_mnist(batch_size) </span><br></pre></td></tr></table></figure></div>

<p>将图像展平，每个图像看做长度为784的向量，因为数据集有十个类别，所以网络输出维度为10。以此设定参数大小并初始化：</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">num_inputs = 784</span><br><span class="line">num_outputs = 10</span><br><span class="line"></span><br><span class="line">W = torch.normal(0, 0.01, size=(num_inputs, num_outputs), requires_grad=True)</span><br><span class="line">b = torch.zeros(num_outputs, requires_grad=True)</span><br></pre></td></tr></table></figure></div>

<p>实现softmax回归模型：</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">def softmax(X):</span><br><span class="line">    X_exp = torch.exp(X)</span><br><span class="line">    partition = X_exp.sum(1, keepdim=True)</span><br><span class="line">    return X_exp / partition</span><br><span class="line">#  矩阵中的非常大或非常小的元素可能造成数值上溢或下溢</span><br><span class="line"></span><br><span class="line">def net(X):</span><br><span class="line">    return softmax(torch.matmul(X.reshape((-1, W.shape[0])), W) + b)</span><br></pre></td></tr></table></figure></div>

<p> 实现交叉熵损失函数：</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">def cross_entropy(y_hat, y):</span><br><span class="line">    return - torch.log(y_hat[range(len(y_hat)), y])</span><br></pre></td></tr></table></figure></div>

<p>计算正确率：</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">def accuracy(y_hat, y):  </span><br><span class="line">    &quot;&quot;&quot;计算预测正确的数量&quot;&quot;&quot;</span><br><span class="line">    if len(y_hat.shape) &gt; 1 and y_hat.shape[1] &gt; 1:</span><br><span class="line">        y_hat = y_hat.argmax(axis=1)</span><br><span class="line">    cmp = y_hat.type(y.dtype) == y</span><br><span class="line">    return float(cmp.type(y.dtype).sum())</span><br><span class="line"> </span><br></pre></td></tr></table></figure></div>

<p>评估net精度</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">def evaluate_accuracy(net, data_iter):  </span><br><span class="line">    &quot;&quot;&quot;计算在指定数据集上模型的精度&quot;&quot;&quot;</span><br><span class="line">    if isinstance(net, torch.nn.Module):</span><br><span class="line">        net.eval()</span><br><span class="line">    metric = Accumulator(2)</span><br><span class="line">    with torch.no_grad():</span><br><span class="line">        for X, y in data_iter:</span><br><span class="line">            metric.add(accuracy(net(X), y), y.numel())</span><br><span class="line">    return metric[0] / metric[1]</span><br></pre></td></tr></table></figure></div>

<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">class Accumulator:  </span><br><span class="line">    &quot;&quot;&quot;在n个变量上累加&quot;&quot;&quot;</span><br><span class="line">    def __init__(self, n):</span><br><span class="line">        self.data = [0.0] * n</span><br><span class="line"></span><br><span class="line">    def add(self, *args):</span><br><span class="line">        self.data = [a + float(b) for a, b in zip(self.data, args)]</span><br><span class="line"></span><br><span class="line">    def reset(self):</span><br><span class="line">        self.data = [0.0] * len(self.data)</span><br><span class="line"></span><br><span class="line">    def __getitem__(self, idx):</span><br><span class="line">        return self.data[idx]</span><br></pre></td></tr></table></figure></div>

<p>softmax回归的训练</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">def train_epoch_ch3(net, train_iter, loss, updater):  #@save</span><br><span class="line">    &quot;&quot;&quot;训练模型一个迭代周期（定义见第3章）&quot;&quot;&quot;</span><br><span class="line">    # 将模型设置为训练模式，不再计算grad之类的内容</span><br><span class="line">    if isinstance(net, torch.nn.Module):</span><br><span class="line">        net.train()</span><br><span class="line">    # 训练损失总和、训练准确度总和、样本数</span><br><span class="line">    metric = Accumulator(3)</span><br><span class="line">    for X, y in train_iter:</span><br><span class="line">        # 计算梯度并更新参数</span><br><span class="line">        y_hat = net(X)</span><br><span class="line">        l = loss(y_hat, y)</span><br><span class="line">        if isinstance(updater, torch.optim.Optimizer):</span><br><span class="line">            # 使用PyTorch内置的优化器和损失函数</span><br><span class="line">            updater.zero_grad()</span><br><span class="line">            l.mean().backward()</span><br><span class="line">            updater.step()</span><br><span class="line">        else:</span><br><span class="line">            # 使用定制的优化器和损失函数</span><br><span class="line">            l.sum().backward()</span><br><span class="line">            updater(X.shape[0])</span><br><span class="line">        metric.add(float(l.sum()), accuracy(y_hat, y), y.numel())</span><br><span class="line">    # 返回训练损失和训练精度</span><br><span class="line">    return metric[0] / metric[2], metric[1] / metric[2]</span><br></pre></td></tr></table></figure></div>

<p>定义一个在动画中绘制数据的实用程序类</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">class Animator:  #@save</span><br><span class="line">    &quot;&quot;&quot;在动画中绘制数据&quot;&quot;&quot;</span><br><span class="line">    def __init__(self, xlabel=None, ylabel=None, legend=None, xlim=None,</span><br><span class="line">                 ylim=None, xscale=&#x27;linear&#x27;, yscale=&#x27;linear&#x27;,</span><br><span class="line">                 fmts=(&#x27;-&#x27;, &#x27;m--&#x27;, &#x27;g-.&#x27;, &#x27;r:&#x27;), nrows=1, ncols=1,</span><br><span class="line">                 figsize=(3.5, 2.5)):</span><br><span class="line">        # 增量地绘制多条线</span><br><span class="line">        if legend is None:</span><br><span class="line">            legend = []</span><br><span class="line">        d2l.use_svg_display()</span><br><span class="line">        self.fig, self.axes = d2l.plt.subplots(nrows, ncols, figsize=figsize)</span><br><span class="line">        if nrows * ncols == 1:</span><br><span class="line">            self.axes = [self.axes, ]</span><br><span class="line">        # 使用lambda函数捕获参数</span><br><span class="line">        self.config_axes = lambda: d2l.set_axes(</span><br><span class="line">            self.axes[0], xlabel, ylabel, xlim, ylim, xscale, yscale, legend)</span><br><span class="line">        self.X, self.Y, self.fmts = None, None, fmts</span><br><span class="line"></span><br><span class="line">    def add(self, x, y):</span><br><span class="line">        # 向图表中添加多个数据点</span><br><span class="line">        if not hasattr(y, &quot;__len__&quot;):</span><br><span class="line">            y = [y]</span><br><span class="line">        n = len(y)</span><br><span class="line">        if not hasattr(x, &quot;__len__&quot;):</span><br><span class="line">            x = [x] * n</span><br><span class="line">        if not self.X:</span><br><span class="line">            self.X = [[] for _ in range(n)]</span><br><span class="line">        if not self.Y:</span><br><span class="line">            self.Y = [[] for _ in range(n)]</span><br><span class="line">        for i, (a, b) in enumerate(zip(x, y)):</span><br><span class="line">            if a is not None and b is not None:</span><br><span class="line">                self.X[i].append(a)</span><br><span class="line">                self.Y[i].append(b)</span><br><span class="line">        self.axes[0].cla()</span><br><span class="line">        for x, y, fmt in zip(self.X, self.Y, self.fmts):</span><br><span class="line">            self.axes[0].plot(x, y, fmt)</span><br><span class="line">        self.config_axes()</span><br><span class="line">        display.display(self.fig)</span><br><span class="line">        display.clear_output(wait=True)</span><br></pre></td></tr></table></figure></div>

<p>定义训练模型：</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">def train_ch3(net, train_iter, test_iter, loss, num_epochs, updater):  </span><br><span class="line">    &quot;&quot;&quot;训练模型（定义见第3章）&quot;&quot;&quot;</span><br><span class="line">    animator = Animator(xlabel=&#x27;epoch&#x27;, xlim=[1, num_epochs], ylim=[0.3, 0.9],</span><br><span class="line">                        legend=[&#x27;train loss&#x27;, &#x27;train acc&#x27;, &#x27;test acc&#x27;])</span><br><span class="line">                        </span><br><span class="line">    for epoch in range(num_epochs):</span><br><span class="line">        train_metrics = train_epoch_ch3(net, train_iter, loss, updater)</span><br><span class="line">        test_acc = evaluate_accuracy(net, test_iter)</span><br><span class="line">        animator.add(epoch + 1, train_metrics + (test_acc,))</span><br><span class="line">    train_loss, train_acc = train_metrics</span><br><span class="line">    </span><br><span class="line">    assert train_loss &lt; 0.5, train_loss</span><br><span class="line">    assert train_acc &lt;= 1 and train_acc &gt; 0.7, train_acc</span><br><span class="line">    assert test_acc &lt;= 1 and test_acc &gt; 0.7, test_acc</span><br></pre></td></tr></table></figure></div>

<p>小批量随机梯度下降优化模型的损失函数</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">lr = 0.1</span><br><span class="line"></span><br><span class="line">def updater(batch_size):</span><br><span class="line">    return d2l.sgd([W, b], lr, batch_size)</span><br></pre></td></tr></table></figure></div>

<p>预测：</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">def predict_ch3(net, test_iter, n=6):  </span><br><span class="line">    &quot;&quot;&quot;预测标签（定义见第3章）&quot;&quot;&quot;</span><br><span class="line">    for X, y in test_iter:</span><br><span class="line">        break</span><br><span class="line">    trues = d2l.get_fashion_mnist_labels(y)</span><br><span class="line">    preds = d2l.get_fashion_mnist_labels(net(X).argmax(axis=1))</span><br><span class="line">    titles = [true +&#x27;\n&#x27; + pred for true, pred in zip(trues, preds)]</span><br><span class="line">    d2l.show_images(</span><br><span class="line">        X[0:n].reshape((n, 28, 28)), 1, n, titles=titles[0:n])</span><br><span class="line"></span><br><span class="line">predict_ch3(net, test_iter)</span><br></pre></td></tr></table></figure></div>

<p><a target="_blank" rel="noopener" href="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/blob/main/imgs/09/09-09.png"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/09/09-09.png"
                      alt="image"
                ></a></p>
<h3 id="softmax的简洁实现"><a href="#softmax的简洁实现" class="headerlink" title="softmax的简洁实现"></a>softmax的简洁实现</h3><blockquote>
<p>调用torch内的网络层</p>
</blockquote>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torch import nn</span><br><span class="line">from d2l import torch as d2l</span><br><span class="line"></span><br><span class="line">batch_size=256</span><br><span class="line">train_iter,test_iter=d2l.load_data_fashion_mnist(batch_size)</span><br><span class="line"># PyTorch不会隐式地调整输入的形状。因此，</span><br><span class="line"># 我们在线性层前定义了展平层（flatten），来调整网络输入的形状</span><br><span class="line">net=nn.Sequential(nn.Flatten(),nn.Linear(784,10))</span><br><span class="line"></span><br><span class="line">def init_weights(m):</span><br><span class="line">    if type(m) == nn.Linear:</span><br><span class="line">        nn.init.normal_(m.weight,std=0.01)</span><br><span class="line"></span><br><span class="line">net.apply(init_weights)</span><br><span class="line">## 在交叉熵损失函数中传递未归一化的预测，并同时计算softmax及其对数</span><br><span class="line">loss=nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line">trainer=torch.optim.SGD(net.parameters(),lr=0.1)</span><br><span class="line"></span><br><span class="line">num_epochs=10</span><br><span class="line">d2l.train_ch3(net,train_iter,test_iter,loss,num_epochs,trainer)</span><br></pre></td></tr></table></figure></div>



<h1 id="多层感知机"><a href="#多层感知机" class="headerlink" title="多层感知机"></a>多层感知机</h1><h2 id="多层感知机（MLP）"><a href="#多层感知机（MLP）" class="headerlink" title="多层感知机（MLP）"></a>多层感知机（MLP）</h2><h3 id="感知机"><a href="#感知机" class="headerlink" title="感知机"></a>感知机</h3><h4 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h4><p>从现在的观点来看，感知机实际上就是神经网络中的一个神经单元</p>
<p><a target="_blank" rel="noopener" href="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/blob/main/imgs/10/%E6%84%9F%E7%9F%A5%E6%9C%BA.png"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/10/%E6%84%9F%E7%9F%A5%E6%9C%BA.png"
                      alt="感知机"
                ></a></p>
<p>感知机能解决二分类问题，但与线性回归和softmax回归有所区别：线性回归与softmax回归的输出均为实数，softmax回归的输出同时还满足概率公理。</p>
<h4 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h4><p>训练感知机的伪代码如下：</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">initialize w = 0 and b = 0</span><br><span class="line">repeat</span><br><span class="line">    #此处表达式小于0代表预测结果错误，表示预测值与真实值同号</span><br><span class="line">    #等价batchsize为1的梯度下降</span><br><span class="line">    if y_i[&lt;w,x_i&gt;+b] &lt;= 0 then</span><br><span class="line">        w=w + yixi</span><br><span class="line">        b=b + yi</span><br><span class="line">    end if</span><br><span class="line">until all classified correctly</span><br></pre></td></tr></table></figure></div>

<p>可以看出这等价于使用如下损失函数的随机梯度下降（batch_size&#x3D;1）: $$ \ell(y,\bold x,\bold w)&#x3D;max(0,-y&lt;\bold w,\bold x&gt;)\ &#x3D;max(0,-y\bold w^T\bold x) $$ 当预测错误时，偏导数为 $$ \frac{\partial \ell}{\partial \bold w}&#x3D;-y\cdot \bold x $$</p>
<p>注：此处为了方便计算，将偏置项b归入w中的最后一维，并在特征x中相应的最后一维加入常数1</p>
<h4 id="收敛定理"><a href="#收敛定理" class="headerlink" title="收敛定理"></a>收敛定理</h4><p>设数据在特征空间能被半径为r的圆（球）覆盖，并且分类时有余量（即$\sigma$函数的输入不会取使输出模棱两可的值）$y(\bold x^T\bold w)\geq \rho$，若初始参数满足$|\bold w|^2+b^2 \leq 1$，则感知机保证在$\frac{r^2+1}{\rho ^2}$步内收敛</p>
<p><a class="link"   target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/46762820" >收敛性的证明 <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p>
<h4 id=""><a href="#" class="headerlink" title=""></a></h4><p>在前面的课程中我们学习了softmax回归，线性回归，他们有将输入向量与一个权重向量做内积再与一个偏置相加得到一个值的过程： $$ O &#x3D;W^TX+b $$ 这个过程被称为仿射变换，它是一个带有偏置项的线性变换，它最终产生的模型被称为线性模型，线性模型的特点是只能以线性的方式对特征空间进行划分：</p>
<p><a target="_blank" rel="noopener" href="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/blob/main/imgs/10/%E7%BA%BF%E6%80%A7%E5%88%92%E5%88%86.png"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/10/%E7%BA%BF%E6%80%A7%E5%88%92%E5%88%86.png"
                      alt="线性化分"
                ></a></p>
<p>然而，这种线性划分依赖于线性假设，是非常不可靠的</p>
<ul>
<li>线性假设意味着单调假设，这是不可靠的：<ul>
<li>对于人体的体温与健康情况的建模，人体在37℃时最为健康，过小过大均有风险，然而这不是单调的</li>
</ul>
</li>
<li>线性假设意味着特征与预测存在线性相关性，这也是不可靠的：<ul>
<li>如果预测一个人偿还债务的可能性，那这个人的资产从0万元增至5万元和从100万元增至105万元对应的偿还债务的可能性的增幅肯定是不相等的，也就是不线性相关的</li>
</ul>
</li>
<li>线性模型的评估标准是有位置依赖性的，这是不可靠的：<ul>
<li>如果需要判断图片中的动物是猫还是狗，对于图片中一个像素的权重的改变永远是不可靠的，因为如果将图片翻转，它的类别不会改变，但是线性模型不具备这种性质，像素的权重将会失效</li>
</ul>
</li>
</ul>
<p>课程中所提到的例子是XOR问题，即希望模型能预测出XOR分类（分割图片中的一三象限与二四象限）：</p>
<p><a target="_blank" rel="noopener" href="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/blob/main/imgs/10/XOR%E9%97%AE%E9%A2%98.png"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/10/XOR%E9%97%AE%E9%A2%98.png"
                      alt="XOR问题"
                ></a></p>
<h3 id="多层感知机-1"><a href="#多层感知机-1" class="headerlink" title="多层感知机"></a>多层感知机</h3><h4 id="XOR问题的多层次解决"><a href="#XOR问题的多层次解决" class="headerlink" title="XOR问题的多层次解决"></a>XOR问题的多层次解决</h4><p>仍以XOR问题为例，XOR问题的一个解决思路是分类两次，先按x轴分类为+和-，再按y轴分类为+和-，最后将两个分类结果相乘，+即为一三象限，-即为二四象限：</p>
<p><a target="_blank" rel="noopener" href="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/blob/main/imgs/10/%E5%A4%9A%E5%B1%82%E5%88%86%E7%B1%BBXOR1.png"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/10/%E5%A4%9A%E5%B1%82%E5%88%86%E7%B1%BBXOR1.png"
                      alt="多层分类XOR1"
                ></a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/blob/main/imgs/10/%E5%A4%9A%E5%B1%82%E5%88%86%E7%B1%BBXOR2.png"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/10/%E5%A4%9A%E5%B1%82%E5%88%86%E7%B1%BBXOR2.png"
                      alt="多层分类XOR2"
                ></a></p>
<p>这实际上将信息进行了多层次的传递：</p>
<p><a target="_blank" rel="noopener" href="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/blob/main/imgs/10/XOR%E4%BF%A1%E6%81%AF%E5%A4%9A%E5%B1%82%E6%AC%A1%E4%BC%A0%E9%80%92.png"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/10/XOR%E4%BF%A1%E6%81%AF%E5%A4%9A%E5%B1%82%E6%AC%A1%E4%BC%A0%E9%80%92.png"
                      alt="XOR信息多层次传递"
                ></a></p>
<p>其中蓝色为按X坐标的正负进行的分类，橙色为按Y坐标的正负进行的分类，灰色为将二者信息的综合，这就实现了用多层次的线性模型对非线性进行预测</p>
<h4 id="多层感知机-2"><a href="#多层感知机-2" class="headerlink" title="多层感知机"></a>多层感知机</h4><p>有了XOR问题的解决经验，可以想到如果将多个感知机堆叠起来，形成具有多个层次的结构，如图：</p>
<p><a target="_blank" rel="noopener" href="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/blob/main/imgs/10/%E5%8D%95%E9%9A%90%E8%97%8F%E5%B1%82.png"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/10/%E5%8D%95%E9%9A%90%E8%97%8F%E5%B1%82.png"
                      alt="单隐藏层"
                ></a></p>
<p>这里的模型称为多层感知机，第一层圆圈$x_1,x_2,x_3,x_4$称为输入（实际上他并非感知机），之后的一层称为隐藏层，由5个感知机构成，他们均以前一层的信息作为输入，最后是输出层，以前一层隐藏层的结果作为输入。除了输入的信息和最后一层的感知机以外，其余的层均称为隐藏层，隐藏层的设置为模型一个重要的超参数，这里的模型有一个隐藏层。</p>
<h4 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h4><p>但是仅仅有线性变换是不够的，如果我们简单的将多个线性变换按层次叠加，由于线性变换的结果仍为线性变换，所以最终的结果等价于线性变换，与单个感知机并无区别，反而加大了模型，浪费了资源，为了防止这个问题，需要对每个单元（感知机）的输出通过激活函数进行处理再交由下一层的感知机进行运算，这些激活函数就是解决非线性问题的关键。</p>
<p><em>激活函数</em>（activation function）通过计算加权和并加上偏置来确定神经元是否应该被激活，它们将输入信号转换为输出的可微运算。大多数激活函数都是非线性的。</p>
<p>主要的激活函数有：</p>
<h5 id="ReLU函数"><a href="#ReLU函数" class="headerlink" title="ReLU函数"></a>ReLU函数</h5><p>最受欢迎的激活函数是<em>修正线性单元</em>（Rectified linear unit，<em>ReLU</em>），因为它实现简单，同时在各种预测任务中表现良好。<strong>ReLU提供了一种非常简单的非线性变换</strong>。给定元素$x$，ReLU函数被定义为该元素与$0$的最大值： $$ \operatorname{ReLU}(x) &#x3D; \max(x, 0) $$ ReLU函数通过将相应的活性值设为0，仅保留正元素并丢弃所有负元素。为了直观感受一下，我们可以画出函数的曲线图。正如从图中所看到，激活函数是分段线性的。使用ReLU的原因是，它求导表现得特别好：要么让参数消失，要么让参数通过。这使得优化表现的更好，并且ReLU减轻了困扰以往神经网络的梯度消失问题</p>
<h5 id="sigmoid函数"><a href="#sigmoid函数" class="headerlink" title="sigmoid函数"></a>sigmoid函数</h5><p><strong>对于一个定义域在$\mathbb{R}$中的输入，*sigmoid函数*将输入变换为区间(0,1)上的输出</strong>。 因此，sigmoid通常称为<em>挤压函数</em>（squashing function）：它将范围$（-\infty, \infty）$中的任意输入压缩到区间（0,1）中的某个值： $$ \operatorname{sigmoid}(x) &#x3D; \frac{1}{1 + e^{-x}}. $$ 在基于梯度的学习中，sigmoid函数是一个自然的选择，因为它是一个平滑的、可微的阈值单元近似。当我们想要将输出视作二元分类问题的概率时，sigmoid仍然被广泛用作输出单元上的激活函数（你可以将sigmoid视为softmax的特例）。然而，sigmoid在隐藏层中已经较少使用，它在大部分时候被更简单、更容易训练的ReLU所取代。</p>
<h5 id="tanh函数"><a href="#tanh函数" class="headerlink" title="tanh函数"></a>tanh函数</h5><p>与sigmoid函数类似，<strong>tanh(双曲正切)函数也能将其输入压缩转换到区间(-1,1)上</strong>。tanh函数的公式如下： $$ \operatorname{tanh}(x) &#x3D; \frac{1 - e^{-2x}}{1 + e^{-2x}} $$</p>
<h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><h4 id="从零实现"><a href="#从零实现" class="headerlink" title="从零实现"></a>从零实现</h4><div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torch import nn</span><br><span class="line">from d2l import torch as d2l</span><br><span class="line"></span><br><span class="line">batch_size=256</span><br><span class="line">train_iter,test_iter = d2l.load_data_fashion_mnist(batch_size)</span><br></pre></td></tr></table></figure></div>

<p>实现一个具有单隐藏层的多层感知机，具有256个隐藏单元。</p>
<p>Fashion-MNIST中的每个图像由 28×28&#x3D;784个灰度像素值组成。 所有图像共分为10个类别。 忽略像素之间的空间结构， 我们可以将每个图像视为具有784个输入特征 和10个类的简单分类数据集。</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">num_inputs, num_outputs, num_hiddens = 784, 10, 256</span><br><span class="line"></span><br><span class="line">W1 = nn.Parameter(torch.radn(num_inputs,num_hiddens,requires_grad = True)* 0.01)</span><br><span class="line">b1 = nn.Parameter(torch.zeros(num_hiddens,requires_grad = True))</span><br><span class="line">W2 = nn.Parameter(torch.randn(num_hiddens,num_outputs,requires_grad = True)* 0.01)</span><br><span class="line">b2 = nn.Parameter(torch.zeros(num_outputs, requires_grad=True))</span><br><span class="line"></span><br><span class="line">params = [W1, b1, W2, b2]</span><br></pre></td></tr></table></figure></div>

<p>实现ReLU激活函数</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">def relu(X):</span><br><span class="line">    a = torch.zeros_like(X)</span><br><span class="line">    return torch.max(X, a)</span><br></pre></td></tr></table></figure></div>

<p>实现模型</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">def net(X):</span><br><span class="line">	X = X.reshape((-1,num_inputs))</span><br><span class="line">	H = relu(X@W1 + b1)</span><br><span class="line">	return (H@W2 + b2)</span><br></pre></td></tr></table></figure></div>

<p>损失函数 直接使用高级API中的内置函数来计算softmax和交叉熵损失</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss = nn.CrossEntropyLoss(reduction=&#x27;none&#x27;)</span><br></pre></td></tr></table></figure></div>

<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">num_epochs, lr = 10, 0.1</span><br><span class="line">updater = torch.optim.SGD(params, lr=lr)</span><br><span class="line">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, updater)</span><br></pre></td></tr></table></figure></div>

<h4 id="简化代码"><a href="#简化代码" class="headerlink" title="简化代码"></a>简化代码</h4><div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torch import nn</span><br><span class="line">from d2l import torch as d2l</span><br><span class="line"></span><br><span class="line">net = nn.Sequential(nn.Flatten(),</span><br><span class="line">                    nn.Linear(784, 256),</span><br><span class="line">                    nn.ReLU(),</span><br><span class="line">                    nn.Linear(256, 10))</span><br><span class="line"></span><br><span class="line">def init_weights(m):</span><br><span class="line">    if type(m) == nn.Linear:</span><br><span class="line">        nn.init.normal_(m.weight, std=0.01)</span><br><span class="line"></span><br><span class="line">net.apply(init_weights)</span><br><span class="line"></span><br><span class="line">batch_size, lr, num_epochs = 256, 0.1, 10</span><br><span class="line">loss = nn.CrossEntropyLoss(reduction=&#x27;none&#x27;)</span><br><span class="line">trainer = torch.optim.SGD(net.parameters(), lr=lr)</span><br><span class="line"></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)</span><br><span class="line">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)</span><br></pre></td></tr></table></figure></div>



<h2 id="模型选择-过拟合和欠拟合"><a href="#模型选择-过拟合和欠拟合" class="headerlink" title="模型选择 + 过拟合和欠拟合"></a>模型选择 + 过拟合和欠拟合</h2><h3 id="概念区分"><a href="#概念区分" class="headerlink" title="概念区分"></a>概念区分</h3><h4 id="训练误差与泛化误差"><a href="#训练误差与泛化误差" class="headerlink" title="训练误差与泛化误差"></a>训练误差与泛化误差</h4><p>训练误差基于训练集，是在训练数据上的误差</p>
<p>泛化误差基于测试集，是在新数据上的误差</p>
<h4 id="验证数据集与测试数据集"><a href="#验证数据集与测试数据集" class="headerlink" title="验证数据集与测试数据集"></a>验证数据集与测试数据集</h4><p>验证数据集：一个用来评估模型好坏的数据集</p>
<p>测试数据集：只用一次的数据集</p>
<h3 id="K-则交叉验证"><a href="#K-则交叉验证" class="headerlink" title="K-则交叉验证"></a>K-则交叉验证</h3><p>（在没有足够多数据时使用）</p>
<p>算法：</p>
<p>​	将训练数据分割成K块</p>
<p>​	使用第i块作为验证数据集，其余作为训练数据集</p>
<p>​	报告k个验证集误差的平均，常用k&#x3D;5&#x2F;10</p>
<h3 id="过拟合和欠拟合"><a href="#过拟合和欠拟合" class="headerlink" title="过拟合和欠拟合"></a>过拟合和欠拟合</h3><p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="C:/Users/Y9000p/AppData/Roaming/Typora/typora-user-images/image-20240702223819681.png"
                      alt="image-20240702223819681"
                ></p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="C:/Users/Y9000p/AppData/Roaming/Typora/typora-user-images/image-20240702224134621.png"
                      alt="image-20240702224134621"
                ></p>
<h4 id="估计模型容量"><a href="#估计模型容量" class="headerlink" title="估计模型容量"></a>估计模型容量</h4><h5 id="难以在不同的种类算法之间比较"><a href="#难以在不同的种类算法之间比较" class="headerlink" title="难以在不同的种类算法之间比较"></a>难以在不同的种类算法之间比较</h5><p>例如树模型和神经网络</p>
<h5 id="给定一个模型种类，将有两个主要因素"><a href="#给定一个模型种类，将有两个主要因素" class="headerlink" title="给定一个模型种类，将有两个主要因素"></a>给定一个模型种类，将有两个主要因素</h5><p>参数个数</p>
<p>参数值的选择范围</p>
<h4 id="数据复杂度"><a href="#数据复杂度" class="headerlink" title="数据复杂度"></a>数据复杂度</h4><p>样本个数</p>
<p>样本的元素个数</p>
<p>时间&#x2F;空间结构</p>
<p>多样性</p>
<h3 id="代码（多项式回归）"><a href="#代码（多项式回归）" class="headerlink" title="代码（多项式回归）"></a>代码（多项式回归）</h3><p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="C:/Users/Y9000p/AppData/Roaming/Typora/typora-user-images/image-20240702225202168.png"
                      alt="image-20240702225202168"
                ></p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">import math</span><br><span class="line">import numpy as np</span><br><span class="line">import torch</span><br><span class="line">from torch import nn</span><br><span class="line">from d2l import torch as d2l</span><br><span class="line"></span><br><span class="line">max_degree = 20  # 多项式的最大阶数</span><br><span class="line">n_train, n_test = 100, 100  # 训练和测试数据集大小</span><br><span class="line">true_w = np.zeros(max_degree)  # 分配大量的空间</span><br><span class="line">true_w[0:4] = np.array([5, 1.2, -3.4, 5.6])  #后面全是噪音</span><br><span class="line"></span><br><span class="line">## 构造数据集</span><br><span class="line">features = np.random.normal(size=(n_train + n_test, 1))  #随机正态分布生成数据与测试集</span><br><span class="line">np.random.shuffle(features)    #随机化</span><br><span class="line">poly_features = np.power(features, np.arange(max_degree).reshape(1, -1))   #生成幂指数</span><br><span class="line">for i in range(max_degree):</span><br><span class="line">    poly_features[:, i] /= math.gamma(i + 1)  # gamma(n)=(n-1)!</span><br><span class="line"># labels的维度:(n_train+n_test,)</span><br><span class="line">labels = np.dot(poly_features, true_w)</span><br><span class="line">labels += np.random.normal(scale=0.1, size=labels.shape)</span><br><span class="line"></span><br><span class="line"># NumPy ndarray转换为tensor</span><br><span class="line">true_w, features, poly_features, labels = [torch.tensor(x, dtype=</span><br><span class="line">    torch.float32) for x in [true_w, features, poly_features, labels]]</span><br></pre></td></tr></table></figure></div>

<p>对模型进行训练和测试</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">def evaluate_loss(net, data_iter, loss):  #@save</span><br><span class="line">    &quot;&quot;&quot;评估给定数据集上模型的损失&quot;&quot;&quot;</span><br><span class="line">    metric = d2l.Accumulator(2)  # 损失的总和,样本数量</span><br><span class="line">    for X, y in data_iter:</span><br><span class="line">        out = net(X)</span><br><span class="line">        y = y.reshape(out.shape)</span><br><span class="line">        l = loss(out, y)</span><br><span class="line">        metric.add(l.sum(), l.numel())</span><br><span class="line">    return metric[0] / metric[1]</span><br></pre></td></tr></table></figure></div>

<p><strong>定义训练函数</strong></p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">def train(train_features, test_features, train_labels, test_labels,num_epochs=400):</span><br><span class="line">    loss = nn.MSELoss(reduction=&#x27;none&#x27;)</span><br><span class="line">    input_shape = train_features.shape[-1]</span><br><span class="line">    # 不设置偏置，因为我们已经在多项式中实现了它</span><br><span class="line">    net = nn.Sequential(nn.Linear(input_shape, 1, bias=False))</span><br><span class="line">    batch_size = min(10, train_labels.shape[0])</span><br><span class="line">    train_iter = d2l.load_array((train_features, train_labels.reshape(-1,1)),batch_size)</span><br><span class="line">    test_iter = d2l.load_array((test_features, test_labels.reshape(-1,1)),batch_size, is_train=False)</span><br><span class="line">    trainer = torch.optim.SGD(net.parameters(), lr=0.01)</span><br><span class="line">    animator = d2l.Animator(xlabel=&#x27;epoch&#x27;, ylabel=&#x27;loss&#x27;, yscale=&#x27;log&#x27;,xlim=[1, num_epochs], ylim=[1e-3, 1e2],legend=[&#x27;train&#x27;, &#x27;test&#x27;])</span><br><span class="line">    for epoch in range(num_epochs):</span><br><span class="line">        d2l.train_epoch_ch3(net, train_iter, loss, trainer)</span><br><span class="line">        if epoch == 0 or (epoch + 1) % 20 == 0:</span><br><span class="line">            animator.add(epoch + 1, (evaluate_loss(net, train_iter, loss),evaluate_loss(net, test_iter, loss)))</span><br><span class="line">    print(&#x27;weight:&#x27;, net[0].weight.data.numpy())</span><br></pre></td></tr></table></figure></div>



<h2 id="权重衰退（正则化模型的技术）"><a href="#权重衰退（正则化模型的技术）" class="headerlink" title="权重衰退（正则化模型的技术）"></a>权重衰退（正则化模型的技术）</h2><p>在训练参数化机器学习模型时， <em>权重衰减</em>（weight decay）是最广泛使用的正则化的技术之一， 它通常也被称为𝐿2<em>正则化</em>。 这项技术通过函数与零的距离来衡量函数的复杂度， 因为在所有函数𝑓中，函数𝑓&#x3D;0（所有输入都得到值0） 在某种意义上是最简单的。</p>
<p>一种简单的方法是通过线性函数 𝑓(𝑥)&#x3D;𝑤⊤𝑥 中的权重向量的某个范数来度量其复杂性， 例如∥𝑤∥2。 要保证权重向量比较小， 最常用方法是将其范数作为惩罚项加到最小化损失的问题中。 将原来的训练目标<em>最小化训练标签上的预测损失</em>， 调整为<em>最小化预测损失和惩罚项之和</em>。 现在，如果我们的权重向量增长的太大， 我们的学习算法可能会更集中于最小化权重范数∥𝑤∥2。</p>
<h3 id="硬性限制-直观理解"><a href="#硬性限制-直观理解" class="headerlink" title="硬性限制&#x2F;直观理解"></a>硬性限制&#x2F;直观理解</h3><p><strong>使用均方范数作为硬性限制</strong></p>
<p>我们的优化目标仍然是<a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/c268c1ccd2378c3872e8678310498cca00c9fd8f26bfdb0ce3cf4e983d7a1d56/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e6c617465783f6d696e2535437370616365253543656c6c28253543626f6c64253742772537442c6229"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://camo.githubusercontent.com/c268c1ccd2378c3872e8678310498cca00c9fd8f26bfdb0ce3cf4e983d7a1d56/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e6c617465783f6d696e2535437370616365253543656c6c28253543626f6c64253742772537442c6229"
                      alt="img"
                ></a>，只是额外对<a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/ff9b452cab8da34e38960130e11bdb43c17681b4d715f35495b631be82f83c1d/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e6c617465783f253543626f6c6425374277253744"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://camo.githubusercontent.com/ff9b452cab8da34e38960130e11bdb43c17681b4d715f35495b631be82f83c1d/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e6c617465783f253543626f6c6425374277253744"
                      alt="img"
                ></a>添加一个限制条件<a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/60135f218d6eff45f9d2a2b226b09f667b1e5862c12b77709fe9fe000ce42fdf/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e6c617465783f253743253743253543626f6c6425374277253744253743253743253545322535436c6571736c616e742535437468657461"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://camo.githubusercontent.com/60135f218d6eff45f9d2a2b226b09f667b1e5862c12b77709fe9fe000ce42fdf/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e6c617465783f253743253743253543626f6c6425374277253744253743253743253545322535436c6571736c616e742535437468657461"
                      alt="img"
                ></a>，即权重的各项平方和小于一个特定的常数<a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/2ee9f22d00819650d1da57b8e92428653b1baaeb0a5efbd4f95ee745612fc9a7/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e6c617465783f2535437468657461"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://camo.githubusercontent.com/2ee9f22d00819650d1da57b8e92428653b1baaeb0a5efbd4f95ee745612fc9a7/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e6c617465783f2535437468657461"
                      alt="img"
                ></a>。那么设定一个较小的<a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/2ee9f22d00819650d1da57b8e92428653b1baaeb0a5efbd4f95ee745612fc9a7/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e6c617465783f2535437468657461"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://camo.githubusercontent.com/2ee9f22d00819650d1da57b8e92428653b1baaeb0a5efbd4f95ee745612fc9a7/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e6c617465783f2535437468657461"
                      alt="img"
                ></a>就会使得<a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/ff9b452cab8da34e38960130e11bdb43c17681b4d715f35495b631be82f83c1d/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e6c617465783f253543626f6c6425374277253744"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://camo.githubusercontent.com/ff9b452cab8da34e38960130e11bdb43c17681b4d715f35495b631be82f83c1d/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e6c617465783f253543626f6c6425374277253744"
                      alt="img"
                ></a>中每个元素的值都不会太大。</p>
<p>通常不会限制偏移b，理论上讲b表示整个数据在零点上的偏移，因此是不应该限制的，但实践中限制与否对结果都没什么影响。</p>
<p><strong>吴恩达课程中对这一现象的解释是w是高维向量，已经包 含了绝大多数参数足以表达高方差问题，b作为单个数字对结果的影响就会很小.</strong></p>
<p><strong>小的<a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/2ee9f22d00819650d1da57b8e92428653b1baaeb0a5efbd4f95ee745612fc9a7/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e6c617465783f2535437468657461"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://camo.githubusercontent.com/2ee9f22d00819650d1da57b8e92428653b1baaeb0a5efbd4f95ee745612fc9a7/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e6c617465783f2535437468657461"
                      alt="img"
                ></a>意味着更强的正则项</strong>，对于相同的<a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/2ee9f22d00819650d1da57b8e92428653b1baaeb0a5efbd4f95ee745612fc9a7/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e6c617465783f2535437468657461"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://camo.githubusercontent.com/2ee9f22d00819650d1da57b8e92428653b1baaeb0a5efbd4f95ee745612fc9a7/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e6c617465783f2535437468657461"
                      alt="img"
                ></a>，<a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/ff9b452cab8da34e38960130e11bdb43c17681b4d715f35495b631be82f83c1d/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e6c617465783f253543626f6c6425374277253744"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://camo.githubusercontent.com/ff9b452cab8da34e38960130e11bdb43c17681b4d715f35495b631be82f83c1d/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e6c617465783f253543626f6c6425374277253744"
                      alt="img"
                ></a>中元素越多则单个元素的值会越小。</p>
<h3 id="柔性限制-实际应用"><a href="#柔性限制-实际应用" class="headerlink" title="柔性限制&#x2F;实际应用"></a>柔性限制&#x2F;实际应用</h3><p>上文说的硬性限制在实际使用时比较麻烦，实际上常用的函数是</p>
<p><a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/aca2480d2e63e33df193ee69d1869f501c3cc3e4123bee06c3510ca892890098/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e6c617465783f6d696e2535437370616365253543656c6c28253543626f6c64253742772537442c62292b253543667261632537422535436c616d62646125374425374232253744253743253743253543626f6c642537427725374425374325374325354532"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://camo.githubusercontent.com/aca2480d2e63e33df193ee69d1869f501c3cc3e4123bee06c3510ca892890098/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e6c617465783f6d696e2535437370616365253543656c6c28253543626f6c64253742772537442c62292b253543667261632537422535436c616d62646125374425374232253744253743253743253543626f6c642537427725374425374325374325354532"
                      alt="img"
                ></a></p>
<p>可以通过拉格朗日乘子证明对于每个<a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/2ee9f22d00819650d1da57b8e92428653b1baaeb0a5efbd4f95ee745612fc9a7/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e6c617465783f2535437468657461"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://camo.githubusercontent.com/2ee9f22d00819650d1da57b8e92428653b1baaeb0a5efbd4f95ee745612fc9a7/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e6c617465783f2535437468657461"
                      alt="img"
                ></a>都可以找到<a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/2ec5723e26bb86479707648ec0b1b1f9183674b99e31bcce5cbe2835d1e7f9e1/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e6c617465783f2535436c616d626461"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://camo.githubusercontent.com/2ec5723e26bb86479707648ec0b1b1f9183674b99e31bcce5cbe2835d1e7f9e1/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e6c617465783f2535436c616d626461"
                      alt="img"
                ></a>使得硬性限制的目标函数等价于上式。</p>
<p>其中<a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/db89c02a9c53b868fba4f1c0c9e4e7d986adeb498d5324dcb072189d24ad3904/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e6c617465783f253543667261632537422535436c616d62646125374425374232253744253743253743253543626f6c642537427725374425374325374325354532"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://camo.githubusercontent.com/db89c02a9c53b868fba4f1c0c9e4e7d986adeb498d5324dcb072189d24ad3904/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e6c617465783f253543667261632537422535436c616d62646125374425374232253744253743253743253543626f6c642537427725374425374325374325354532"
                      alt="img"
                ></a>这一项被称为罚(penalty)，<a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/2ec5723e26bb86479707648ec0b1b1f9183674b99e31bcce5cbe2835d1e7f9e1/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e6c617465783f2535436c616d626461"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://camo.githubusercontent.com/2ec5723e26bb86479707648ec0b1b1f9183674b99e31bcce5cbe2835d1e7f9e1/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e6c617465783f2535436c616d626461"
                      alt="img"
                ></a><strong>是超参数，控制了正则项的重要程度</strong>。</p>
<p>当<a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/bc056ceedf61585d3b6a3f773ef4edf48c1311f60f335d60d30ba6060270c40c/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e6c617465783f2535436c616d6264613d30"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://camo.githubusercontent.com/bc056ceedf61585d3b6a3f773ef4edf48c1311f60f335d60d30ba6060270c40c/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e6c617465783f2535436c616d6264613d30"
                      alt="img"
                ></a>时无作用，<a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/61cf8e5887264e92d223f8e3044131588ab79b6276c3c73935362d73f6fa9cc3/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e6c617465783f2535436c616d62646125354372696768746172726f77253543696e667479"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://camo.githubusercontent.com/61cf8e5887264e92d223f8e3044131588ab79b6276c3c73935362d73f6fa9cc3/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e6c617465783f2535436c616d62646125354372696768746172726f77253543696e667479"
                      alt="img"
                ></a>时最优解<a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/5deec64969e5a58e2d4259232575940bfbbdf093a0472cea94e2a5305d3739ba/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e6c617465783f253543626f6c64253742772537442535452a25354372696768746172726f7730"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://camo.githubusercontent.com/5deec64969e5a58e2d4259232575940bfbbdf093a0472cea94e2a5305d3739ba/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e6c617465783f253543626f6c64253742772537442535452a25354372696768746172726f7730"
                      alt="img"
                ></a>，也就是说<a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/2ec5723e26bb86479707648ec0b1b1f9183674b99e31bcce5cbe2835d1e7f9e1/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e6c617465783f2535436c616d626461"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://camo.githubusercontent.com/2ec5723e26bb86479707648ec0b1b1f9183674b99e31bcce5cbe2835d1e7f9e1/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e6c617465783f2535436c616d626461"
                      alt="img"
                ></a>越大模型复杂度就被控制的越低。</p>
<p><a target="_blank" rel="noopener" href="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/blob/main/imgs/12/12-01.JPG"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/12/12-01.JPG"
                      alt="12-01"
                ></a></p>
<p>以<a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/ff9b452cab8da34e38960130e11bdb43c17681b4d715f35495b631be82f83c1d/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e6c617465783f253543626f6c6425374277253744"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://camo.githubusercontent.com/ff9b452cab8da34e38960130e11bdb43c17681b4d715f35495b631be82f83c1d/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e6c617465783f253543626f6c6425374277253744"
                      alt="img"
                ></a>中只有两个参数为例，其中绿色的部分是原本损失函数函数值的“等高线”，黄色部分可以看作是正则项对应函数值的“等高线” ，使用权重衰减后需要优化的损失函数相当于图中两组等高线叠加。原本最优解位于绿色中心，现在这一位置在对于正则项有很高的损失，而正则项最小值位于原点，因此现在的最终优化解会更靠近原点，而当所有参数都更靠近原点时模型的规模也就更小。</p>
<p>（我们用参数值的范围来衡量模型的大小）</p>
<h3 id="参数更新"><a href="#参数更新" class="headerlink" title="参数更新"></a>参数更新</h3><h4 id="计算梯度"><a href="#计算梯度" class="headerlink" title="计算梯度"></a>计算梯度</h4><p><a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/816e5c55c47c5414a4ddafeae7feaccbd6446371d3719dcd4bb12b53196747da/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e6c617465783f253543667261632537422535437061727469616c2537422537442537442537422535437061727469616c253742253543626f6c642537427725374425374425374428253543656c6c28253543626f6c64253742772537442c62292b253543667261632537422535436c616d62646125374425374232253744253743253743253543626f6c642537427725374425374325374325354532293d253543667261632537422535437061727469616c253742253543656c6c28253543626f6c64253742772537442c62292537442537442537422537422535437061727469616c253742253543626f6c64253742772537442537442537442537442b2535436c616d626461253543626f6c6425374277253744"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://camo.githubusercontent.com/816e5c55c47c5414a4ddafeae7feaccbd6446371d3719dcd4bb12b53196747da/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e6c617465783f253543667261632537422535437061727469616c2537422537442537442537422535437061727469616c253742253543626f6c642537427725374425374425374428253543656c6c28253543626f6c64253742772537442c62292b253543667261632537422535436c616d62646125374425374232253744253743253743253543626f6c642537427725374425374325374325354532293d253543667261632537422535437061727469616c253742253543656c6c28253543626f6c64253742772537442c62292537442537442537422537422535437061727469616c253742253543626f6c64253742772537442537442537442537442b2535436c616d626461253543626f6c6425374277253744"
                      alt="img"
                ></a>、</p>
<h4 id="更新参数"><a href="#更新参数" class="headerlink" title="更新参数"></a>更新参数</h4><p>将上式结果带入更新参数公式整理可得</p>
<p><a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/221e250a978f50f162533c2a1f5bc34e70bfd1751255e95b1c11eaf4bfdec1a1/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e6c617465783f253543626f6c64253742772537445f253742742b312537443d28312d2535436574612535436c616d62646129253543626f6c64253742772537445f253742742537442d253543657461253543667261632537422535437061727469616c253742253543656c6c28253543626f6c64253742772537445f742c625f74292537442537442537422537422535437061727469616c253742253543626f6c64253742772537445f25374274253744253744253744253744"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://camo.githubusercontent.com/221e250a978f50f162533c2a1f5bc34e70bfd1751255e95b1c11eaf4bfdec1a1/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e6c617465783f253543626f6c64253742772537445f253742742b312537443d28312d2535436574612535436c616d62646129253543626f6c64253742772537445f253742742537442d253543657461253543667261632537422535437061727469616c253742253543656c6c28253543626f6c64253742772537445f742c625f74292537442537442537422537422535437061727469616c253742253543626f6c64253742772537445f25374274253744253744253744253744"
                      alt="img"
                ></a></p>
<p>注意到这个公式中后一项与原来更新参数的公式没有区别，仅仅是在前一项<a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/779be9c1d70105fed3cfa9549c24c591ab988cf098abfce4af42c0d069c64c18/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e6c617465783f253543626f6c64253742772537445f25374274253744"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://camo.githubusercontent.com/779be9c1d70105fed3cfa9549c24c591ab988cf098abfce4af42c0d069c64c18/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e6c617465783f253543626f6c64253742772537445f25374274253744"
                      alt="img"
                ></a> 上加了一个系数<a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/36e26287fd68d846aa72c6befe07fbaf4130e11a2a7a5a45c8eb4fe552993054/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e6c617465783f28312d2535436574612535436c616d62646129"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://camo.githubusercontent.com/36e26287fd68d846aa72c6befe07fbaf4130e11a2a7a5a45c8eb4fe552993054/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e6c617465783f28312d2535436574612535436c616d62646129"
                      alt="img"
                ></a>。通常<a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/4c3bff3b6dcd9ffc7a7122b114f60f2f3687b910cebc2f00f751b30a4071f11d/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e6c617465783f2535436574612535436c616d62646125334331"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://camo.githubusercontent.com/4c3bff3b6dcd9ffc7a7122b114f60f2f3687b910cebc2f00f751b30a4071f11d/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e6c617465783f2535436574612535436c616d62646125334331"
                      alt="img"
                ></a> ，也就是说由于引入了<a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/2ec5723e26bb86479707648ec0b1b1f9183674b99e31bcce5cbe2835d1e7f9e1/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e6c617465783f2535436c616d626461"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://camo.githubusercontent.com/2ec5723e26bb86479707648ec0b1b1f9183674b99e31bcce5cbe2835d1e7f9e1/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e6c617465783f2535436c616d626461"
                      alt="img"
                ></a>，每次更新参数前先给待更新参数乘上一个小于1的权重再更新，权重衰退由此得名。</p>
<p>（由于lambda的引入，我们每次的w都会先进行衰退再减去学习率*步长）</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul>
<li>权重衰退通过L2正则项使得模型参数不会过大，从而控制模型复杂度</li>
<li>正则项权重（<a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/2ec5723e26bb86479707648ec0b1b1f9183674b99e31bcce5cbe2835d1e7f9e1/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e6c617465783f2535436c616d626461"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://camo.githubusercontent.com/2ec5723e26bb86479707648ec0b1b1f9183674b99e31bcce5cbe2835d1e7f9e1/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e6c617465783f2535436c616d626461"
                      alt="img"
                ></a>）是控制模型复杂度的超参数</li>
</ul>
<h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><p>生成数据</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="C:/Users/Y9000p/AppData/Roaming/Typora/typora-user-images/image-20240703012706882.png"
                      alt="image-20240703012706882"
                ></p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torch import nn</span><br><span class="line">from d2l import torch as d2l</span><br><span class="line"></span><br><span class="line">n_train, n_test, num_inputs, batch_size = 20, 100, 200, 5</span><br><span class="line">true_w, true_b = torch.ones((num_inputs, 1)) * 0.01, 0.05</span><br><span class="line">train_data = d2l.synthetic_data(true_w, true_b, n_train)</span><br><span class="line">train_iter = d2l.load_array(train_data, batch_size)</span><br><span class="line">test_data = d2l.synthetic_data(true_w, true_b, n_test)</span><br><span class="line">test_iter = d2l.load_array(test_data, batch_size, is_train=False)</span><br></pre></td></tr></table></figure></div>

<p>初始化模型参数</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">def init_params():</span><br><span class="line">	w = torch.normal(0, 1, size=(num_inputs, 1),											requires_grad=True)</span><br><span class="line">	b = torch.zeros(1,requires_grad=True)</span><br><span class="line">	return [w,b]</span><br></pre></td></tr></table></figure></div>

<p>定义L2范数</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">def L2_penalty():</span><br><span class="line">	return torch.sum(w.pow(2))/2</span><br></pre></td></tr></table></figure></div>

<p>定义训练代码实现</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">def train(lambd):</span><br><span class="line">    w, b = init_params()</span><br><span class="line">    net, loss = lambda X: d2l.linreg(X, w, b), d2l.squared_loss</span><br><span class="line">    num_epochs, lr = 100, 0.003</span><br><span class="line">    animator = d2l.Animator(xlabel=&#x27;epochs&#x27;, ylabel=&#x27;loss&#x27;, yscale=&#x27;log&#x27;, xlim=[5, num_epochs], legend=[&#x27;train&#x27;, &#x27;test&#x27;])</span><br><span class="line">    for epoch in range(num_epochs):</span><br><span class="line">        for X, y in train_iter:</span><br><span class="line">            # 增加了L2范数惩罚项，</span><br><span class="line">            # 广播机制使l2_penalty(w)成为一个长度为batch_size的向量</span><br><span class="line">            l = loss(net(X), y) + lambd * l2_penalty(w)</span><br><span class="line">            l.sum().backward()</span><br><span class="line">            d2l.sgd([w, b], lr, batch_size)</span><br><span class="line">        if (epoch + 1) % 5 == 0:</span><br><span class="line">            animator.add(epoch + 1, (d2l.evaluate_loss(net, train_iter, loss),d2l.evaluate_loss(net, test_iter, loss)))</span><br><span class="line">    print(&#x27;w的L2范数是：&#x27;, torch.norm(w).item())</span><br><span class="line">	</span><br></pre></td></tr></table></figure></div>

<p>简介实现</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">def train_concise(wd):</span><br><span class="line">    net = nn.Sequential(nn.Linear(num_inputs, 1))</span><br><span class="line">    for param in net.parameters():</span><br><span class="line">        param.data.normal_()</span><br><span class="line">    loss = nn.MSELoss(reduction=&#x27;none&#x27;)</span><br><span class="line">    num_epochs, lr = 100, 0.003</span><br><span class="line">    # 偏置参数没有衰减</span><br><span class="line">    trainer = torch.optim.SGD([</span><br><span class="line">        &#123;&quot;params&quot;:net[0].weight,&#x27;weight_decay&#x27;: wd&#125;,</span><br><span class="line">        &#123;&quot;params&quot;:net[0].bias&#125;], lr=lr)</span><br><span class="line">    animator = d2l.Animator(xlabel=&#x27;epochs&#x27;, ylabel=&#x27;loss&#x27;, yscale=&#x27;log&#x27;,</span><br><span class="line">                            xlim=[5, num_epochs], legend=[&#x27;train&#x27;, &#x27;test&#x27;])</span><br><span class="line">    for epoch in range(num_epochs):</span><br><span class="line">        for X, y in train_iter:</span><br><span class="line">            trainer.zero_grad()</span><br><span class="line">            l = loss(net(X), y)</span><br><span class="line">            l.mean().backward()</span><br><span class="line">            trainer.step()</span><br><span class="line">        if (epoch + 1) % 5 == 0:</span><br><span class="line">            animator.add(epoch + 1,</span><br><span class="line">                         (d2l.evaluate_loss(net, train_iter, loss),</span><br><span class="line">                          d2l.evaluate_loss(net, test_iter, loss)))</span><br><span class="line">    print(&#x27;w的L2范数：&#x27;, net[0].weight.norm().item())</span><br></pre></td></tr></table></figure></div>



<h2 id="丢弃法"><a href="#丢弃法" class="headerlink" title="丢弃法"></a>丢弃法</h2><h3 id="丢弃法动机、实现及原则"><a href="#丢弃法动机、实现及原则" class="headerlink" title="丢弃法动机、实现及原则"></a>丢弃法动机、实现及原则</h3><h4 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h4><ul>
<li>一个好的模型需要对输入数据的扰动鲁棒（健壮性）</li>
</ul>
<h4 id="-1"><a href="#-1" class="headerlink" title=" "></a><a target="_blank" rel="noopener" href="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/blob/main/imgs/13/13-02.jpg"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/13/13-02.jpg"
                      alt="13-02"
                ></a> <a target="_blank" rel="noopener" href="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/blob/main/imgs/13/13-03.jpg"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/13/13-03.jpg"
                      alt="13-03"
                ></a></h4><h4 id="如何实现模型的这一能力"><a href="#如何实现模型的这一能力" class="headerlink" title="如何实现模型的这一能力"></a>如何实现模型的这一能力</h4><ul>
<li>使用有噪音的数据。</li>
<li>丢弃法：在层之间加入噪音。</li>
</ul>
<h4 id="加入噪音的原则"><a href="#加入噪音的原则" class="headerlink" title="加入噪音的原则"></a>加入噪音的原则</h4><p>无偏差的加入噪音</p>
<p><a target="_blank" rel="noopener" href="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/blob/main/imgs/13/13-01.png"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/13/13-01.png"
                      alt="13-01"
                ></a></p>
<ul>
<li>例如模型的功能是识别猫猫，加入噪音可以是输入模糊的猫猫图片，但尽量不要是狗狗的图片。</li>
</ul>
<h3 id="丢弃法内容"><a href="#丢弃法内容" class="headerlink" title="丢弃法内容"></a>丢弃法内容</h3><ul>
<li>丢弃法对每个元素作如下扰动</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/blob/main/imgs/13/13-04.png"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/13/13-04.png"
                      alt="13-04"
                ></a></p>
<ul>
<li>能够满足加入噪音的期望相同原则</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/blob/main/imgs/13/13-05.png"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/13/13-05.png"
                      alt="13-05"
                ></a></p>
<h3 id="丢弃法使用"><a href="#丢弃法使用" class="headerlink" title="丢弃法使用"></a>丢弃法使用</h3><h4 id="丢弃法的使用位置"><a href="#丢弃法的使用位置" class="headerlink" title="丢弃法的使用位置"></a>丢弃法的使用位置</h4><p>通常将丢弃法作用在隐藏全连接层的输出上</p>
<p><a target="_blank" rel="noopener" href="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/blob/main/imgs/13/13-06.png"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/13/13-06.png"
                      alt="13-06"
                ></a></p>
<p>随机选中某些神经元将其输出置位0，因此模型不会过分依赖某些神经元</p>
<p><a target="_blank" rel="noopener" href="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/blob/main/imgs/13/13-07.png"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/13/13-07.png"
                      alt="13-07"
                ></a></p>
<h4 id="训练中的丢弃法"><a href="#训练中的丢弃法" class="headerlink" title="训练中的丢弃法"></a>训练中的丢弃法</h4><ul>
<li>正则项（丢弃法）仅在训练中使用：影响模型参数的更新，预测的时候便不再使用</li>
</ul>
<h3 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h3><ul>
<li>丢弃法将一些输出项随机置0来控制模型复杂度</li>
<li>常作用在多层感知机的隐藏层输出上</li>
<li>丢弃概率是控制模型复杂度的超参数（常取0.9，0.5，0.1）</li>
</ul>
<h3 id="代码部分-1"><a href="#代码部分-1" class="headerlink" title="代码部分"></a>代码部分</h3><h4 id="Drpout部分"><a href="#Drpout部分" class="headerlink" title="Drpout部分"></a>Drpout部分</h4><div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torch import nn</span><br><span class="line">from d2l import torch as d2l</span><br><span class="line"></span><br><span class="line"># X为dropout层的输入，dropout为设置的丢弃概率</span><br><span class="line">def dropout_layer (X,dropout)：  </span><br><span class="line">    assert 0&lt;=dropout&lt;=1        #丢弃概率介于0，1之间</span><br><span class="line">    if dropout == 1:</span><br><span class="line">       return torch.zeros_like(x) #若丢弃概率为1，则X的全部项均被置0</span><br><span class="line">    if dropout == 0:</span><br><span class="line">       return X                   #若丢弃概率为0，不对X作丢弃操作，直接返回X</span><br><span class="line">       </span><br><span class="line">    mask=(torch.randn(X.shape)&gt;dropout).float() </span><br><span class="line">    return mask*X/(1-dropout) </span><br><span class="line">    #将mask与X相乘实现丢弃操作，并除以(1-dropout)，</span><br><span class="line">    这里不使用选中X中元素置0的原因是相乘操作相比选中操作更快</span><br></pre></td></tr></table></figure></div>

<h4 id="在神经网络中使用丢弃法"><a href="#在神经网络中使用丢弃法" class="headerlink" title="在神经网络中使用丢弃法"></a>在神经网络中使用丢弃法</h4><div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">num_inputs, num_outputs, num_hiddens1, num_hiddens2 = 784, 10, 256, 256</span><br><span class="line"></span><br><span class="line">dropout1, dropout2 = 0.2, 0.5</span><br><span class="line"></span><br><span class="line">class Net(nn.Module)</span><br><span class="line">    # 确认是在训练阶段</span><br><span class="line">    def _init_(self,num_inputs,num_outputs,</span><br><span class="line">    num_outputs,num_hiddens1,num_hiddens2,is_training=True):</span><br><span class="line">       super(Net,self)._init_()</span><br><span class="line">       self.num_inputs=num_inputs</span><br><span class="line">       self.training=is_training</span><br><span class="line">       self.lin1=nn.Linear(num_inputs,num_hiddens1)</span><br><span class="line">       self.lin2=nn.Linear(num_hiddens1,num_hiddens2)</span><br><span class="line">       self.lin2=nn.Linear(num_hiddens2,num_outputs)</span><br><span class="line">       self.relu=nn.ReLU()</span><br><span class="line">    </span><br><span class="line">    def forward(self,X):</span><br><span class="line">      H1=self.relu(self.lin1(X.reshape((-1,self.num_inputs))))</span><br><span class="line">       if self.training == True:  #丢弃法仅在训练中使用</span><br><span class="line">           H1=dropout_layer(H1,dropout1)</span><br><span class="line">       H2=self.relu(self.lin2(H1))</span><br><span class="line">       if self.training == True: #丢弃法仅在训练中使用</span><br><span class="line">           H2=dropout_layer(H2,dropout2)</span><br><span class="line">       out=self.lin3(H2)  #output层不再使用丢弃法</span><br><span class="line">       return out</span><br></pre></td></tr></table></figure></div>

<p>简洁实现</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(nn.Flatten(),</span><br><span class="line">        nn.Linear(784, 256),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        # 在第一个全连接层之后添加一个dropout层</span><br><span class="line">        nn.Dropout(dropout1),</span><br><span class="line">        nn.Linear(256, 256),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        # 在第二个全连接层之后添加一个dropout层</span><br><span class="line">        nn.Dropout(dropout2),</span><br><span class="line">        nn.Linear(256, 10))</span><br><span class="line"></span><br><span class="line">def init_weights(m):</span><br><span class="line">    if type(m) == nn.Linear:</span><br><span class="line">        nn.init.normal_(m.weight, std=0.01)</span><br><span class="line"></span><br><span class="line">net.apply(init_weights);</span><br></pre></td></tr></table></figure></div>



<h2 id="数值稳定性-模型初始化和激活函数"><a href="#数值稳定性-模型初始化和激活函数" class="headerlink" title="数值稳定性 + 模型初始化和激活函数"></a>数值稳定性 + 模型初始化和激活函数</h2><h3 id="数值稳定性"><a href="#数值稳定性" class="headerlink" title="数值稳定性"></a>数值稳定性</h3><p>数值稳定性是深度学习中比较重要的点，特别是当神经网络变得很深的时候，数值通常很容易变得不稳定。</p>
<h4 id="神经网络的梯度"><a href="#神经网络的梯度" class="headerlink" title="神经网络的梯度"></a>神经网络的梯度</h4><p><a target="_blank" rel="noopener" href="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/blob/main/imgs/14/14-01.png"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/14/14-01.png"
                      alt="image"
                ></a></p>
<p><strong>考虑d层神经网络</strong></p>
<ul>
<li>t表示层数，<a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/a71cebb102ebd87361c6309af0486f84cf349a20dec37eb3f2cae5bc0b90b22a/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e696d6167653f685e7b742d317d"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://camo.githubusercontent.com/a71cebb102ebd87361c6309af0486f84cf349a20dec37eb3f2cae5bc0b90b22a/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e696d6167653f685e7b742d317d"
                      alt="h^{t-1}"
                ></a>表示第<em>t-1</em>层的输出，经过一个<a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/ea7eda19f58ecfb0aa08ee4f8aa706c973834f1d814cfd6cc862932921b2ea4b/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e696d6167653f665f7b747d"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://camo.githubusercontent.com/ea7eda19f58ecfb0aa08ee4f8aa706c973834f1d814cfd6cc862932921b2ea4b/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e696d6167653f665f7b747d"
                      alt="f_{t}"
                ></a>函数后，得到第<em>t</em>层的输出。</li>
<li>最终输出y的表示：输入x经过若干层(<em>d</em>层)的函数作用，最后被损失函数作用得到输出y。</li>
</ul>
<p><strong>计算损失函数*L*关于第*t*层参数<a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/cd456da509cd823ff05c8a8147686bebe7f70b0479d17469c343f39a9cc2a8d7/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e696d6167653f575f7b747d2673706163653b"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://camo.githubusercontent.com/cd456da509cd823ff05c8a8147686bebe7f70b0479d17469c343f39a9cc2a8d7/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e696d6167653f575f7b747d2673706163653b"
                      alt="W_{t} "
                ></a>的梯度</strong></p>
<ul>
<li>由链导法则得到上图中乘积公式</li>
<li>需要进行d-t次<strong>矩阵乘法</strong>（为什么是矩阵乘法？答：由于所有的<em>h</em>都是一些<strong>向量</strong>，导数中分子分母均为向量，所以求导得到的是矩阵，维数为[分子维度]x[分母维度]，可以参考第6节<a class="link"   target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1eZ4y1w7PY" >视频 <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a>和<a class="link"   target="_blank" rel="noopener" href="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/blob/main/notes/06-%E7%9F%A9%E9%98%B5%E8%AE%A1%E7%AE%97.md" >笔记 <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a>）。这也是导致数值稳定性问题的<strong>主要因素</strong>，由于做了太多次的矩阵乘法。</li>
</ul>
<h4 id="数值稳定性的常见两个问题"><a href="#数值稳定性的常见两个问题" class="headerlink" title="数值稳定性的常见两个问题"></a>数值稳定性的常见两个问题</h4><p><strong>梯度爆炸</strong></p>
<p>假设梯度都是一些比1大的数比如1.5，做100次乘积之后得到<a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/cb03a67eca0aab939a881ca6e61c07bfa0c7a014080a832c6ef22dba43c45436/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e696d6167653f345c74696d65732673706163653b31305e7b31377d"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://camo.githubusercontent.com/cb03a67eca0aab939a881ca6e61c07bfa0c7a014080a832c6ef22dba43c45436/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e696d6167653f345c74696d65732673706163653b31305e7b31377d"
                      alt="4\times 10^{17}"
                ></a>，这个数字很容易带来一些浮点数上限的问题（需了解更多请参考计算机系统-计算机中浮点数的存储方式）。</p>
<p><strong>梯度消失</strong></p>
<p>假设梯度都是一些比1小的数比如0.8，做100次乘积之后得到<a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/a231a5a2891622e8dc7f0d3a45b8e4c5fc159338ba446402166557ea948d16e4/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e696d6167653f325c74696d657331305e7b2d31307d"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://camo.githubusercontent.com/a231a5a2891622e8dc7f0d3a45b8e4c5fc159338ba446402166557ea948d16e4/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e696d6167653f325c74696d657331305e7b2d31307d"
                      alt="2\times10^{-10}"
                ></a>，也可能会带来浮点数下溢的问题。</p>
<h4 id="例子：MLP"><a href="#例子：MLP" class="headerlink" title="例子：MLP"></a>例子：MLP</h4><p>此处我们着重探讨<a class="link"   target="_blank" rel="noopener" href="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/blob/main/notes/14-%E6%95%B0%E5%80%BC%E7%A8%B3%E5%AE%9A%E6%80%A7.md#11-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%A2%AF%E5%BA%A6" >1.1节 <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a>中所述的求梯度时所做的d-t次矩阵乘法，并以一个实例MLP来探讨其结果的具体形式。</p>
<p><a target="_blank" rel="noopener" href="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/blob/main/imgs/14/14-02.png"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/14/14-02.png"
                      alt="image"
                ></a></p>
<ul>
<li><p>第一行公式，定义<a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/be807542e1881c650372a6b6555cb672468be104bb488370e96db8ac22417766/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e696d6167653f685e7b747d"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://camo.githubusercontent.com/be807542e1881c650372a6b6555cb672468be104bb488370e96db8ac22417766/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e696d6167653f685e7b747d"
                      alt="h^{t}"
                ></a>和<a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/a71cebb102ebd87361c6309af0486f84cf349a20dec37eb3f2cae5bc0b90b22a/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e696d6167653f685e7b742d317d"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://camo.githubusercontent.com/a71cebb102ebd87361c6309af0486f84cf349a20dec37eb3f2cae5bc0b90b22a/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e696d6167653f685e7b742d317d"
                      alt="h^{t-1}"
                ></a>(均为向量)的函数关系<a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/ea7eda19f58ecfb0aa08ee4f8aa706c973834f1d814cfd6cc862932921b2ea4b/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e696d6167653f665f7b747d"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://camo.githubusercontent.com/ea7eda19f58ecfb0aa08ee4f8aa706c973834f1d814cfd6cc862932921b2ea4b/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e696d6167653f665f7b747d"
                      alt="f_{t}"
                ></a>，第t层的权重矩阵作用于t-1层的输出<a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/a71cebb102ebd87361c6309af0486f84cf349a20dec37eb3f2cae5bc0b90b22a/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e696d6167653f685e7b742d317d"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://camo.githubusercontent.com/a71cebb102ebd87361c6309af0486f84cf349a20dec37eb3f2cae5bc0b90b22a/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e696d6167653f685e7b742d317d"
                      alt="h^{t-1}"
                ></a>后经过激活函数<a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/ebf39e7aad643a5167823309290a6aafec495508d69747797eb052e48131fded/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e696d6167653f5c7369676d612673706163653b"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://camo.githubusercontent.com/ebf39e7aad643a5167823309290a6aafec495508d69747797eb052e48131fded/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e696d6167653f5c7369676d612673706163653b"
                      alt="\sigma "
                ></a>得到<a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/be807542e1881c650372a6b6555cb672468be104bb488370e96db8ac22417766/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e696d6167653f685e7b747d"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://camo.githubusercontent.com/be807542e1881c650372a6b6555cb672468be104bb488370e96db8ac22417766/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e696d6167653f685e7b747d"
                      alt="h^{t}"
                ></a>，注意激活函数<a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/ebf39e7aad643a5167823309290a6aafec495508d69747797eb052e48131fded/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e696d6167653f5c7369676d612673706163653b"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://camo.githubusercontent.com/ebf39e7aad643a5167823309290a6aafec495508d69747797eb052e48131fded/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e696d6167653f5c7369676d612673706163653b"
                      alt="\sigma "
                ></a>逐元素计算。</p>
</li>
<li><p>第二行公式：这里用到链导法则，激活函数<a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/ebf39e7aad643a5167823309290a6aafec495508d69747797eb052e48131fded/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e696d6167653f5c7369676d612673706163653b"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://camo.githubusercontent.com/ebf39e7aad643a5167823309290a6aafec495508d69747797eb052e48131fded/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e696d6167653f5c7369676d612673706163653b"
                      alt="\sigma "
                ></a>先对内部向量逐元素求导，然后把求导后这个向量变成对角矩阵（可以理解为链导法则中内部向量<a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/2e6701f04d89ef9b5335964cbec64476b9ddadb5b6ca6bc64d73b2b8009f06c9/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e696d6167653f575f7b747d685f7b742d317d"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://camo.githubusercontent.com/2e6701f04d89ef9b5335964cbec64476b9ddadb5b6ca6bc64d73b2b8009f06c9/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e696d6167653f575f7b747d685f7b742d317d"
                      alt="W_{t}h_{t-1}"
                ></a>对自身进行求导，变成一个nxn的对角矩阵，更多请参考<a class="link"   target="_blank" rel="noopener" href="https://nndl.github.io/nndl-book.pdf" >邱锡鹏 《神经网络与深度学习》 <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a><a class="link"   target="_blank" rel="noopener" href="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/blob/main/notes/14-%E6%95%B0%E5%80%BC%E7%A8%B3%E5%AE%9A%E6%80%A7.md#user-content-fn-%E5%9B%BE%E7%89%871-2bb18940390e507adda6b4ee943dcd6a" >1 <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a>）</p>
<p><a target="_blank" rel="noopener" href="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/blob/main/imgs/14/14-03.png"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/14/14-03.png"
                      alt="image"
                ></a></p>
</li>
<li><p>视频中<strong>勘误说明</strong>：链导法则中 <a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/71fa9f16c839f40f6fe979e6f72bc88ddf24c3d304e576cf6463c3ad09e6a010/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e696d6167653f5c667261637b5c7061727469616c2673706163653b575e7b747d685e7b742d317d7d7b5c7061727469616c2673706163653b685e7b742d317d7d3d2673706163653b575e7b747d"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://camo.githubusercontent.com/71fa9f16c839f40f6fe979e6f72bc88ddf24c3d304e576cf6463c3ad09e6a010/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e696d6167653f5c667261637b5c7061727469616c2673706163653b575e7b747d685e7b742d317d7d7b5c7061727469616c2673706163653b685e7b742d317d7d3d2673706163653b575e7b747d"
                      alt="\frac{\partial W^{t}h^{t-1}}{\partial h^{t-1}}= W^{t}"
                ></a> 而不是<a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/83dc04c534c36c75a02d9516553d3555aef6df231859509223a9953a04942948/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e696d6167653f5c6c6566742673706163653b28575e7b747d2673706163653b2673706163653b5c72696768742673706163653b295e7b547d"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://camo.githubusercontent.com/83dc04c534c36c75a02d9516553d3555aef6df231859509223a9953a04942948/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e696d6167653f5c6c6566742673706163653b28575e7b747d2673706163653b2673706163653b5c72696768742673706163653b295e7b547d"
                      alt="\left (W^{t} \right )^{T}"
                ></a>（这点由分子分母维度也容易推出），故最终求导结果包含<a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/90fa60edb85fc018d18ff5c3e21e28f3c075a514b1c31cba204dba5995760657/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e696d6167653f575e7b747d"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://camo.githubusercontent.com/90fa60edb85fc018d18ff5c3e21e28f3c075a514b1c31cba204dba5995760657/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e696d6167653f575e7b747d"
                      alt="W^{t}"
                ></a>，而不是其转置。</p>
</li>
</ul>
<h4 id="梯度爆炸"><a href="#梯度爆炸" class="headerlink" title="梯度爆炸"></a>梯度爆炸</h4><h5 id="使用ReLU作为激活函数"><a href="#使用ReLU作为激活函数" class="headerlink" title="使用ReLU作为激活函数"></a>使用ReLU作为激活函数</h5><p><a target="_blank" rel="noopener" href="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/blob/main/imgs/14/14-04.png"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/14/14-04.png"
                      alt="image"
                ></a></p>
<p>由于激活函数Relu求导后或者是1或者是0，变为对角矩阵的斜对角线元素后，与<a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/b672ed6c4894aead93fb0506d22544a527353e71b36e572f7dd042825883dbb3/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e696d6167653f575e7b697d"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://camo.githubusercontent.com/b672ed6c4894aead93fb0506d22544a527353e71b36e572f7dd042825883dbb3/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e696d6167653f575e7b697d"
                      alt="W^{i}"
                ></a>做乘积，斜对角线为1的部分会使得W中元素保留，最终该连乘式中有一些元素来自<a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/4eb293144f80b51159f7c69a62a047aad97e5d9bbe61e83f4fe6234323f63360/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e696d6167653f5c70726f645c6c6566742673706163653b282673706163653b575e7b697d2673706163653b5c72696768742673706163653b292673706163653b"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://camo.githubusercontent.com/4eb293144f80b51159f7c69a62a047aad97e5d9bbe61e83f4fe6234323f63360/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e696d6167653f5c70726f645c6c6566742673706163653b282673706163653b575e7b697d2673706163653b5c72696768742673706163653b292673706163653b"
                      alt="\prod\left ( W^{i} \right ) "
                ></a>，如果大部分<a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/b672ed6c4894aead93fb0506d22544a527353e71b36e572f7dd042825883dbb3/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e696d6167653f575e7b697d"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://camo.githubusercontent.com/b672ed6c4894aead93fb0506d22544a527353e71b36e572f7dd042825883dbb3/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e696d6167653f575e7b697d"
                      alt="W^{i}"
                ></a>中 值都大于1，且层数比较大，那么连乘之后可能导致梯度爆炸的问题。</p>
<h5 id="梯度爆炸问题"><a href="#梯度爆炸问题" class="headerlink" title="梯度爆炸问题"></a>梯度爆炸问题</h5><ul>
<li>值超出值域（infinity）<ul>
<li>对于16位浮点数尤为严重（数值区间 [6e-5 , 6e4]），GPU用16位浮点数更快</li>
</ul>
</li>
<li>对学习率敏感<ul>
<li>如果学习率太大→大参数值→更大的梯度，如此循环几次，容易导致梯度爆炸</li>
<li>如果学习率太小→训练无进展</li>
<li>我们可能需要在训练过程中不断调整学习率</li>
</ul>
</li>
</ul>
<h4 id="梯度消失"><a href="#梯度消失" class="headerlink" title="梯度消失"></a>梯度消失</h4><h5 id="使用Sigmoid作为激活函数"><a href="#使用Sigmoid作为激活函数" class="headerlink" title="使用Sigmoid作为激活函数"></a>使用Sigmoid作为激活函数</h5><p><a target="_blank" rel="noopener" href="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/blob/main/imgs/14/14-05.png"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/14/14-05.png"
                      alt="image"
                ></a></p>
<ul>
<li>蓝色曲线为函数值</li>
<li>黄色曲线为梯度，注意到当输入x值取±6时，此时梯度已经变得很小，由图也可以看出，当输入值稍大或稍小都很容易引起小梯度。</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/blob/main/imgs/14/14-06.png"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/14/14-06.png"
                      alt="image"
                ></a></p>
<p>所以最终连乘式中<a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/ed117f76d7cda611a8d81fcae67f6bf383bce937de9b079c5ccaaf30bdb6c5b8/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e696d6167653f5c70726f642673706163653b646961675c6c6566742673706163653b282673706163653b5c7369676d612673706163653b5e7b277d5c6c6566742673706163653b282673706163653b575e7b697d685e7b692d317d2673706163653b5c72696768742673706163653b292673706163653b5c72696768742673706163653b292673706163653b"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://camo.githubusercontent.com/ed117f76d7cda611a8d81fcae67f6bf383bce937de9b079c5ccaaf30bdb6c5b8/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e696d6167653f5c70726f642673706163653b646961675c6c6566742673706163653b282673706163653b5c7369676d612673706163653b5e7b277d5c6c6566742673706163653b282673706163653b575e7b697d685e7b692d317d2673706163653b5c72696768742673706163653b292673706163653b5c72696768742673706163653b292673706163653b"
                      alt="\prod diag\left ( \sigma ^{&#39;}\left ( W^{i}h^{i-1} \right ) \right ) "
                ></a>项乘出来会很小，导致整个梯度很小，产生梯度消失问题。</p>
<h5 id="梯度消失的问题"><a href="#梯度消失的问题" class="headerlink" title="梯度消失的问题"></a>梯度消失的问题</h5><ul>
<li>梯度值变为0<ul>
<li>对16位浮点数尤为严重</li>
</ul>
</li>
<li>训练没有进展<ul>
<li>不管如何选择学习率，由于梯度已经为0了，学习率x梯度&#x3D;0</li>
</ul>
</li>
<li>对于底部层尤为严重<ul>
<li>仅仅顶部层训练得较好。第<em>t</em>层导数包含d-t个矩阵乘积，越往底层走，t越小，乘得越多，梯度消失越严重，所以底部层效果更差。</li>
<li>无法让神经网络更深。只能把顶部层训练得比较好，底部层跑不动，这和给一个浅的神经网络没有什么区别。</li>
</ul>
</li>
</ul>
<h3 id="模型初始化和激活函数"><a href="#模型初始化和激活函数" class="headerlink" title="模型初始化和激活函数"></a>模型初始化和激活函数</h3><h4 id="让训练更加稳定"><a href="#让训练更加稳定" class="headerlink" title="让训练更加稳定"></a>让训练更加稳定</h4><p>我们的一个核心目标是如何让训练更稳定，梯度值不要太大也不要太小</p>
<ul>
<li>目标：让梯度值在合理的范围内<ul>
<li>例如 [1e-6, 1e3]</li>
</ul>
</li>
<li>常用方法：<ul>
<li>将乘法变加法：<ul>
<li>ResNet（跳跃连接，如果很多层，加入加法进去）</li>
<li>LSTM（引入记忆细胞，更新门，遗忘门，通过门权重求和，控制下一步是否更新）</li>
</ul>
</li>
<li>归一化：<ul>
<li>梯度归一化（归一化均值，方差）</li>
<li>梯度裁剪(clipping)：比如大于&#x2F;小于一个固定的阈值，就让梯度等于这个阈值，将梯度限制在一个范围中。（可以缓解梯度爆炸）</li>
</ul>
</li>
<li>合理的权重初始和激活函数：本节课讲述重点</li>
</ul>
</li>
</ul>
<p><strong>下面我们重点探讨最后一种方法：合理的权重初始和激活函数</strong></p>
<h4 id="基本假设：让每层的均值-方差是一个常数"><a href="#基本假设：让每层的均值-方差是一个常数" class="headerlink" title="基本假设：让每层的均值&#x2F;方差是一个常数"></a>基本假设：让每层的均值&#x2F;方差是一个常数</h4><ul>
<li><p><strong>将每层的输出和梯度都看做随机变量</strong></p>
<p>比如第i层有100维，就将输出和梯度分别看成100个随机变量</p>
</li>
<li><p><strong>让它们的均值和方差都保持一致</strong></p>
<p>我们的目标，这样不管神经网络多深，最后一层总与第一层差不多，从而不会梯度爆炸和消失</p>
</li>
</ul>
<p>根据我们的假设，可以列出如下方程式：</p>
<p><a target="_blank" rel="noopener" href="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/blob/main/imgs/14/14-07.png"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/14/14-07.png"
                      alt="image"
                ></a></p>
<h4 id="权重初始化"><a href="#权重初始化" class="headerlink" title="权重初始化"></a>权重初始化</h4><ul>
<li><p>在合理值区间里随机初始参数</p>
</li>
<li><p>训练</p>
<p>开始</p>
<p>的时候更容易有数值不稳定</p>
<ul>
<li>远离最优解的地方损失函数表面可能很复杂</li>
<li>最优解附近表面会比较平</li>
</ul>
</li>
<li><p>使用N(0, 0.01)分布来初始可能对小网络没问题，但不能保证深度神经网络</p>
</li>
</ul>
<h4 id="例子：MLP-1"><a href="#例子：MLP-1" class="headerlink" title="例子：MLP"></a>例子：MLP</h4><p>下面我们以MLP为例，考虑需要什么条件，才能满足<a class="link"   target="_blank" rel="noopener" href="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/blob/main/notes/14-%E6%95%B0%E5%80%BC%E7%A8%B3%E5%AE%9A%E6%80%A7.md#22-%E5%9F%BA%E6%9C%AC%E5%81%87%E8%AE%BE%EF%BC%9A%E8%AE%A9%E6%AF%8F%E5%B1%82%E7%9A%84%E5%9D%87%E5%80%BC/%E6%96%B9%E5%B7%AE%E6%98%AF%E4%B8%80%E4%B8%AA%E5%B8%B8%E6%95%B0" >2.2节 <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a>的假设。</p>
<h5 id="模型假设"><a href="#模型假设" class="headerlink" title="模型假设"></a>模型假设</h5><ul>
<li>每一层<strong>权重</strong>中的变量均为<strong>独立同分布</strong>，并设出均值、方差。</li>
<li>每一层<strong>输入</strong>的变量<strong>独立于</strong>该层<strong>权重</strong>变量。同时<strong>输入变量</strong>之间<strong>独立同分布</strong>。</li>
<li>假设没有激活函数(先简化分析，之后会考虑有激活函数的情况)，可以求得该层输出的期望为0。</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/blob/main/imgs/14/14-08.png"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/14/14-08.png"
                      alt="image"
                ></a></p>
<p>此处用到了一个重要性质：</p>
<p><a target="_blank" rel="noopener" href="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/blob/main/imgs/14/14-09.png"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/14/14-09.png"
                      alt="image"
                ></a></p>
<p>更多均值、方差运算可以参考<a class="link"   target="_blank" rel="noopener" href="https://blog.csdn.net/MissXy_/article/details/80705828" >期望、方差、协方差及相关系数的基本运算 <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p>
<h5 id="正向方差"><a href="#正向方差" class="headerlink" title="正向方差"></a>正向方差</h5><p><a target="_blank" rel="noopener" href="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/blob/main/imgs/14/14-10.png"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/14/14-10.png"
                      alt="image"
                ></a></p>
<ul>
<li>第二行的计算中仍然用到了<a class="link"   target="_blank" rel="noopener" href="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/blob/main/notes/241%E6%A8%A1%E5%9E%8B%E5%81%87%E8%AE%BE" >2.4.1节 <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a>的期望的重要性质：如果两个变量独立，它们乘积的均值&#x3D;均值的乘积，再结合w的期望为0(注意w和h独立，w之间独立同分布)，即有第二行末项期望为0。</li>
<li>最后一行由于wi,j独立同分布，方差相同，加上做了hj独立同分布的假设，所以可以写成 <strong>[t-1层输出维度] x [t层权重方差] x [t-1层输出方差]</strong> 的形式</li>
<li>此时，我们回过头来看我们的终极目标<a class="link"   target="_blank" rel="noopener" href="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/blob/main/notes/14-%E6%95%B0%E5%80%BC%E7%A8%B3%E5%AE%9A%E6%80%A7.md#22-%E5%9F%BA%E6%9C%AC%E5%81%87%E8%AE%BE%EF%BC%9A%E8%AE%A9%E6%AF%8F%E5%B1%82%E7%9A%84%E5%9D%87%E5%80%BC/%E6%96%B9%E5%B7%AE%E6%98%AF%E4%B8%80%E4%B8%AA%E5%B8%B8%E6%95%B0" >2.2节 <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a>的假设，每层输出期望为0我们已经可以满足(2.4.1节已经推导出)，而方差相同这一目标，通过上图的推导，我们发现需要<a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/f78bf82d006b08b156283f8f36616c89d1f31532215546b92819e2cc772c773a/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e696d6167653f2673706163653b6e5f7b742d317d5c67616d6d612673706163653b5f7b747d3d312673706163653b"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://camo.githubusercontent.com/f78bf82d006b08b156283f8f36616c89d1f31532215546b92819e2cc772c773a/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e696d6167653f2673706163653b6e5f7b742d317d5c67616d6d612673706163653b5f7b747d3d312673706163653b"
                      alt=" n_{t-1}\gamma _{t}=1 "
                ></a>。</li>
</ul>
<h5 id="反向均值和方差"><a href="#反向均值和方差" class="headerlink" title="反向均值和方差"></a>反向均值和方差</h5><p><a target="_blank" rel="noopener" href="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/blob/main/imgs/14/14-11.png"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/14/14-11.png"
                      alt="image"
                ></a></p>
<p>反向的情况和正向的类似，不过此时我们需要满足的式子变为<a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/9b5cf0aa698dc23314756d854eac930e1469588685dffa3b9fbefeec54b963a6/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e696d6167653f2673706163653b6e5f7b747d5c67616d6d612673706163653b5f7b747d3d312673706163653b"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://camo.githubusercontent.com/9b5cf0aa698dc23314756d854eac930e1469588685dffa3b9fbefeec54b963a6/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e696d6167653f2673706163653b6e5f7b747d5c67616d6d612673706163653b5f7b747d3d312673706163653b"
                      alt=" n_{t}\gamma _{t}=1 "
                ></a>。</p>
<h5 id="Xavier初始"><a href="#Xavier初始" class="headerlink" title="Xavier初始"></a>Xavier初始</h5><ul>
<li><p>上述推导带来的问题：难以同时满足<a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/f78bf82d006b08b156283f8f36616c89d1f31532215546b92819e2cc772c773a/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e696d6167653f2673706163653b6e5f7b742d317d5c67616d6d612673706163653b5f7b747d3d312673706163653b"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://camo.githubusercontent.com/f78bf82d006b08b156283f8f36616c89d1f31532215546b92819e2cc772c773a/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e696d6167653f2673706163653b6e5f7b742d317d5c67616d6d612673706163653b5f7b747d3d312673706163653b"
                      alt=" n_{t-1}\gamma _{t}=1 "
                ></a>和<a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/9b5cf0aa698dc23314756d854eac930e1469588685dffa3b9fbefeec54b963a6/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e696d6167653f2673706163653b6e5f7b747d5c67616d6d612673706163653b5f7b747d3d312673706163653b"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://camo.githubusercontent.com/9b5cf0aa698dc23314756d854eac930e1469588685dffa3b9fbefeec54b963a6/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e696d6167653f2673706163653b6e5f7b747d5c67616d6d612673706163653b5f7b747d3d312673706163653b"
                      alt=" n_{t}\gamma _{t}=1 "
                ></a>。（需要每层输出的维度都相同）</p>
</li>
<li><p>采用Xavier折中解决，不能同时满足上面两式，转而满足 [<strong>上面两式做加法后除以2</strong>] 得到的式子，用两种分布进行初始化（每层方差、均值满足推导式）。</p>
<p><a target="_blank" rel="noopener" href="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/blob/main/imgs/14/14-12.png"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/14/14-12.png"
                      alt="image"
                ></a></p>
</li>
<li><p>如果能确定每层输入、输出维度大小，则能确定该层权重的方差大小。</p>
</li>
<li><p>权重初始化方式：正态分布、均匀分布，均值&#x2F;方差满足Xavier的假设。</p>
</li>
</ul>
<h5 id="假设线性的激活函数"><a href="#假设线性的激活函数" class="headerlink" title="假设线性的激活函数"></a>假设线性的激活函数</h5><p>真实情况下，我们并不会用线性的激活函数（这样相当于没有进行激活），这里为了简化问题，假设激活函数是线性的。</p>
<ul>
<li><strong>正向</strong></li>
</ul>
<p><a target="_blank" rel="noopener" href="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/blob/main/imgs/14/14-13.png"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/14/14-13.png"
                      alt="image"
                ></a></p>
<p>上述推导表明，为了使得前向传播的均值为0，方差固定的话，激活函数必须f(x)&#x3D;x，这种恒等映射。</p>
<ul>
<li><strong>反向</strong></li>
</ul>
<p><a target="_blank" rel="noopener" href="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/blob/main/imgs/14/14-14.png"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/14/14-14.png"
                      alt="image"
                ></a></p>
<p>PPT上的推导似乎有点问题（上图中第二行方程），笔者重新进行了下述推导，读者也可自行推导验证：</p>
<p><a target="_blank" rel="noopener" href="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/blob/main/imgs/14/14-15.png"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/14/14-15.png"
                      alt="image"
                ></a></p>
<p><strong>通过正向和反向的推导，我们可以得出的【结论】是：当激活函数为f(x)&#x3D;x，这种恒等映射更有利于维持神经网络的稳定性。</strong></p>
<h5 id="检查常用激活函数"><a href="#检查常用激活函数" class="headerlink" title="检查常用激活函数"></a>检查常用激活函数</h5><p><a target="_blank" rel="noopener" href="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/blob/main/imgs/14/14-16.png"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/14/14-16.png"
                      alt="image"
                ></a></p>
<p>对于常用激活函数：tanh，relu满足在零点附近有f(x)&#x3D;x，而sigmoid函数在零点附近不满足要求，可以对sigmoid函数进行调整（根据Taylor展开式，调整其过原点）</p>
<h3 id="总结-2"><a href="#总结-2" class="headerlink" title="总结"></a>总结</h3><ul>
<li>当数值过大或者过小时，会导致数值问题。</li>
<li>常发生在深度模型中，因为其会对n个数累乘。</li>
<li>合理的权重初始值(如Xavier)和激活函数的选取(如relu, tanh, 调整后的sigmoid)可以提升数值稳定性。</li>
</ul>
<h2 id="实战"><a href="#实战" class="headerlink" title="实战"></a>实战</h2><div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">import hashlib  #这个模块可以用于生成文件哈希值</span><br><span class="line">import os  #处理文件和目录</span><br><span class="line">import tarfile  #处理.tar和.tar.gz文件</span><br><span class="line">import zipfile  #用于处理.zip文件</span><br><span class="line">import requests  </span><br><span class="line"></span><br><span class="line">DATA_HUB = dict()</span><br><span class="line">DATA_URL = ###</span><br><span class="line"></span><br><span class="line"># cache_dir为缓存目录，默认值为.../data</span><br><span class="line">def download(name,cache_dir=os.path.join(&#x27;..&#x27;,&#x27;data&#x27;)):</span><br><span class="line">	# 检查名字是否存在</span><br><span class="line">	assert name in DATA_HUB,f&quot;&#123;name&#125; 不存在于 &#123;DATA_HUB&#125;&quot;</span><br><span class="line">	# 获取数据集的 URL 和 SHA1 哈希值</span><br><span class="line">	url,sha1_hash = DATA_HUB[name]</span><br><span class="line">	# 创建缓存目录</span><br><span class="line">	os.makedirs(cash_dir,exist_ok=True)</span><br><span class="line">	fname = os.path.join(cach_dir,url.split(&#x27;/&#x27;)[-1])</span><br><span class="line">	if os.path.exists(fname):</span><br><span class="line">		sha1 = hashlib.sha1()</span><br><span class="line">			with open(fname,&#x27;rb&#x27;) as f:</span><br><span class="line">                while True:</span><br><span class="line">                    data = f.read(1048576):</span><br><span class="line">                    if not data:</span><br><span class="line">                        break</span><br><span class="line">                    sha1.update(data)</span><br><span class="line">            if sha1.hexdigest() == sha1_hash:</span><br><span class="line">            	return fname</span><br><span class="line">    print(f&#x27;正在从&#123;url&#125;下载&#123;fname&#125;...&#x27;)</span><br><span class="line">    r = requests.get(url, stream=True, verify=True)</span><br><span class="line">    with open(fname, &#x27;wb&#x27;) as f:</span><br><span class="line">        f.write(r.content)</span><br><span class="line">    return fname</span><br><span class="line"></span><br><span class="line">def download_extract(name, folder=None):  #@save</span><br><span class="line">    &quot;&quot;&quot;下载并解压zip/tar文件&quot;&quot;&quot;</span><br><span class="line">    fname = download(name)</span><br><span class="line">    base_dir = os.path.dirname(fname)</span><br><span class="line">    data_dir, ext = os.path.splitext(fname)</span><br><span class="line">    if ext == &#x27;.zip&#x27;:</span><br><span class="line">        fp = zipfile.ZipFile(fname, &#x27;r&#x27;)</span><br><span class="line">    elif ext in (&#x27;.tar&#x27;, &#x27;.gz&#x27;):</span><br><span class="line">        fp = tarfile.open(fname, &#x27;r&#x27;)</span><br><span class="line">    else:</span><br><span class="line">        assert False, &#x27;只有zip/tar文件可以被解压缩&#x27;</span><br><span class="line">    fp.extractall(base_dir)</span><br><span class="line">    return os.path.join(base_dir, folder) if folder else data_dir</span><br><span class="line"></span><br><span class="line">def download_all():  #@save</span><br><span class="line">    &quot;&quot;&quot;下载DATA_HUB中的所有文件&quot;&quot;&quot;</span><br><span class="line">    for name in DATA_HUB:</span><br><span class="line">        download(name)</span><br></pre></td></tr></table></figure></div>

<p>数据展示-数据处理（不需要的数值删除，缺失值替换，处理离散值我们用one-hot编码来替换它们）-从pandas格式中提取NUMPY格式，并将其转化成张量形式</p>
<p>对于数据预处理部分：</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="C:/Users/Y9000p/AppData/Roaming/Typora/typora-user-images/image-20240703193142034.png"
                      alt="image-20240703193142034"
                ></p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import pandas as pd</span><br><span class="line">import torch</span><br><span class="line">from torch import nn</span><br><span class="line">from d2l import torch as d2l</span><br><span class="line"></span><br><span class="line">#读取数据</span><br><span class="line">train_data = pd.read.csv((download(&#x27;kaggle_house_train&#x27;))</span><br><span class="line">test_data = pd.read_csv(download(&#x27;kaggle_house_test&#x27;))</span><br><span class="line"></span><br><span class="line">#读取数据集形状</span><br><span class="line">print(train_data.shape)</span><br><span class="line">print(test_data.shape)</span><br><span class="line"></span><br><span class="line"># 读取数据集特征及对应标签</span><br><span class="line">print(train_data.iloc[0:4, [0, 1, 2, 3, -3, -2, -1]])</span><br><span class="line"></span><br><span class="line">由于ID与数据集本身无太大关联（不携带预测信息）</span><br><span class="line">all_features = pd.concat((train_data.iloc[:,1:-1],test_data[:,1:]))</span><br><span class="line"></span><br><span class="line">## 数据预处理</span><br><span class="line"># 将所有缺失的值替换为相应特征的平均值。通过将特征重新缩放到零均值和单位方差来标准化数据</span><br><span class="line"># 若无法获得测试数据，则可根据训练数据计算均值和标准差</span><br><span class="line">numeric_features = all_features.dtype[all_features.dtype!=&#x27;object&#x27;].index</span><br><span class="line">all_features[numeric_features] = all_features[numeric_features].apply(lambd x:(x-x.mean())/x.std())</span><br><span class="line"># 在标准化数据之后，所有均值消失，因此我们可以将缺失值设置为0</span><br><span class="line">all_features[numeric_features] = all_features[numeric_features].fillna(0)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 处理离散值 若特征仅有两个值1则一个看作0，另一个看作1</span><br><span class="line"># 使用 pd.get_dummies 函数进行独热编码（One-Hot Encoding）和处理缺失值的目的是将类别型特征转换为适合机器学习算法的数值格式。</span><br><span class="line"># “Dummy_na=True”将“na”（缺失值）视为有效的特征值，并为其创建指示符特征</span><br><span class="line">all_features = pd.get_dummies(all_features,dummy_na=True)</span><br><span class="line">all_features.shape</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">## 通过values属性，我们可以 [从pandas格式中提取NumPy格式，并将其转换为张量表示]用于训练。</span><br><span class="line">n_train= train.data.shape[0]</span><br><span class="line">train_featrue = torch.tensor(all_features[:n_train].values,dtype=torch.float32)</span><br><span class="line">test_featrue = torch.tensor(all_features[n_train:].values,dtype=torch.float32)</span><br><span class="line">train_labels = torch.tensor(train_data.SalePrices.values.reshape(-1:1),dtype=torch.float32)</span><br></pre></td></tr></table></figure></div>

<p>训练</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="C:/Users/Y9000p/AppData/Roaming/Typora/typora-user-images/image-20240703195033844.png"
                      alt="image-20240703195033844"
                ></p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line">#带损失平方的线性模型（base）</span><br><span class="line">loss = nn.MSELoss</span><br><span class="line">in_features = train_features.shape[1]</span><br><span class="line"></span><br><span class="line">def get_nn():</span><br><span class="line">	net = nn.Sequential(nn.linear(infeatures,1))</span><br><span class="line">	return net</span><br><span class="line">	</span><br><span class="line">	</span><br><span class="line"># 使用相对误差（由于数据值相差较大）</span><br><span class="line">def log_rmse(net, features, labels):</span><br><span class="line">    # 使用 torch.clamp 函数将预测值限制在 [1, ∞) 之间，避免对数计算中的负值或零</span><br><span class="line">    clipped_preds = torch.clamp(net(features), 1,float(&#x27;inf&#x27;))</span><br><span class="line">    # 计算 log(RMSE)</span><br><span class="line">    rmse = torch.sqrt(nn.MSELoss()(torch.log(clipped_preds), torch.log(labels)))</span><br><span class="line">    return rmse.item()</span><br><span class="line">	</span><br><span class="line">	</span><br><span class="line"># 训练函数借助ADAM优化器</span><br><span class="line">def train(net, train_features, train_labels, test_features, test_labels, num_epochs, weight_decay, learning_rate, batch_size):</span><br><span class="line">	# 用于保存每个 epoch 的训练和测试损失。</span><br><span class="line">    train_ls, test_ls = [], []</span><br><span class="line">    train_iter = d2l.load_array((train_features, train_labels), batch_size)</span><br><span class="line">    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate, weight_decay=weight_decay)</span><br><span class="line">    loss_fn = nn.MSELoss()  # 假设您使用 MSE 作为损失函数</span><br><span class="line">    </span><br><span class="line">    for epoch in range(num_epochs):</span><br><span class="line">        net.train()  # 设置网络为训练模式</span><br><span class="line">        for X, y in train_iter:</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            l = loss_fn(net(X), y)</span><br><span class="line">            l.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line">        net.eval()  # 设置网络为评估模式</span><br><span class="line">        train_ls.append(log_rmse(net, train_features, train_labels))</span><br><span class="line">        if test_labels is not None:</span><br><span class="line">            test_ls.append(log_rmse(net, test_features, test_labels))</span><br><span class="line">    return train_ls, test_ls</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"># K折交叉验证</span><br><span class="line">def get_k_fold_data(k,i,X,y):</span><br><span class="line">	assert k&gt;1</span><br><span class="line">	flod_size = X.shape[0] // k</span><br><span class="line">	X_train,y_train=None,None</span><br><span class="line">	for j in range(k):</span><br><span class="line">		idx = slice(j*fold_size,(j+1)*fold_size)</span><br><span class="line">		X_part,y_part = X[idx,:],y[idx,:]</span><br><span class="line">		if j==i:</span><br><span class="line">			X_valid,y_valid=X_part,y_part</span><br><span class="line">		elif X_train is None:</span><br><span class="line">            X_train, y_train = X_part, y_part</span><br><span class="line">        else:</span><br><span class="line">        #将当前折的数据与已有的训练集数据拼接起来。</span><br><span class="line">        X_train = torch.cat([X_train,X_part],0)</span><br><span class="line">        y_train = torch.cat([y_train, y_part], 0)</span><br><span class="line">    return X_train, y_train, X_valid, y_valid</span><br><span class="line"></span><br><span class="line">def k_fold(k, X_train, y_train, num_epochs, learning_rate, weight_decay,batch_size):</span><br><span class="line">	train_l_sum,valid_l_sum=0,0</span><br><span class="line">	for i in range(k):</span><br><span class="line">		data =  get_k_fold_data(k, i, X_train, y_train)</span><br><span class="line">		net = get_net()</span><br><span class="line">		train_ls, valid_ls = train(net, *data, num_epochs, learning_rate, weight_decay, batch_size)</span><br><span class="line">		train_l_sum += train_ls[-1]</span><br><span class="line">        valid_l_sum += valid_ls[-1]</span><br><span class="line">    if i == 0:</span><br><span class="line">            d2l.plot(list(range(1, num_epochs + 1)), [train_ls, valid_ls],</span><br><span class="line">                     xlabel=&#x27;epoch&#x27;, ylabel=&#x27;rmse&#x27;, xlim=[1, num_epochs],</span><br><span class="line">                     legend=[&#x27;train&#x27;, &#x27;valid&#x27;], yscale=&#x27;log&#x27;)</span><br><span class="line">        print(f&#x27;折&#123;i + 1&#125;，训练log rmse&#123;float(train_ls[-1]):f&#125;, &#x27;</span><br><span class="line">              f&#x27;验证log rmse&#123;float(valid_ls[-1]):f&#125;&#x27;)</span><br><span class="line">    return train_l_sum / k, valid_l_sum / k</span><br></pre></td></tr></table></figure></div>

<p>提交预测</p>
<h1 id="深度学习计算"><a href="#深度学习计算" class="headerlink" title="深度学习计算"></a>深度学习计算</h1><h2 id="PyTorch-神经网络基础"><a href="#PyTorch-神经网络基础" class="headerlink" title="PyTorch 神经网络基础"></a>PyTorch 神经网络基础</h2><h3 id="层和块"><a href="#层和块" class="headerlink" title="层和块"></a>层和块</h3><p>在之前的内容中，我们认识了一些神经网络，比如：线性回归，Softmax回归，多层感知机；他们有的是整个模型，有的是一层神经网络，有的甚至只是一个单元，他们的功能以及复杂程度也各不相同，但他们都有着如下三个特征：</p>
<ul>
<li>接受一些输入</li>
<li>产生对应的输出</li>
<li>由一组可调整参数描述</li>
</ul>
<p>对于一些复杂的网络，研究讨论比层大但比整个模型小的部分很有意义，因为复杂的网络中经常有重复出现的部分，每个部分也常常有自己的功能。考虑到上面的三个特征，这就使得我们思考是否可以对这些部分进行一个抽象，这就得到了块的概念：块指单个层，多个层组成的部分，或者整个模型本身。使用块对整个模型进行描述就简便许多，这一过程是递归的，块的内部还可以划分为多个块，直至满足需要为止。</p>
<p>PyTorch帮我们实现了块的大部分所需功能，包括自动求导，我们只需从nn.Module继承并改写其中的一部分就能得到我们需要的块以及模型。</p>
<p><strong><code>nn.Sequential</code></strong></p>
<p>**定义了一种特殊的<code>Module</code>**， 即在PyTorch中表示一个块的类， 它维护了一个由<code>Module</code>组成的有序列表。 </p>
<h4 id="自定义块"><a href="#自定义块" class="headerlink" title="自定义块"></a><strong>自定义块</strong></h4><p><strong>每个块必须提供的基本功能：</strong></p>
<ol>
<li>将输入数据作为其前向传播函数的参数。</li>
<li>通过前向传播函数来生成输出。请注意，输出的形状可能与输入的形状不同。例如，我们上面模型中的第一个全连接的层接收一个20维的输入，但是返回一个维度为256的输出。</li>
<li>计算其输出关于输入的梯度，可通过其反向传播函数进行访问。通常这是自动发生的。</li>
<li>存储和访问前向传播计算所需的参数。</li>
<li>根据需要初始化模型参数。</li>
</ol>
<p>**实例</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torch import nn</span><br><span class="line">import torch.nn.functional as F</span><br><span class="line"></span><br><span class="line">class MLP(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super().__init__()</span><br><span class="line">        self.hidden_layer = nn.Linear(20, 256)</span><br><span class="line">        self.out_layer = nn.Linear(256, 10)</span><br><span class="line"></span><br><span class="line">    def forward(self, X):</span><br><span class="line">        X = F.relu(self.hidden_layer(X))</span><br><span class="line">        return self.out_layer(X)</span><br><span class="line"></span><br><span class="line"># 示例：如何使用该模型</span><br><span class="line"># 假设输入数据有 20 个特征</span><br><span class="line">model = MLP()</span><br><span class="line">X = torch.rand(5, 20)  # 生成 5 个样本，每个样本有 20 个特征</span><br><span class="line">output = model(X)</span><br><span class="line">print(output)</span><br><span class="line"></span><br></pre></td></tr></table></figure></div>

<h5 id="顺序块"><a href="#顺序块" class="headerlink" title="顺序块"></a>顺序块</h5><p>现在我们可以更仔细地看看<code>Sequential</code>类是如何工作的， 回想一下<code>Sequential</code>的设计是为了把其他模块串起来。 为了构建我们自己的简化的<code>MySequential</code>， 我们只需要定义两个关键函数：</p>
<ol>
<li>一种将块逐个追加到列表中的函数；</li>
<li>一种前向传播函数，用于将输入按追加块的顺序传递给块组成的“链条”。</li>
</ol>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">class mySequential(nn.Module):</span><br><span class="line">	def __init__(self,**args):</span><br><span class="line">		super().__init__</span><br><span class="line">		for idx,module in enumerate(args):</span><br><span class="line">			self._modules[str(idx)] = module</span><br><span class="line">	</span><br><span class="line">	def forward(self,X):</span><br><span class="line">		for block in self._module.values:</span><br><span class="line">			X = block(X)</span><br><span class="line">		return X</span><br></pre></td></tr></table></figure></div>

<h5 id="在前向传播函数中执行代码"><a href="#在前向传播函数中执行代码" class="headerlink" title="在前向传播函数中执行代码"></a><strong>在前向传播函数中执行代码</strong></h5><p><code>Sequential</code>类使模型构造变得简单， 允许我们组合新的架构，而不必定义自己的类。 然而，并不是所有的架构都是简单的顺序架构。 当需要更强的灵活性时，我们需要定义自己的块。 例如，我们可能希望在前向传播函数中执行Python的控制流。 此外，我们可能希望执行任意的数学运算， 而不是简单地依赖预定义的神经网络层。</p>
<p>到目前为止， 我们网络中的所有操作都对网络的激活值及网络的参数起作用。 然而，有时我们可能希望合并既不是上一层的结果也不是可更新参数的项， 我们称之为<em>常数参数</em>（constant parameter）。 例如，我们需要一个计算函数 𝑓(𝑥,𝑤)&#x3D;𝑐⋅𝑤⊤𝑥的层， 其中𝑥是输入， 𝑤是参数， 𝑐是某个在优化过程中没有更新的指定常量。 因此我们实现了一个<code>FixedHiddenMLP</code>类，如下所示：</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">class FixedHiddenMLP(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super().__init__()</span><br><span class="line">        # 不计算梯度的随机权重参数。因此其在训练期间保持不变</span><br><span class="line">        self.rand_weight = torch.rand((20, 20), requires_grad=False)</span><br><span class="line">        self.linear = nn.Linear(20, 20)</span><br><span class="line"></span><br><span class="line">    def forward(self, X):</span><br><span class="line">        X = self.linear(X)</span><br><span class="line">        # 使用创建的常量参数以及relu和mm函数</span><br><span class="line">        X = F.relu(torch.mm(X, self.rand_weight) + 1)</span><br><span class="line">        # 复用全连接层。这相当于两个全连接层共享参数</span><br><span class="line">        X = self.linear(X)</span><br><span class="line">        # 控制流</span><br><span class="line">        while X.abs().sum() &gt; 1:</span><br><span class="line">            X /= 2</span><br><span class="line">        return X.sum()</span><br></pre></td></tr></table></figure></div>

<h3 id="参数管理"><a href="#参数管理" class="headerlink" title="参数管理"></a>参数管理</h3><p>在选择了架构并设置了超参数后，我们就进入了训练阶段。此时，我们的目标是找到使损失函数最小化的模型参数值。经过训练后，我们将需要使用这些参数来做出未来的预测。此外，有时我们希望提取参数，以便在其他环境中复用它们，将模型保存下来，以便它可以在其他软件中执行，或者为了获得科学的理解而进行检查。</p>
<h4 id="参数访问"><a href="#参数访问" class="headerlink" title="参数访问"></a>参数访问</h4><div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 我们先构建一个单隐藏层的多层感知机</span><br><span class="line">net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(), nn.Linear(8, 1))</span><br><span class="line">## 参数访问+索引访问+利用state_dict</span><br><span class="line">net[2].state_dict()</span><br></pre></td></tr></table></figure></div>

<h4 id="目标参数"><a href="#目标参数" class="headerlink" title="目标参数"></a>目标参数</h4><div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 提取偏置</span><br><span class="line">net[2].bias.data</span><br><span class="line">net[2].bias</span><br><span class="line">#</span><br><span class="line">tensor([-0.1032])</span><br><span class="line">tensor([-0.1032], requires_grad=True)</span><br></pre></td></tr></table></figure></div>

<h4 id="一次性访问所有参数"><a href="#一次性访问所有参数" class="headerlink" title="一次性访问所有参数"></a>一次性访问所有参数</h4><div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 利用named_parameters()</span><br><span class="line">print(*[(name,param.shape) for name,param in net[0].named_parameters()])</span><br><span class="line">print(*[(name,param.shape) for name,param in net.named_parameters()])</span><br><span class="line"># 另一种采用state_dict，这是一个Ordereddict</span><br><span class="line">netp[2].state_dict()[&#x27;2.bias;].data</span><br></pre></td></tr></table></figure></div>

<h4 id="从嵌套块中收集参数"><a href="#从嵌套块中收集参数" class="headerlink" title="从嵌套块中收集参数"></a>从嵌套块中收集参数</h4><div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">def block1():</span><br><span class="line">    return nn.Sequential(nn.Linear(4, 8), nn.ReLU(),</span><br><span class="line">                         nn.Linear(8, 4), nn.ReLU())</span><br><span class="line"></span><br><span class="line">def block2():</span><br><span class="line">    net = nn.Sequential()</span><br><span class="line">    for i in range(4):</span><br><span class="line">    	net.add_module(f&#x27;blcok&#123;i&#125;&#x27;,block1())</span><br><span class="line">    return net</span><br><span class="line"></span><br><span class="line">rgnet = nn.Sequential(block2(),nn.Linear(4,1))</span><br><span class="line">rgnet(X)</span><br></pre></td></tr></table></figure></div>

<h4 id="参数初始化"><a href="#参数初始化" class="headerlink" title="参数初始化"></a>参数初始化</h4><h5 id="内置初始化"><a href="#内置初始化" class="headerlink" title="内置初始化"></a>内置初始化</h5><div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">def init_normal(m):</span><br><span class="line">    if type(m) == nn.Linear:</span><br><span class="line">    	#用正态分布初始化</span><br><span class="line">    	nn.init.normal(m.weight,mean=0,std=0.01)</span><br><span class="line">    	#常数</span><br><span class="line">    	nn.init.constant_(m.weight, 1)</span><br><span class="line">    	#xavier</span><br><span class="line">    	nn.init.xavier_uniform_(m.weight)</span><br><span class="line">    	</span><br><span class="line">    	nn.init.zeros(m.bias)</span><br><span class="line">    	</span><br><span class="line">net.apply(init_normal)</span><br><span class="line">net[0].weight.data[0], net[0].bias.data[0]</span><br></pre></td></tr></table></figure></div>

<h5 id="自定义初始化"><a href="#自定义初始化" class="headerlink" title="自定义初始化"></a>自定义初始化</h5><p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="C:/Users/Y9000p/AppData/Roaming/Typora/typora-user-images/image-20240704152743711.png"
                      alt="image-20240704152743711"
                ></p>
<h4 id="参数绑定"><a href="#参数绑定" class="headerlink" title="参数绑定"></a>参数绑定</h4><p>有时我们希望在多个层间共享参数： 我们可以定义一个稠密层，然后使用它的参数来设置另一个层的参数。</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># 我们需要给共享层一个名称，以便可以引用它的参数</span><br><span class="line">shared = nn.Linear(8, 8)</span><br><span class="line">net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(),</span><br><span class="line">                    shared, nn.ReLU(),</span><br><span class="line">                    shared, nn.ReLU(),</span><br><span class="line">                    nn.Linear(8, 1))</span><br><span class="line">net(X)</span><br><span class="line"># 检查参数是否相同</span><br><span class="line">print(net[2].weight.data[0] == net[4].weight.data[0])</span><br><span class="line">net[2].weight.data[0, 0] = 100</span><br><span class="line"># 确保它们实际上是同一个对象，而不只是有相同的值</span><br><span class="line">print(net[2].weight.data[0] == net[4].weight.data[0])</span><br></pre></td></tr></table></figure></div>

<h3 id="延后初始化"><a href="#延后初始化" class="headerlink" title="延后初始化"></a>延后初始化</h3><p>有时在建立网络时，我们不会指定网络的输入输出维度，也就不能确定网络的参数形状，深度学习框架支持延后初始化，即当第一次将数据传入模型时自动的得到所有的维度，然后初始化所有的参数。</p>
<p>PyTorch也支持这一点，比如nn.LazyLinear，但本门课程中并未介绍。</p>
<h3 id="自定义层"><a href="#自定义层" class="headerlink" title="自定义层"></a>自定义层</h3><p>深度学习成功背后的一个因素是神经网络的灵活性：我们可以用创造性的方式组合不同的层，从而设计出适用于各种任务的架构。例如，研究人员发明了专门用于处理图像、文本、序列数据和执行动态规划的层。同样的，对于层而言，深度学习框架并不能满足我们所有的需求，然而，层本身也具有极大的灵活性，我们可以自定义想要的层。</p>
<h3 id="读写文件"><a href="#读写文件" class="headerlink" title="读写文件"></a>读写文件</h3><p>到目前为止，我们讨论了如何处理数据，以及如何构建、训练和测试深度学习模型。然而，有时我们希望保存训练的模型，以备将来在各种环境中使用（比如在部署中进行预测）。此外，当运行一个耗时较长的训练过程时，最佳的做法是定期保存中间结果，以确保在服务器电源被不小心断掉时，我们不会损失几天的计算结果。</p>
<h1 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h1><h2 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h2><h3 id="平移不变性"><a href="#平移不变性" class="headerlink" title="平移不变性"></a>平移不变性</h3><p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="C:/Users/Y9000p/AppData/Roaming/Typora/typora-user-images/image-20240704182222900.png"
                      alt="image-20240704182222900"
                ></p>
<h3 id="局部性"><a href="#局部性" class="headerlink" title="局部性"></a>局部性</h3><p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="C:/Users/Y9000p/AppData/Roaming/Typora/typora-user-images/image-20240704183058348.png"
                      alt="image-20240704183058348"
                ></p>
<h3 id="代码-1"><a href="#代码-1" class="headerlink" title="代码"></a>代码</h3><p>互相关运算</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torch import nn</span><br><span class="line"></span><br><span class="line">def corr2d(X,K):    #X为输入，K为核矩阵</span><br><span class="line">    h,w=K.shape    #h得到K的行数，w得到K的列数</span><br><span class="line">    Y=torch.zeros((X.shape[0]-h+1,X.shape[1]-w+1))  #用0初始化输出矩阵Y</span><br><span class="line">    for i in range(Y.shape[0]):   #卷积运算</span><br><span class="line">        for j in range(Y.shape[1]):</span><br><span class="line">          Y[i,j]=(X[i:i+h,j:j+w]*K).sum()</span><br><span class="line">    return Y</span><br></pre></td></tr></table></figure></div>

<p>实现二维卷积层</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">#实现二维卷积层</span><br><span class="line">class Conv2d(nn.Module):</span><br><span class="line">    def _init_(self,kernel_size):</span><br><span class="line">        super()._init_()</span><br><span class="line">        self.weight=nn.Parameter(torch.rand(kerner_size))</span><br><span class="line">        self.bias=nn.Parameter(torch.zeros(1))</span><br><span class="line">    def forward(self,x):</span><br><span class="line">        return corr2d(x,self.weight)+self.bias </span><br></pre></td></tr></table></figure></div>

<p>建立一个边缘检测</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">X=torch.ones((6,8))</span><br><span class="line">X[:,2:6]=0</span><br><span class="line">X</span><br></pre></td></tr></table></figure></div>

<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; tensor([[1., 1., 0., 0., 0., 0., 1., 1.],</span><br><span class="line">        [1., 1., 0., 0., 0., 0., 1., 1.],</span><br><span class="line">        [1., 1., 0., 0., 0., 0., 1., 1.],</span><br><span class="line">        [1., 1., 0., 0., 0., 0., 1., 1.],</span><br><span class="line">        [1., 1., 0., 0., 0., 0., 1., 1.],</span><br><span class="line">        [1., 1., 0., 0., 0., 0., 1., 1.]])</span><br></pre></td></tr></table></figure></div>

<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">K=torch.tensor([[-1,1]])  #这个K只能检测垂直边缘</span><br><span class="line">Y=corr2d(X,K)</span><br><span class="line">Y</span><br></pre></td></tr></table></figure></div>

<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; tensor([[ 0., -1.,  0.,  0.,  0.,  1.,  0.],</span><br><span class="line">            [ 0., -1.,  0.,  0.,  0.,  1.,  0.],</span><br><span class="line">            [ 0., -1.,  0.,  0.,  0.,  1.,  0.],</span><br><span class="line">            [ 0., -1.,  0.,  0.,  0.,  1.,  0.],</span><br><span class="line">            [ 0., -1.,  0.,  0.,  0.,  1.,  0.],</span><br><span class="line">            [ 0., -1.,  0.,  0.,  0.,  1.,  0.]])</span><br></pre></td></tr></table></figure></div>

<p>检测垂直的边缘</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">corr2d(X.t(),K)</span><br></pre></td></tr></table></figure></div>

<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; tensor([[0., 0., 0., 0., 0.],</span><br><span class="line">            [0., 0., 0., 0., 0.],</span><br><span class="line">            [0., 0., 0., 0., 0.],</span><br><span class="line">            [0., 0., 0., 0., 0.],</span><br><span class="line">            [0., 0., 0., 0., 0.],</span><br><span class="line">            [0., 0., 0., 0., 0.],</span><br><span class="line">            [0., 0., 0., 0., 0.],</span><br><span class="line">            [0., 0., 0., 0., 0.]])</span><br></pre></td></tr></table></figure></div>

<p>学习由X生成Y的卷积核</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"># 输入，输出，卷积核，偏置</span><br><span class="line">conv2d = nn.Conv2d(1, 1, kernel_size=(1, 2), bias=False)</span><br><span class="line"></span><br><span class="line"># 通道数，批量大小，高度，宽度</span><br><span class="line">X = X.reshape((1, 1, 6, 8))</span><br><span class="line">Y = Y.reshape((1, 1, 6, 7))</span><br><span class="line"></span><br><span class="line">for i in range(10):</span><br><span class="line">    Y_hat = conv2d(X)</span><br><span class="line">    l = (Y_hat-Y)**2//2</span><br><span class="line">    conv2d.zero_grad()</span><br><span class="line">    l.sum().backward()</span><br><span class="line">    conv2d.weight.data[:]-=6e-2*conv2d.weight.grad</span><br><span class="line">    if (i+1) % 2 == 0:</span><br><span class="line">    	print(f&#x27;batch &#123;i+1&#125; ,loss &#123;l.sum():3f&#125;&#125;</span><br></pre></td></tr></table></figure></div>

<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; batch 2, loss 3.852</span><br><span class="line">    batch 4, loss 1.126</span><br><span class="line">    batch 6, loss 0.386</span><br><span class="line">    batch 8, loss 0.145</span><br><span class="line">    batch 10, loss 0.057</span><br></pre></td></tr></table></figure></div>

<h2 id="卷积层里的填充和步幅"><a href="#卷积层里的填充和步幅" class="headerlink" title="卷积层里的填充和步幅"></a>卷积层里的填充和步幅</h2><h3 id="填充"><a href="#填充" class="headerlink" title="填充"></a>填充</h3><p><strong>填充</strong>(Padding)指的是在输入周围添加额外的行&#x2F;列</p>
<p><a target="_blank" rel="noopener" href="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/blob/main/imgs/20/20-01.png"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/20/20-01.png"
                      alt="image"
                ></a></p>
<p><strong>维度变化</strong>：</p>
<p><a target="_blank" rel="noopener" href="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/blob/main/imgs/20/20-02.png"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/20/20-02.png"
                      alt="image"
                ></a></p>
<p><strong>两种不同的卷积方式</strong>： ①Valid 卷积：不进行填充，卷积运算过后得到的矩阵形状为(n-f+1)×(n-f+1)。</p>
<p>②Same 卷积：先对矩阵进行填充，然后再进行卷积运算，使得运算前后矩阵大小不变。</p>
<p><a target="_blank" rel="noopener" href="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/blob/main/imgs/20/20-03.png"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/20/20-03.png"
                      alt="image"
                ></a></p>
<h3 id="步幅"><a href="#步幅" class="headerlink" title="步幅"></a>步幅</h3><p><strong>想法来源：如果按照原来的操作(卷积步长为1)，那么给定输入大小为224x224，在使用5x5卷积核的情况下，需要55层</strong>才能将输出降低到4x4，也就是说，需要大量的计算才能得到维度较小的输出。</p>
<p><strong>步幅</strong>是指行&#x2F;列的滑动步长</p>
<p><a target="_blank" rel="noopener" href="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/blob/main/imgs/20/20-04.png"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/20/20-04.png"
                      alt="image"
                ></a></p>
<p><strong>维度变化</strong>:</p>
<p><a target="_blank" rel="noopener" href="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/blob/main/imgs/20/20-05.png"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/20/20-05.png"
                      alt="image"
                ></a></p>
<p>注意：第三点可以当做结论来记(Same卷积或Valid卷积(且s≥k时))。一般来说，如果n是偶数，s取2，池化层做Valid卷积(不填充)且k&#x3D;2，此时输出维度直接可以写成n&#x2F;2 x n&#x2F;2。如果怕搞混，直接记第一个公式每次现推也可。</p>
<h3 id="总结-3"><a href="#总结-3" class="headerlink" title="总结"></a>总结</h3><ul>
<li>填充和步幅是卷积层的<strong>超参数</strong></li>
<li><strong>填充</strong>(padding)在输入周围添加额外的行&#x2F;列，来控制输出形状的减少量</li>
<li><strong>步幅</strong>(stride)是每次滑动核窗口时的行&#x2F;列的步长，可以成倍地减少输出形状</li>
</ul>
<h3 id="代码-2"><a href="#代码-2" class="headerlink" title="代码"></a>代码</h3><h4 id="填充和步幅"><a href="#填充和步幅" class="headerlink" title="填充和步幅"></a>填充和步幅</h4><p><strong>导入包，定义comp_conv2d函数 (进行卷积操作, 输出后两维，便于观察高宽的维度变化)</strong></p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torch import nn</span><br><span class="line"></span><br><span class="line">def comp_conv2d(conv2d, X):</span><br><span class="line">	# 维度的前面加入批量大小数(batch_size)和输入通道数(channel_in)</span><br><span class="line">    X = X.reshape((1, 1) + X.shape) </span><br><span class="line">    Y = conv2d(X)                    </span><br><span class="line">    return Y.reshape(Y.shape[2:])  #去掉前面的两维后(原来四维) 进行输出</span><br></pre></td></tr></table></figure></div>

<h4 id="padding"><a href="#padding" class="headerlink" title="padding"></a>padding</h4><p><strong>在所有侧边填充1个像素(padding&#x3D;1, 即(1,1))</strong></p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">conv2d = nn.Conv2d(1, 1, kernel_size=3, padding=1) #输入输出通道数为1, 卷积核大小3x3, 填充为1(上下左右各填充一行)</span><br><span class="line">X = torch.rand(size=(8, 8))         </span><br><span class="line">comp_conv2d(conv2d, X).shape</span><br></pre></td></tr></table></figure></div>

<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; torch.Size([8, 8])</span><br></pre></td></tr></table></figure></div>

<p><strong>填充不同的高度和宽度(padding&#x3D;(2,1))</strong></p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conv2d = nn.Conv2d(1, 1, kernel_size=(5, 3), padding=(2, 1))</span><br><span class="line">comp_conv2d(conv2d, X).shape</span><br></pre></td></tr></table></figure></div>

<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; torch.Size([8, 8])</span><br></pre></td></tr></table></figure></div>

<h4 id="stride"><a href="#stride" class="headerlink" title="stride"></a>stride</h4><p><strong>将高度和宽度的步幅设置为2</strong></p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conv2d = nn.Conv2d(1, 1, kernel_size=3, padding=1, stride=2)</span><br><span class="line">comp_conv2d(conv2d, X).shape</span><br></pre></td></tr></table></figure></div>

<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; torch.Size([4, 4])</span><br></pre></td></tr></table></figure></div>

<p><strong>一个稍微复杂的例子</strong></p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conv2d = nn.Conv2d(1, 1, kernel_size=(3, 5), padding=(0, 1), stride=(3, 4))</span><br><span class="line">comp_conv2d(conv2d, X).shape</span><br></pre></td></tr></table></figure></div>

<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; torch.Size([2, 2])</span><br></pre></td></tr></table></figure></div>

<h2 id="多输入多输出通道"><a href="#多输入多输出通道" class="headerlink" title="多输入多输出通道"></a>多输入多输出通道</h2><h3 id="多个输入通道"><a href="#多个输入通道" class="headerlink" title="多个输入通道"></a>多个输入通道</h3><ul>
<li><p>彩色图像可能有RGB三个通道</p>
</li>
<li><p>转换为灰度会丢失信息</p>
<p><a target="_blank" rel="noopener" href="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/blob/main/imgs/21/21-01.png"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/21/21-01.png"
                      alt="image"
                ></a></p>
</li>
<li><p>每个通道都有一个卷积和，结果是所有通道卷积结果的和</p>
</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/blob/main/imgs/21/21-02.png"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/21/21-02.png"
                      alt="image"
                ></a></p>
<ul>
<li>输入<strong>X</strong>:<a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/ede4ec360206a8830f9e4238a35c436d5881462fce077e695ae3bfd683ccbe56/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e696d6167653f635f7b697d5c74696d65732673706163653b6e5f7b687d5c74696d65732673706163653b6e5f7b777d"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://camo.githubusercontent.com/ede4ec360206a8830f9e4238a35c436d5881462fce077e695ae3bfd683ccbe56/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e696d6167653f635f7b697d5c74696d65732673706163653b6e5f7b687d5c74696d65732673706163653b6e5f7b777d"
                      alt="c_{i}\times n_{h}\times n_{w}"
                ></a></li>
<li>核<strong>W</strong>：<a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/e4217dc5acd1775e3830925e71415d8059621f1b5616a2ae8e53d9264303f872/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e696d6167653f635f7b697d5c74696d65732673706163653b6b5f7b687d5c74696d65732673706163653b6b5f7b777d"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://camo.githubusercontent.com/e4217dc5acd1775e3830925e71415d8059621f1b5616a2ae8e53d9264303f872/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e696d6167653f635f7b697d5c74696d65732673706163653b6b5f7b687d5c74696d65732673706163653b6b5f7b777d"
                      alt="c_{i}\times k_{h}\times k_{w}"
                ></a></li>
<li>输出<strong>Y</strong>:<a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/0704054ba0d6b123fe7d3508371099665253aecec2c758f3cba4005d24305367/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e696d6167653f6d5f7b687d5c74696d65732673706163653b6d5f7b777d"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://camo.githubusercontent.com/0704054ba0d6b123fe7d3508371099665253aecec2c758f3cba4005d24305367/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e696d6167653f6d5f7b687d5c74696d65732673706163653b6d5f7b777d"
                      alt="m_{h}\times m_{w}"
                ></a></li>
</ul>
<p><a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/11eddda729f3b5b08fb8c797e2fe70e66dcd6a4758c676ad943dc41a773fc910/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e696d6167653f593d5c73756d2673706163653b5f7b693d307d5e7b635f7b697d7d585f7b692c3a2c3a7d5c626967737461722673706163653b575f7b692c3a2c3a7d"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://camo.githubusercontent.com/11eddda729f3b5b08fb8c797e2fe70e66dcd6a4758c676ad943dc41a773fc910/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e696d6167653f593d5c73756d2673706163653b5f7b693d307d5e7b635f7b697d7d585f7b692c3a2c3a7d5c626967737461722673706163653b575f7b692c3a2c3a7d"
                      alt="Y=\sum _{i=0}^{c_{i}}X_{i,:,:}\bigstar W_{i,:,:}"
                ></a></p>
<p>多个输入通道：</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from d2l import torch as d2l</span><br><span class="line"></span><br><span class="line">def corr2d_multi_in(X, K):</span><br><span class="line">    return sum(d2l.corr2d(x, k) for x, k in zip(X, K))</span><br></pre></td></tr></table></figure></div>

<h3 id="多个输出通道"><a href="#多个输出通道" class="headerlink" title="多个输出通道"></a>多个输出通道</h3><ul>
<li>无论有多少输入通道，到目前位置我们植绒到单输出通道</li>
<li>我们可以有多个三维卷积核，每个核生成一个输出通道</li>
<li>输入<strong>X</strong>:<a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/e4217dc5acd1775e3830925e71415d8059621f1b5616a2ae8e53d9264303f872/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e696d6167653f635f7b697d5c74696d65732673706163653b6b5f7b687d5c74696d65732673706163653b6b5f7b777d"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://camo.githubusercontent.com/e4217dc5acd1775e3830925e71415d8059621f1b5616a2ae8e53d9264303f872/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e696d6167653f635f7b697d5c74696d65732673706163653b6b5f7b687d5c74696d65732673706163653b6b5f7b777d"
                      alt="c_{i}\times k_{h}\times k_{w}"
                ></a></li>
<li>核<strong>W</strong>：<a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/db922e672cf0c939ff058271b413d0b0c81b0da02a111bf6714bea10c20ea491/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e696d6167653f635f7b6f7d5c74696d65732673706163653b635f7b697d5c74696d65732673706163653b6b5f7b687d5c74696d65732673706163653b6b5f7b777d"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://camo.githubusercontent.com/db922e672cf0c939ff058271b413d0b0c81b0da02a111bf6714bea10c20ea491/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e696d6167653f635f7b6f7d5c74696d65732673706163653b635f7b697d5c74696d65732673706163653b6b5f7b687d5c74696d65732673706163653b6b5f7b777d"
                      alt="c_{o}\times c_{i}\times k_{h}\times k_{w}"
                ></a></li>
<li>输出<strong>Y</strong>：<a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/d7f7c4abac6c1ae3b751fc15b6f42d2592d0ebe0146a2c088f305eade884b6d2/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e696d6167653f635f7b6f7d5c74696d65732673706163653b6d5f7b687d5c74696d65732673706163653b6d5f7b777d"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://camo.githubusercontent.com/d7f7c4abac6c1ae3b751fc15b6f42d2592d0ebe0146a2c088f305eade884b6d2/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e696d6167653f635f7b6f7d5c74696d65732673706163653b6d5f7b687d5c74696d65732673706163653b6d5f7b777d"
                      alt="c_{o}\times m_{h}\times m_{w}"
                ></a></li>
</ul>
<p><a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/ccb530df0e5be9267635cc5817021485ff677fd9b6be34b18bd5a249489a7da1/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e696d6167653f595f7b692c3a2c3a7d3d585c626967737461722673706163653b575f7b692c3a2c3a7d5c71717561642673706163653b666f722673706163653b5c717561642673706163653b693d312c2e2e2e2c635f7b6f7d"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://camo.githubusercontent.com/ccb530df0e5be9267635cc5817021485ff677fd9b6be34b18bd5a249489a7da1/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e696d6167653f595f7b692c3a2c3a7d3d585c626967737461722673706163653b575f7b692c3a2c3a7d5c71717561642673706163653b666f722673706163653b5c717561642673706163653b693d312c2e2e2e2c635f7b6f7d"
                      alt="Y_{i,:,:}=X\bigstar W_{i,:,:}\qquad for \quad i=1,...,c_{o}"
                ></a></p>
<p>多个输出通道：</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">def corr2d_multi_in_out(X, K):</span><br><span class="line">    return torch.stack([corr2d_multi_in(X, k) for k in K], 0)</span><br></pre></td></tr></table></figure></div>

<h3 id="多个输入和输出通道"><a href="#多个输入和输出通道" class="headerlink" title="多个输入和输出通道"></a>多个输入和输出通道</h3><ul>
<li>每个通道可以识别特定的模式</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/blob/main/imgs/21/21-03.png"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/21/21-03.png"
                      alt="image"
                ></a></p>
<ul>
<li>输入通道核识别并组合输入中的模式</li>
</ul>
<h3 id="1X1卷积层"><a href="#1X1卷积层" class="headerlink" title="1X1卷积层"></a>1X1卷积层</h3><p><a target="_blank" rel="noopener" href="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/blob/main/imgs/21/21-04.png"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/21/21-04.png"
                      alt="image"
                ></a></p>
<h3 id="二维卷积层"><a href="#二维卷积层" class="headerlink" title="二维卷积层"></a>二维卷积层</h3><p><a target="_blank" rel="noopener" href="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/blob/main/imgs/21/21-05.png"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/21/21-05.png"
                      alt="image"
                ></a></p>
<h3 id="总结-4"><a href="#总结-4" class="headerlink" title="总结"></a>总结</h3><ul>
<li>输出通道数是卷积层的超参数</li>
<li>每个输入通道有独立的二维卷积和，所有通道结果相加得到一个输出通道结果</li>
<li>每个输出通道有独立的三维卷积核</li>
</ul>
<h3 id="代码-3"><a href="#代码-3" class="headerlink" title="代码"></a>代码</h3><p>多输入互相关运算</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from d2l import torch as d2l</span><br><span class="line"></span><br><span class="line">def corr2d_multi_in(X,K):</span><br><span class="line">	return sum(cd2l.orr2d(x,k) for x,k in zip(X,K))</span><br></pre></td></tr></table></figure></div>

<p>多通道互相关运算</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">def corr2d_multi_out(X,K):</span><br><span class="line">	return torch.stack([d2l.torch(X,k) for k in K],0)</span><br><span class="line">K = torch.stack((K,K+1,K+2),0)</span><br></pre></td></tr></table></figure></div>

<p>1*1卷积</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">def corr2d_multi_in_out_1x1(X, K):</span><br><span class="line">    c_i, h, w = X.shape</span><br><span class="line">    c_o = K.shape[0]</span><br><span class="line">    X = X.reshape((c_i, h * w))</span><br><span class="line">    K = K.reshape((c_o, c_i))</span><br><span class="line">    # 全连接层中的矩阵乘法</span><br><span class="line">    Y = torch.matmul(K, X)</span><br><span class="line">    return Y.reshape((c_o, h, w))</span><br></pre></td></tr></table></figure></div>

<h2 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h2><p>双重目的：降低卷积层对<strong>位置</strong>的敏感性，同时降低对<strong>空间</strong>降采样表示的敏感性。</p>
<h3 id="最大汇聚层和平均汇聚层"><a href="#最大汇聚层和平均汇聚层" class="headerlink" title="最大汇聚层和平均汇聚层"></a>最大汇聚层和平均汇聚层</h3><h4 id="二维最大池化"><a href="#二维最大池化" class="headerlink" title="二维最大池化"></a>二维最大池化</h4><p>返回滑动窗口中的最大值：保留最大信息特征</p>
<h4 id="平均池化层"><a href="#平均池化层" class="headerlink" title="平均池化层"></a>平均池化层</h4><p>将最大操作替换为平均，磨平&#x2F;柔和操作</p>
<h3 id="填充和步幅-1"><a href="#填充和步幅-1" class="headerlink" title="填充和步幅"></a>填充和步幅</h3><p>池化层也具有填充和步幅，跟卷积相似，但是没有kernel的可学习参数</p>
<p>输出通道数&#x3D;输入通道数，将通道融合交给了卷积层</p>
<h3 id="代码-4"><a href="#代码-4" class="headerlink" title="代码"></a>代码</h3><p>实现池化层的正向传播</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torch import nn</span><br><span class="line">from d2l import torch as d2l</span><br><span class="line"></span><br><span class="line">def pool2d(X, pool_size, mode=&#x27;max&#x27;):</span><br><span class="line">    p_h, p_w = pool_size</span><br><span class="line">    Y = torch.zeros((X.shape[0] - p_h + 1, X.shape[1] - p_w + 1))</span><br><span class="line">    for i in range(Y.shape[0]):</span><br><span class="line">        for j in range(Y.shape[1]):</span><br><span class="line">            if mode == &#x27;max&#x27;:</span><br><span class="line">                Y[i, j] = X[i: i + p_h, j: j + p_w].max()</span><br><span class="line">            elif mode == &#x27;avg&#x27;:</span><br><span class="line">                Y[i, j] = X[i: i + p_h, j: j + p_w].mean()</span><br><span class="line">    return Y</span><br></pre></td></tr></table></figure></div>

<h2 id="经典卷积神经网络LeNet"><a href="#经典卷积神经网络LeNet" class="headerlink" title="经典卷积神经网络LeNet"></a>经典卷积神经网络LeNet</h2><h3 id="LeNet卷积神经网络"><a href="#LeNet卷积神经网络" class="headerlink" title="LeNet卷积神经网络"></a>LeNet卷积神经网络</h3><h4 id="手写数字识别"><a href="#手写数字识别" class="headerlink" title="手写数字识别"></a>手写数字识别</h4><ul>
<li>LeNet网络最早是为了应用于手写数字的识别应用。</li>
<li>应用背景：<ul>
<li>邮政局希望可以自动读出信件上的邮政编码</li>
<li>人们希望可以用支票自动取钱</li>
</ul>
</li>
<li>该模型在80年代末的银行被真正的部署</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/blob/main/imgs/23/23-01.png"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/23/23-01.png"
                      alt="image"
                ></a></p>
<h4 id="MNIST"><a href="#MNIST" class="headerlink" title="MNIST"></a>MNIST</h4><ul>
<li>LeNet所使用的数据集</li>
<li>50，000个训练数据</li>
<li>10，000个测试数据</li>
<li>图像大小为28*28</li>
<li>10类</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/blob/main/imgs/23/23-02.png"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/23/23-02.png"
                      alt="image"
                ></a></p>
<h4 id="LeNet的具体模型"><a href="#LeNet的具体模型" class="headerlink" title="LeNet的具体模型"></a>LeNet的具体模型</h4><p><a target="_blank" rel="noopener" href="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/blob/main/imgs/23/23-03.png"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/23/23-03.png"
                      alt="image"
                ></a></p>
<h4 id="总结-5"><a href="#总结-5" class="headerlink" title="总结"></a>总结</h4><ul>
<li>LeNet是早期成功的神经网络</li>
<li>先使用卷积层来学习图片空间信息</li>
<li>然后使用全连接层来转换到类别空间</li>
</ul>
<h3 id="代码部分-2"><a href="#代码部分-2" class="headerlink" title="代码部分"></a>代码部分</h3><h4 id="定义网络结构和准备工作"><a href="#定义网络结构和准备工作" class="headerlink" title="定义网络结构和准备工作"></a>定义网络结构和准备工作</h4><ul>
<li>导入所需的库</li>
</ul>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">#导入所需的库</span><br><span class="line">import torch</span><br><span class="line">from torch import nn</span><br><span class="line">from d2l import torch as d2l</span><br></pre></td></tr></table></figure></div>

<ul>
<li>定义网络结构（具体可参考上文“具体模型”的图）</li>
</ul>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">#定义网络结构</span><br><span class="line">net = nn.Sequential(</span><br><span class="line">    nn.Conv2d(1, 6, kernel_size=5, padding=2), nn.Sigmoid(),</span><br><span class="line">    nn.AvgPool2d(2, stride=2),</span><br><span class="line">    nn.Conv2d(6, 16, kernel_size=5), nn.Sigmoid(),</span><br><span class="line">    nn.AvgPool2d(kernel_size=2, stride=2),nn.Flatten(),</span><br><span class="line">    nn.Linear(16 * 5 * 5, 120), nn.Sigmoid(),</span><br><span class="line">    nn.Linear(120, 84), nn.Sigmoid(),</span><br><span class="line">    nn.Linear(84, 10))</span><br></pre></td></tr></table></figure></div>

<ul>
<li>查看每一层数据的变化情况</li>
</ul>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#把每一层数据的shape给打印出来</span><br><span class="line">X = torch.rand(size=(1, 1, 28, 28), dtype=torch.float32)#创建符合要求的张量</span><br><span class="line">for layer in net:</span><br><span class="line">    X = layer(X)#通过每一层</span><br><span class="line">    print(layer.__class__.__name__,&#x27;output shape: \t&#x27;,X.shape)#打印</span><br></pre></td></tr></table></figure></div>

<h4 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h4><ul>
<li>下载数据集</li>
</ul>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">batch_size = 256 #批量大小</span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size=batch_size)#下载或加载数据集，得到训练和测试集的迭代对象</span><br></pre></td></tr></table></figure></div>

<ul>
<li>使用GPU计算模型在数据集上的精度</li>
</ul>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">def evaluate_accuracy_gpu(net, data_iter, device=None): #@save</span><br><span class="line">    &quot;&quot;&quot;使用GPU计算模型在数据集上的精度&quot;&quot;&quot;</span><br><span class="line">    if isinstance(net, nn.Module):</span><br><span class="line">        net.eval()  # 设置为评估模式</span><br><span class="line">        if not device:</span><br><span class="line">            device = next(iter(net.parameters())).device</span><br><span class="line">    # 正确预测的数量，总预测的数量</span><br><span class="line">    metric = d2l.Accumulator(2)#创建一个累加器，包含2个要累加的元素</span><br><span class="line">    with torch.no_grad():</span><br><span class="line">        for X, y in data_iter:</span><br><span class="line">            if isinstance(X, list):</span><br><span class="line">                # BERT微调所需的（之后将介绍）</span><br><span class="line">                X = [x.to(device) for x in X]</span><br><span class="line">            else:</span><br><span class="line">                X = X.to(device)</span><br><span class="line">            y = y.to(device)</span><br><span class="line">            metric.add(d2l.accuracy(net(X), y), y.numel())#把每一组数据预测结果正确的个数和长度累加</span><br><span class="line">    return metric[0] / metric[1]</span><br></pre></td></tr></table></figure></div>

<ul>
<li>训练函数</li>
</ul>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">def train_ch6(net, train_iter, test_iter, num_epochs, lr, device): </span><br><span class="line">    &quot;&quot;&quot;用GPU训练模型(在第六章定义)&quot;&quot;&quot;</span><br><span class="line">    def init_weights(m):</span><br><span class="line">        if type(m) == nn.Linear or type(m) == nn.Conv2d:</span><br><span class="line">            nn.init.xavier_uniform_(m.weight)#对linear类型的层用xavier初始化</span><br><span class="line">    net.apply(init_weights)</span><br><span class="line">    print(&#x27;training on&#x27;, device)</span><br><span class="line">    net.to(device)</span><br><span class="line">    optimizer = torch.optim.SGD(net.parameters(), lr=lr)</span><br><span class="line">    loss = nn.CrossEntropyLoss()</span><br><span class="line">    animator = d2l.Animator(xlabel=&#x27;epoch&#x27;, xlim=[1, num_epochs],</span><br><span class="line">                            legend=[&#x27;train loss&#x27;, &#x27;train acc&#x27;, &#x27;test acc&#x27;])#动画需要</span><br><span class="line">    timer, num_batches = d2l.Timer(), len(train_iter)</span><br><span class="line">    for epoch in range(num_epochs):</span><br><span class="line">        # 训练损失之和，训练准确率之和，范例数</span><br><span class="line">        metric = d2l.Accumulator(3)</span><br><span class="line">        net.train()</span><br><span class="line">        for i, (X, y) in enumerate(train_iter):</span><br><span class="line">            timer.start()</span><br><span class="line">            optimizer.zero_grad()#梯度清零</span><br><span class="line">            X, y = X.to(device), y.to(device)</span><br><span class="line">            y_hat = net(X)#正向传播</span><br><span class="line">            l = loss(y_hat, y)#计算损失</span><br><span class="line">            l.backward()#反向传播</span><br><span class="line">            optimizer.step()#梯度下降</span><br><span class="line">            with torch.no_grad():</span><br><span class="line">                metric.add(l * X.shape[0], d2l.accuracy(y_hat, y), X.shape[0])#训练损失之和，训练准确率之和，范例数</span><br><span class="line">            timer.stop()</span><br><span class="line">            train_l = metric[0] / metric[2]</span><br><span class="line">            train_acc = metric[1] / metric[2]</span><br><span class="line">            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:</span><br><span class="line">                animator.add(epoch + (i + 1) / num_batches,</span><br><span class="line">                             (train_l, train_acc, None))</span><br><span class="line">        test_acc = evaluate_accuracy_gpu(net, test_iter)#评估测试集的精度</span><br><span class="line">        animator.add(epoch + 1, (None, None, test_acc))</span><br><span class="line">    print(f&#x27;loss &#123;train_l:.3f&#125;, train acc &#123;train_acc:.3f&#125;, &#x27;</span><br><span class="line">          f&#x27;test acc &#123;test_acc:.3f&#125;&#x27;)</span><br><span class="line">    print(f&#x27;&#123;metric[2] * num_epochs / timer.sum():.1f&#125; examples/sec &#x27;</span><br><span class="line">          f&#x27;on &#123;str(device)&#125;&#x27;)</span><br></pre></td></tr></table></figure></div>

<ul>
<li>运行</li>
</ul>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">lr, num_epochs = 0.9, 10</span><br><span class="line">train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())</span><br></pre></td></tr></table></figure></div>



<h1 id="现代卷积神经网络"><a href="#现代卷积神经网络" class="headerlink" title="现代卷积神经网络"></a>现代卷积神经网络</h1><h2 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h2><h3 id="历史"><a href="#历史" class="headerlink" title="历史"></a>历史</h3><h4 id="2000-流行的机器学习方法——SVM，核方法"><a href="#2000-流行的机器学习方法——SVM，核方法" class="headerlink" title="2000 流行的机器学习方法——SVM，核方法"></a>2000 流行的机器学习方法——SVM，核方法</h4><ul>
<li>核方法替代了之前的神经网络网络方法，SVM对于调参不敏感，现在也有一些应用</li>
<li>本质上是特征提取，具体的方法是选择核函数来计算，把特征映射到高纬空间，使得他们线性可分</li>
<li>经过核函数计算之后，原问题可以转化为凸优化问题，这是2006年左右的研究热点</li>
<li>核方法有很多漂亮的定理，有很好的数学解释性</li>
<li>2010年左右，深度学习才兴起</li>
</ul>
<h4 id="2000计算机视觉主要方法——几何学"><a href="#2000计算机视觉主要方法——几何学" class="headerlink" title="2000计算机视觉主要方法——几何学"></a>2000计算机视觉主要方法——几何学</h4><ul>
<li>首先还是对图片进行特征抽取</li>
<li>希望把计算机视觉问题描述成几何问题，建立（非）凸优化模型，可以得到很多漂亮的定理。</li>
<li>可以假设这是一个几何问题，假设这个假设被满足了，可以推出很好的效果</li>
</ul>
<h4 id="2010计算机视觉的热点问题——特征工程"><a href="#2010计算机视觉的热点问题——特征工程" class="headerlink" title="2010计算机视觉的热点问题——特征工程"></a>2010计算机视觉的热点问题——特征工程</h4><ul>
<li>特征工程就是怎么抽取一张图片的特征，因为直接输入一张图片效果非常的差</li>
<li>特征描述子：SIFT,SURF</li>
</ul>
<h4 id="硬件的发展奠定了深度学习的兴起"><a href="#硬件的发展奠定了深度学习的兴起" class="headerlink" title="硬件的发展奠定了深度学习的兴起"></a>硬件的发展奠定了深度学习的兴起</h4><ul>
<li>数据的增长，硬件的计算能力奠定了人们对于方法的选择</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/blob/main/imgs/24/24-01.png"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/24/24-01.png"
                      alt="image"
                ></a></p>
<h4 id="ImageNet（2010）"><a href="#ImageNet（2010）" class="headerlink" title="ImageNet（2010）"></a>ImageNet（2010）</h4><p><a target="_blank" rel="noopener" href="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/blob/main/imgs/24/24-02.png"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/24/24-02.png"
                      alt="image"
                ></a></p>
<ul>
<li><p>AlexNet赢得了2012年ImageNet竞赛冠军</p>
</li>
<li><p>本质上是一个加强版的LeNet，更深更大</p>
</li>
<li><p>AlexNet主要改进措施：</p>
<ul>
<li>dropout（正则）</li>
<li>ReLu（梯度更大）</li>
<li>MaxPooling（取最大值，梯度相对增大）</li>
</ul>
</li>
<li><p>影响：计算机视觉方法论的改变，从人工提取特征过渡到CNN学习特征</p>
<p><a target="_blank" rel="noopener" href="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/blob/main/imgs/24/24-03.png"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/24/24-03.png"
                      alt="image"
                ></a></p>
</li>
</ul>
<h3 id="AlexNet架构"><a href="#AlexNet架构" class="headerlink" title="AlexNet架构"></a>AlexNet架构</h3><p><a target="_blank" rel="noopener" href="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/blob/main/imgs/24/24-04.png"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/24/24-04.png"
                      alt="image"
                ></a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/blob/main/imgs/24/24-05.png"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/24/24-05.png"
                      alt="image"
                ></a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/blob/main/imgs/24/24-06.png"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/24/24-06.png"
                      alt="image"
                ></a></p>
<ul>
<li>网络代码</li>
</ul>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torch import nn</span><br><span class="line">from d2l import torch as d2l</span><br><span class="line"></span><br><span class="line"> net=nn.Sequential(</span><br><span class="line"> 	nn.Conv2d(1,96,kernel_size=11,stride=4,padding=1),</span><br><span class="line"> 	nn.ReLu()，</span><br><span class="line"> 	nn.MaxPool2d(pool_size=3,stride=2),</span><br><span class="line"> 	</span><br><span class="line"> 	nn.Conv2d(96,256,kernel_size=5,stride=1,padding=2),</span><br><span class="line"> 	nn.ReLu()，</span><br><span class="line"> 	nn.MaxPool2d(pool_size=3,stride=2),</span><br><span class="line"> 	</span><br><span class="line"> 	nn.Conv2d(256,384,kernel_size=3,stride=1,padding=1),</span><br><span class="line"> 	nn.ReLu()，</span><br><span class="line"> 	nn.Conv2d(384,384,kernel_size=3,stride=1,padding=1),</span><br><span class="line"> 	nn.ReLu()，</span><br><span class="line"> 	nn.Conv2d(384,384,kernel_size=3,stride=1,padding=1),</span><br><span class="line"> 	nn.ReLu()，</span><br><span class="line"> 	nn.MaxPool2d(pool_size=3,stride=2),</span><br><span class="line"> 	</span><br><span class="line"> 	nn.Linear(6400,4096),nn.ReLu(),nn.dropout(p=0.5),</span><br><span class="line"> 	nn.Linear(4096,4096),nn.ReLu(),nn.dropout(p=0.5),</span><br><span class="line"> 	nn.Linear(4096,1000)</span><br><span class="line"> )</span><br><span class="line"> </span><br><span class="line"> X=torch.rand(1,1,224,224)</span><br></pre></td></tr></table></figure></div>

<ul>
<li>更多细节<ul>
<li>激活函数从sigmoid变成Relu，减缓梯度消失</li>
<li>隐藏全连接层后加入了丢弃层（2个4096之后加入了dropout）</li>
<li>数据增强，将一张图片进行变化，选取多个位置、光照之类的。</li>
</ul>
</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/blob/main/imgs/24/24-07.png"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/24/24-07.png"
                      alt="image"
                ></a></p>
<ul>
<li>复杂度对比<ul>
<li>参数个数增加，每次更新数据增加</li>
</ul>
</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/blob/main/imgs/24/24-08.png"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/24/24-08.png"
                      alt="image"
                ></a></p>
<h3 id="总结-6"><a href="#总结-6" class="headerlink" title="总结"></a>总结</h3><ul>
<li>AlexNet 是更大更深的LeNet，10x参数个数，260x计算复杂度</li>
<li>新加入了dropout，relu，maxpooling，数据增强</li>
<li>标志着新一轮神经网络热潮开始了</li>
</ul>
<h2 id="使用块的网络VGG"><a href="#使用块的网络VGG" class="headerlink" title="使用块的网络VGG"></a>使用块的网络VGG</h2><h3 id="NiNVGG块"><a href="#NiNVGG块" class="headerlink" title="NiNVGG块"></a>NiNVGG块</h3><p>直到现在更深更大的模型也是我们努力的方向，在当时AlexNet比LeNet更深更大得到了更好的精度，大家也希望把网络做的更深更大。选择之一是使用更多的全连接层，但全连接层的成本很高；第二个选择是使用更多的卷积层，但<strong>缺乏好的指导思想</strong>来说明在哪加，加多少。最终<strong>VGG采取了将卷积层组合成块</strong>，再把<strong>卷积块组合到一起</strong>的思路。</p>
<p>VGG块可以看作是AlexNet思路的拓展，AlexNet中将三个相同的卷积层放在一起再加上一个池化层，而VGG将其拓展成可以使用任意个3x3，不改变输入大小的的卷积层，最后加上一个2x2的最大池化层。</p>
<p><a target="_blank" rel="noopener" href="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/blob/main/imgs/25/25-01.PNG"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/25/25-01.PNG"
                      alt="25-01"
                ></a></p>
<p>为什么选择3x3卷积呢？在计算量相同的情况下选用更大的卷积核涉及对网络会越浅，VGG作者经过实验发现用3x3卷积的效果要比5x5好，也就是说神经网络库深且窄的效果会更好。</p>
<h3 id="VGG架构"><a href="#VGG架构" class="headerlink" title="VGG架构"></a>VGG架构</h3><p>多个VGG块后接全连接层，不同次数的重复块得到不同的架构，如VGG-16, VGG-19等，后面的数字取决于网络层数。</p>
<p>可以讲VGG看作是将AlexNet中连续卷积的部分取出加以推广和复制，并删去了AlexNet中不那么规整的前几层。</p>
<p><a target="_blank" rel="noopener" href="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/blob/main/imgs/25/25-02.PNG"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/25/25-02.PNG"
                      alt="25-02"
                ></a></p>
<p>VGG较AlexNet相比性能有很大的提升，而代价是处理样本速度的降低和内存占用的增加。</p>
<h3 id="总结-7"><a href="#总结-7" class="headerlink" title="总结"></a>总结</h3><ul>
<li>VGG使用可重复使用的卷积块来构建深度卷积网络</li>
<li>不同卷积块个数和超参数可以得到不同复杂度的变种</li>
</ul>
<p>这些思想影响了后面神经网络的设计，在之后的模型中被广泛使用。</p>
<h3 id="代码-5"><a href="#代码-5" class="headerlink" title="代码"></a>代码</h3><p>VGG块</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torch import nn</span><br><span class="line">from d2l import torch as d2l</span><br><span class="line"></span><br><span class="line">def vgg_block(num_convs, in_channels, out_channels):</span><br><span class="line">    layers = []</span><br><span class="line">    for _ in range(num_convs):</span><br><span class="line">        layers.append(nn.Conv2d(in_channels, out_channels,</span><br><span class="line">                                kernel_size=3, padding=1))</span><br><span class="line">        layers.append(nn.ReLU())</span><br><span class="line">        in_channels = out_channels</span><br><span class="line">    layers.append(nn.MaxPool2d(kernel_size=2,stride=2))</span><br><span class="line">    return nn.Sequential(*layers)</span><br><span class="line">    </span><br><span class="line">conv_arch = ((1, 64), (1, 128), (2, 256), (2, 512), (2, 512))</span><br><span class="line"></span><br><span class="line">def vgg(conv_arch):</span><br><span class="line">	</span><br></pre></td></tr></table></figure></div>

<h2 id="NiN-网络中的网络"><a href="#NiN-网络中的网络" class="headerlink" title="NiN-网络中的网络"></a>NiN-网络中的网络</h2><h3 id="动机-1"><a href="#动机-1" class="headerlink" title="动机"></a>动机</h3><p><strong>全连接层的问题</strong></p>
<ul>
<li><strong>卷积层</strong>需要的<strong>参数较少</strong></li>
<li>而卷积层后的第一个<strong>全连接层</strong>的<strong>参数较多</strong></li>
</ul>
<p><a target="_blank" rel="noopener" href="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/blob/main/imgs/26/26-01.png"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/26/26-01.png"
                      alt="image"
                ></a></p>
<p>以VGG为例(图示)，全连接层需要先Flatten，输入维度为512x7x7，输出维度为4096，则需要参数个数为512x7x7x4096&#x3D;102M。</p>
<h3 id="NiN块"><a href="#NiN块" class="headerlink" title="NiN块"></a>NiN块</h3><ul>
<li>核心思想：一个卷积层后面跟两个1x1的卷积层，后两层起到全连接层的作用。</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/blob/main/imgs/26/26-02.png"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/26/26-02.png"
                      alt="image"
                ></a></p>
<h3 id="NiN架构"><a href="#NiN架构" class="headerlink" title="NiN架构"></a>NiN架构</h3><ul>
<li>无全连接层</li>
<li>交替使用NiN块和步幅为2的最大池化层<ul>
<li>逐步减小高宽和增大通道数</li>
</ul>
</li>
<li>最后使用全局平均池化得到输出<ul>
<li>其输入通道是类别数</li>
</ul>
</li>
</ul>
<h3 id="NiN-Networks"><a href="#NiN-Networks" class="headerlink" title="NiN Networks"></a>NiN Networks</h3><p><a target="_blank" rel="noopener" href="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/blob/main/imgs/26/26-03.png"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/26/26-03.png"
                      alt="image"
                ></a></p>
<p>NiN架构如上图右边所示，若干个NiN块(图示中为4个块)+池化层；前3个块后接最大池化层，最后一块连接一个全局平均池化层。</p>
<h3 id="总结-8"><a href="#总结-8" class="headerlink" title="总结"></a>总结</h3><ul>
<li>NiN块结构：使用卷积层加两个1x1卷积层<ul>
<li>后者对每个像素增加了非线性性</li>
</ul>
</li>
<li>NiN使用全局平均池化层来替代VGG和AlexNet中的全连接层<ul>
<li>不容易过拟合，更少的参数个数</li>
</ul>
</li>
</ul>
<h3 id="代码-6"><a href="#代码-6" class="headerlink" title="代码"></a>代码</h3><div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 如果在Colab上跑, 或没有安装过d2l包, 需要最开始pip install d2l</span><br><span class="line">!pip install git+https://github.com/d2l-ai/d2l-zh@release  # installing d2l</span><br></pre></td></tr></table></figure></div>

<p><strong>NiN块</strong></p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torch import nn</span><br><span class="line">from d2l import torch as d2l</span><br><span class="line"></span><br><span class="line"># 定义NiN块</span><br><span class="line">def nin_block(in_channels, out_channels, kernel_size, strides, padding):</span><br><span class="line">    return nn.Sequential(</span><br><span class="line">        nn.Conv2d(in_channels, out_channels, kernel_size, strides, padding),</span><br><span class="line">        nn.ReLU(), nn.Conv2d(out_channels, out_channels, kernel_size=1),</span><br><span class="line">        nn.ReLU(), nn.Conv2d(out_channels, out_channels, kernel_size=1),</span><br><span class="line">        nn.ReLU())</span><br></pre></td></tr></table></figure></div>

<p><strong>NiN模型</strong></p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(</span><br><span class="line">    nin_block(1, 96, kernel_size=11, strides=4, padding=0),</span><br><span class="line">    nn.MaxPool2d(3, stride=2),</span><br><span class="line">    nin_block(96, 256, kernel_size=5, strides=1, padding=2),</span><br><span class="line">    nn.MaxPool2d(3, stride=2),</span><br><span class="line">    nin_block(256, 384, kernel_size=3, strides=1, padding=1),</span><br><span class="line">    nn.MaxPool2d(3, stride=2), nn.Dropout(0.5),</span><br><span class="line">    # 标签类别数是10</span><br><span class="line">    nin_block(384, 10, kernel_size=3, strides=1, padding=1),</span><br><span class="line">    nn.AdaptiveAvgPool2d((1, 1)),          #全局平均池化，高宽都变成1</span><br><span class="line">    nn.Flatten())             #消掉最后两个维度, 变成(batch_size, 10)</span><br></pre></td></tr></table></figure></div>

<p><strong>demo测试，查看每个块的输出情况</strong></p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">X = torch.rand(size=(1, 1, 224, 224))</span><br><span class="line">for layer in net:</span><br><span class="line">    X = layer(X)</span><br><span class="line">    print(layer.__class__.__name__, &#x27;output shape:\t&#x27;, X.shape)</span><br></pre></td></tr></table></figure></div>

<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt;</span><br><span class="line">Sequential output shape:	 torch.Size([1, 96, 54, 54])</span><br><span class="line">MaxPool2d output shape:		 torch.Size([1, 96, 26, 26])</span><br><span class="line">Sequential output shape:	 torch.Size([1, 256, 26, 26])</span><br><span class="line">MaxPool2d output shape:		 torch.Size([1, 256, 12, 12])</span><br><span class="line">Sequential output shape:	 torch.Size([1, 384, 12, 12])</span><br><span class="line">MaxPool2d output shape:		 torch.Size([1, 384, 5, 5])</span><br><span class="line">Dropout output shape:		 torch.Size([1, 384, 5, 5])</span><br><span class="line">Sequential output shape:	 torch.Size([1, 10, 5, 5])</span><br><span class="line">AdaptiveAvgPool2d output shape:	 torch.Size([1, 10, 1, 1])</span><br><span class="line">Flatten output shape:		 torch.Size([1, 10])</span><br></pre></td></tr></table></figure></div>

<p><strong>训练模型</strong></p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">lr, num_epochs, batch_size = 0.1, 10, 128</span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=224)</span><br><span class="line">d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())</span><br></pre></td></tr></table></figure></div>

<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; &lt;Figure size 252x180 with 1 Axes&gt;</span><br></pre></td></tr></table></figure></div>

<h2 id="GoogleNet"><a href="#GoogleNet" class="headerlink" title="GoogleNet"></a>GoogleNet</h2><h3 id="含并行连结的网络"><a href="#含并行连结的网络" class="headerlink" title="含并行连结的网络"></a>含并行连结的网络</h3><ul>
<li>GoogLeNet吸收了NiN中串联网络的思想，并在此基础上做了改进。我们往往不确定到底选取什么样的层效果更好，到底是3X3卷积层还是5X5的卷积层，诸如此类的问题是GooLeNet选择了另一种思路“小学生才做选择，我全都要”，这也使得GooLeNet成为了第一个模型中超过1000个层的模型。</li>
</ul>
<h3 id="Inception块"><a href="#Inception块" class="headerlink" title="Inception块"></a>Inception块</h3><ul>
<li><p>在GoogLeNet中，基本的卷积块被称为<em>Inception块</em>（Inception block）</p>
<p><a target="_blank" rel="noopener" href="https://github.com/kinza99/DeepLearning-MuLi-Notes/blob/main/imgs/27/27-1.png"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/kinza99/DeepLearning-MuLi-Notes/raw/main/imgs/27/27-1.png"
                      alt="截屏2022-01-23 上午10.11.18"
                ></a></p>
</li>
<li><p>Inception块由四条并行路径组成。 前三条路径使用窗口大小为1×11×1、3×33×3和5×55×5的卷积层，从不同空间大小中提取信息。 中间的两条路径在输入上执行1×11×1卷积，以减少通道数，从而降低模型的复杂性。 第四条路径使用3×33×3最大汇聚层，然后使用1×11×1卷积层来改变通道数。 这四条路径都使用合适的填充来使输入与输出的高和宽一致，最后我们将每条线路的输出在通道维度上连结，并构成Inception块的输出。在Inception块中，通常调整的超参数是每层输出通道数。</p>
</li>
</ul>
<h3 id="GooLeNet模型"><a href="#GooLeNet模型" class="headerlink" title="GooLeNet模型"></a>GooLeNet模型</h3><ul>
<li>GoogLeNet一共使用9个Inception块和全局平均汇聚层的堆叠来生成其估计值。Inception块之间的最大汇聚层可降低维度。 第一个模块类似于AlexNet和LeNet，Inception块的组合从VGG继承，全局平均汇聚层避免了在最后使用全连接层。<a target="_blank" rel="noopener" href="https://github.com/kinza99/DeepLearning-MuLi-Notes/blob/main/imgs/27/27-2.png"><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/kinza99/DeepLearning-MuLi-Notes/raw/main/imgs/27/27-2.png"
                      alt="截屏2022-01-23 上午10.17.11"
                ></a></li>
<li>第一个模块是7×7卷积层。</li>
<li>第二个模块使用两个卷积层：第一个卷积层是1×1卷积层；第二个卷积层使用将通道数量增加三倍的3×3卷积层。 这对应于Inception块中的第二条路径。</li>
<li>第三个模块串联两个完整的Inception块。 第一个Inception块的输出通道数为64+128+32+32&#x3D;25664+128+32+32&#x3D;256，四个路径之间的输出通道数量比为64:128:32:32&#x3D;2:4:1:164:128:32:32&#x3D;2:4:1:1。 第二个和第三个路径首先将输入通道的数量分别减少到96&#x2F;192&#x3D;1&#x2F;296&#x2F;192&#x3D;1&#x2F;2和16&#x2F;192&#x3D;1&#x2F;1216&#x2F;192&#x3D;1&#x2F;12，然后连接第二个卷积层。第二个Inception块的输出通道数增加到128+192+96+64&#x3D;480128+192+96+64&#x3D;480，四个路径之间的输出通道数量比为128:192:96:64&#x3D;4:6:3:2128:192:96:64&#x3D;4:6:3:2。 第二条和第三条路径首先将输入通道的数量分别减少到128&#x2F;256&#x3D;1&#x2F;2128&#x2F;256&#x3D;1&#x2F;2和32&#x2F;256&#x3D;1&#x2F;832&#x2F;256&#x3D;1&#x2F;8。</li>
<li>第四模块更加复杂， 它串联了5个Inception块，其输出通道数分别是192+208+48+64&#x3D;512192+208+48+64&#x3D;512、160+224+64+64&#x3D;512160+224+64+64&#x3D;512、128+256+64+64&#x3D;512128+256+64+64&#x3D;512、112+288+64+64&#x3D;528112+288+64+64&#x3D;528和256+320+128+128&#x3D;832256+320+128+128&#x3D;832。 这些路径的通道数分配和第三模块中的类似，首先是含3×3卷积层的第二条路径输出最多通道，其次是仅含1×1卷积层的第一条路径，之后是含5×5卷积层的第三条路径和含3×3最大汇聚层的第四条路径。 其中第二、第三条路径都会先按比例减小通道数。 这些比例在各个Inception块中都略有不同。</li>
</ul>
<h3 id="总结-9"><a href="#总结-9" class="headerlink" title="总结"></a>总结</h3><ul>
<li>Inception块相当于一个有4条路径的子网络。它通过不同窗口形状的卷积层和最大汇聚层来并行抽取信息，并使用1×1卷积层减少每像素级别上的通道维数从而降低模型复杂度。</li>
<li>GoogLeNet将多个设计精细的Inception块与其他层（卷积层、全连接层）串联起来。其中Inception块的通道数分配之比是在ImageNet数据集上通过大量的实验得来的。</li>
<li>GoogLeNet和它的后继者们一度是ImageNet上最有效的模型之一：它以较低的计算复杂度提供了类似的测试精度。</li>
</ul>
<h2 id="批量归一化"><a href="#批量归一化" class="headerlink" title="批量归一化"></a>批量归一化</h2><h2 id="残差网络ResNet"><a href="#残差网络ResNet" class="headerlink" title="残差网络ResNet"></a>残差网络ResNet</h2><h1 id="循环神经网络"><a href="#循环神经网络" class="headerlink" title="循环神经网络"></a>循环神经网络</h1><h1 id="现代循环神经网络"><a href="#现代循环神经网络" class="headerlink" title="现代循环神经网络"></a>现代循环神经网络</h1><h1 id="注意力机制"><a href="#注意力机制" class="headerlink" title="注意力机制"></a>注意力机制</h1><h1 id="优化算法"><a href="#优化算法" class="headerlink" title="优化算法"></a>优化算法</h1><h1 id="计算性能"><a href="#计算性能" class="headerlink" title="计算性能"></a>计算性能</h1><h1 id="自然语言处理"><a href="#自然语言处理" class="headerlink" title="自然语言处理"></a>自然语言处理</h1>
        </div>

        
            <div class="post-copyright-info w-full my-8 px-2 sm:px-6 md:px-8">
                <div class="article-copyright-info-container">
    <ul>
        <li><strong>Title:</strong> lm_pytorch</li>
        <li><strong>Author:</strong> YIWEI WU</li>
        <li><strong>Created at
                :</strong> 2024-06-29 21:14:37</li>
        
            <li>
                <strong>Updated at
                    :</strong> 2024-07-05 17:53:13
            </li>
        
        <li>
            <strong>Link:</strong> https://sonder730.github.io/2024/06/29/lm-pytorch/
        </li>
        <li>
            <strong>
                License:
            </strong>
            

            
                This work is licensed under <a class="license" target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0">CC BY-NC-SA 4.0</a>.
            
        </li>
    </ul>
</div>

            </div>
        

        
            <ul class="post-tags-box text-lg mt-1.5 flex-wrap justify-center flex md:hidden">
                
                    <li class="tag-item mx-0.5">
                        <a href="/tags/DEEP-LEARNING/">#DEEP LEARNING</a>&nbsp;
                    </li>
                
            </ul>
        

        

        
            <div class="article-nav my-8 flex justify-between items-center px-2 sm:px-6 md:px-8">
                
                
                    <div class="article-next border-border-color shadow-redefine-flat shadow-shadow-color-2 rounded-medium px-4 py-2 hover:shadow-redefine-flat-hover hover:shadow-shadow-color-2">
                        <a class="next"
                        rel="next"
                        href="/2024/06/28/PYTHON%E4%B9%A6%E5%86%99%E6%8C%87%E5%8D%97%EF%BC%88%E6%91%98%E5%BD%95%EF%BC%89/"
                        >
                            <span class="title flex justify-center items-center">
                                <span class="post-nav-title-item"></span>
                                <span class="post-nav-item">Next posts</span>
                            </span>
                            <span class="right arrow-icon flex justify-center items-center">
                                <i class="fa-solid fa-chevron-right"></i>
                            </span>
                        </a>
                    </div>
                
            </div>
        


        
            <div class="comment-container px-2 sm:px-6 md:px-8 pb-8">
                <div class="comments-container mt-10 w-full ">
    <div id="comment-anchor" class="w-full h-2.5"></div>
    <div class="comment-area-title w-full my-1.5 md:my-2.5 text-xl md:text-3xl font-bold">
        Comments
    </div>
    

        
            
    <div id="gitalk-container"></div>
    <script data-swup-reload-script
            src="//cdn.staticfile.org/gitalk/1.8.0/gitalk.min.js"></script>
    <script data-swup-reload-script>

        function loadGitalk() {
            let __gitalk__pathname = decodeURI(location.pathname);
            const __gitalk__pathnameLength = __gitalk__pathname.length;
            const __gitalk__pathnameMaxLength = 50;
            if (__gitalk__pathnameLength > __gitalk__pathnameMaxLength) {
                __gitalk__pathname = __gitalk__pathname.substring(0, __gitalk__pathnameMaxLength - 3) + '...';
            }

            try {
                Gitalk && new Gitalk({
                    clientID: 'Ov23ctUONHgVQxZaY0TW',
                    clientSecret: '80b0952f13f12f5755f3f11de78f456aa0464fbb',
                    repo: 'Gitalk',
                    owner: 'SONDER730',
                    admin: ['SONDER730'],
                    id: __gitalk__pathname,
                    language: 'en',
                    proxy: 'https://github.com/login/oauth/access_token'
                }).render('gitalk-container');

            } catch (e) {
                window.Gitalk = null;
            }
        }

        if ('true') {
            const loadGitalkTimeout = setTimeout(() => {
                loadGitalk();
                clearTimeout(loadGitalkTimeout);
            }, 1000);
        } else {
            window.addEventListener('DOMContentLoaded', loadGitalk);
        }
    </script>



        
    
</div>

            </div>
        
    </div>

    
        <div class="toc-content-container">
            <div class="post-toc-wrap">
    <div class="post-toc">
        <div class="toc-title">On this page</div>
        <div class="page-title">lm_pytorch</div>
        <ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%BA%BF%E6%80%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">1.</span> <span class="nav-text">线性神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%90%91%E9%87%8F%E9%93%BE%E5%BC%8F%E6%B3%95%E5%88%99"><span class="nav-number">1.1.</span> <span class="nav-text">向量链式法则</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A0%87%E9%87%8F%E9%93%BE%E5%BC%8F%E6%B3%95%E5%88%99"><span class="nav-number">1.1.1.</span> <span class="nav-text">标量链式法则</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%8B%93%E5%B1%95%E5%88%B0%E5%90%91%E9%87%8F"><span class="nav-number">1.1.2.</span> <span class="nav-text">拓展到向量</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%87%AA%E5%8A%A8%E6%B1%82%E5%AF%BC"><span class="nav-number">1.1.3.</span> <span class="nav-text">自动求导</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%A3%E7%A0%81%E9%83%A8%E5%88%86"><span class="nav-number">1.1.4.</span> <span class="nav-text">代码部分</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92-%E5%9F%BA%E7%A1%80%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95"><span class="nav-number">1.2.</span> <span class="nav-text">线性回归+基础优化算法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="nav-number">1.2.1.</span> <span class="nav-text">线性回归</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E5%9F%BA%E7%A1%80%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95"><span class="nav-number">1.2.2.</span> <span class="nav-text">2.基础优化算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%9A%84%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E5%AE%9E%E7%8E%B0"><span class="nav-number">1.2.3.</span> <span class="nav-text">3.线性回归的从零开始实现</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-%E6%96%B0%E5%9E%8B%E5%9B%9E%E5%BD%92%E7%9A%84%E7%AE%80%E6%B4%81%E5%AE%9E%E7%8E%B0"><span class="nav-number">1.2.4.</span> <span class="nav-text">4.新型回归的简洁实现</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Softmax-%E5%9B%9E%E5%BD%92-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0-%E5%9B%BE%E7%89%87%E5%88%86%E7%B1%BB%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="nav-number">1.3.</span> <span class="nav-text">Softmax 回归 + 损失函数 + 图片分类数据集</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9B%9E%E5%BD%92VS%E5%88%86%E7%B1%BB%EF%BC%9A"><span class="nav-number">1.3.1.</span> <span class="nav-text">回归VS分类：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Softmax%E5%92%8C%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1"><span class="nav-number">1.3.2.</span> <span class="nav-text">Softmax和交叉熵损失</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="nav-number">1.3.3.</span> <span class="nav-text">2.损失函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9B%BE%E7%89%87%E5%88%86%E7%B1%BB%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="nav-number">1.3.4.</span> <span class="nav-text">图片分类数据集</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%8E%E9%9B%B6%E5%AE%9E%E7%8E%B0softmax%E5%9B%9E%E5%BD%92"><span class="nav-number">1.3.5.</span> <span class="nav-text">从零实现softmax回归</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#softmax%E7%9A%84%E7%AE%80%E6%B4%81%E5%AE%9E%E7%8E%B0"><span class="nav-number">1.3.6.</span> <span class="nav-text">softmax的简洁实现</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA"><span class="nav-number">2.</span> <span class="nav-text">多层感知机</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA%EF%BC%88MLP%EF%BC%89"><span class="nav-number">2.1.</span> <span class="nav-text">多层感知机（MLP）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%84%9F%E7%9F%A5%E6%9C%BA"><span class="nav-number">2.1.1.</span> <span class="nav-text">感知机</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA-1"><span class="nav-number">2.1.2.</span> <span class="nav-text">多层感知机</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0"><span class="nav-number">2.1.3.</span> <span class="nav-text">代码实现</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9-%E8%BF%87%E6%8B%9F%E5%90%88%E5%92%8C%E6%AC%A0%E6%8B%9F%E5%90%88"><span class="nav-number">2.2.</span> <span class="nav-text">模型选择 + 过拟合和欠拟合</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A6%82%E5%BF%B5%E5%8C%BA%E5%88%86"><span class="nav-number">2.2.1.</span> <span class="nav-text">概念区分</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#K-%E5%88%99%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81"><span class="nav-number">2.2.2.</span> <span class="nav-text">K-则交叉验证</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%BF%87%E6%8B%9F%E5%90%88%E5%92%8C%E6%AC%A0%E6%8B%9F%E5%90%88"><span class="nav-number">2.2.3.</span> <span class="nav-text">过拟合和欠拟合</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%A3%E7%A0%81%EF%BC%88%E5%A4%9A%E9%A1%B9%E5%BC%8F%E5%9B%9E%E5%BD%92%EF%BC%89"><span class="nav-number">2.2.4.</span> <span class="nav-text">代码（多项式回归）</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%9D%83%E9%87%8D%E8%A1%B0%E9%80%80%EF%BC%88%E6%AD%A3%E5%88%99%E5%8C%96%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%8A%80%E6%9C%AF%EF%BC%89"><span class="nav-number">2.3.</span> <span class="nav-text">权重衰退（正则化模型的技术）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%A1%AC%E6%80%A7%E9%99%90%E5%88%B6-%E7%9B%B4%E8%A7%82%E7%90%86%E8%A7%A3"><span class="nav-number">2.3.1.</span> <span class="nav-text">硬性限制&#x2F;直观理解</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9F%94%E6%80%A7%E9%99%90%E5%88%B6-%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8"><span class="nav-number">2.3.2.</span> <span class="nav-text">柔性限制&#x2F;实际应用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%82%E6%95%B0%E6%9B%B4%E6%96%B0"><span class="nav-number">2.3.3.</span> <span class="nav-text">参数更新</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%80%BB%E7%BB%93"><span class="nav-number">2.3.4.</span> <span class="nav-text">总结</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%A3%E7%A0%81"><span class="nav-number">2.3.5.</span> <span class="nav-text">代码</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%A2%E5%BC%83%E6%B3%95"><span class="nav-number">2.4.</span> <span class="nav-text">丢弃法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%A2%E5%BC%83%E6%B3%95%E5%8A%A8%E6%9C%BA%E3%80%81%E5%AE%9E%E7%8E%B0%E5%8F%8A%E5%8E%9F%E5%88%99"><span class="nav-number">2.4.1.</span> <span class="nav-text">丢弃法动机、实现及原则</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%A2%E5%BC%83%E6%B3%95%E5%86%85%E5%AE%B9"><span class="nav-number">2.4.2.</span> <span class="nav-text">丢弃法内容</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%A2%E5%BC%83%E6%B3%95%E4%BD%BF%E7%94%A8"><span class="nav-number">2.4.3.</span> <span class="nav-text">丢弃法使用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%80%BB%E7%BB%93-1"><span class="nav-number">2.4.4.</span> <span class="nav-text">总结</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%A3%E7%A0%81%E9%83%A8%E5%88%86-1"><span class="nav-number">2.4.5.</span> <span class="nav-text">代码部分</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%95%B0%E5%80%BC%E7%A8%B3%E5%AE%9A%E6%80%A7-%E6%A8%A1%E5%9E%8B%E5%88%9D%E5%A7%8B%E5%8C%96%E5%92%8C%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="nav-number">2.5.</span> <span class="nav-text">数值稳定性 + 模型初始化和激活函数</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%95%B0%E5%80%BC%E7%A8%B3%E5%AE%9A%E6%80%A7"><span class="nav-number">2.5.1.</span> <span class="nav-text">数值稳定性</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E5%88%9D%E5%A7%8B%E5%8C%96%E5%92%8C%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="nav-number">2.5.2.</span> <span class="nav-text">模型初始化和激活函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%80%BB%E7%BB%93-2"><span class="nav-number">2.5.3.</span> <span class="nav-text">总结</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%9E%E6%88%98"><span class="nav-number">2.6.</span> <span class="nav-text">实战</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%A1%E7%AE%97"><span class="nav-number">3.</span> <span class="nav-text">深度学习计算</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#PyTorch-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80"><span class="nav-number">3.1.</span> <span class="nav-text">PyTorch 神经网络基础</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B1%82%E5%92%8C%E5%9D%97"><span class="nav-number">3.1.1.</span> <span class="nav-text">层和块</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%82%E6%95%B0%E7%AE%A1%E7%90%86"><span class="nav-number">3.1.2.</span> <span class="nav-text">参数管理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BB%B6%E5%90%8E%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="nav-number">3.1.3.</span> <span class="nav-text">延后初始化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%87%AA%E5%AE%9A%E4%B9%89%E5%B1%82"><span class="nav-number">3.1.4.</span> <span class="nav-text">自定义层</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AF%BB%E5%86%99%E6%96%87%E4%BB%B6"><span class="nav-number">3.1.5.</span> <span class="nav-text">读写文件</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">4.</span> <span class="nav-text">卷积神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8D%B7%E7%A7%AF%E5%B1%82"><span class="nav-number">4.1.</span> <span class="nav-text">卷积层</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B9%B3%E7%A7%BB%E4%B8%8D%E5%8F%98%E6%80%A7"><span class="nav-number">4.1.1.</span> <span class="nav-text">平移不变性</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B1%80%E9%83%A8%E6%80%A7"><span class="nav-number">4.1.2.</span> <span class="nav-text">局部性</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%A3%E7%A0%81-1"><span class="nav-number">4.1.3.</span> <span class="nav-text">代码</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8D%B7%E7%A7%AF%E5%B1%82%E9%87%8C%E7%9A%84%E5%A1%AB%E5%85%85%E5%92%8C%E6%AD%A5%E5%B9%85"><span class="nav-number">4.2.</span> <span class="nav-text">卷积层里的填充和步幅</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A1%AB%E5%85%85"><span class="nav-number">4.2.1.</span> <span class="nav-text">填充</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%AD%A5%E5%B9%85"><span class="nav-number">4.2.2.</span> <span class="nav-text">步幅</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%80%BB%E7%BB%93-3"><span class="nav-number">4.2.3.</span> <span class="nav-text">总结</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%A3%E7%A0%81-2"><span class="nav-number">4.2.4.</span> <span class="nav-text">代码</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%A4%9A%E8%BE%93%E5%85%A5%E5%A4%9A%E8%BE%93%E5%87%BA%E9%80%9A%E9%81%93"><span class="nav-number">4.3.</span> <span class="nav-text">多输入多输出通道</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A4%9A%E4%B8%AA%E8%BE%93%E5%85%A5%E9%80%9A%E9%81%93"><span class="nav-number">4.3.1.</span> <span class="nav-text">多个输入通道</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A4%9A%E4%B8%AA%E8%BE%93%E5%87%BA%E9%80%9A%E9%81%93"><span class="nav-number">4.3.2.</span> <span class="nav-text">多个输出通道</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A4%9A%E4%B8%AA%E8%BE%93%E5%85%A5%E5%92%8C%E8%BE%93%E5%87%BA%E9%80%9A%E9%81%93"><span class="nav-number">4.3.3.</span> <span class="nav-text">多个输入和输出通道</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1X1%E5%8D%B7%E7%A7%AF%E5%B1%82"><span class="nav-number">4.3.4.</span> <span class="nav-text">1X1卷积层</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BA%8C%E7%BB%B4%E5%8D%B7%E7%A7%AF%E5%B1%82"><span class="nav-number">4.3.5.</span> <span class="nav-text">二维卷积层</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%80%BB%E7%BB%93-4"><span class="nav-number">4.3.6.</span> <span class="nav-text">总结</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%A3%E7%A0%81-3"><span class="nav-number">4.3.7.</span> <span class="nav-text">代码</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%B1%A0%E5%8C%96%E5%B1%82"><span class="nav-number">4.4.</span> <span class="nav-text">池化层</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9C%80%E5%A4%A7%E6%B1%87%E8%81%9A%E5%B1%82%E5%92%8C%E5%B9%B3%E5%9D%87%E6%B1%87%E8%81%9A%E5%B1%82"><span class="nav-number">4.4.1.</span> <span class="nav-text">最大汇聚层和平均汇聚层</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A1%AB%E5%85%85%E5%92%8C%E6%AD%A5%E5%B9%85-1"><span class="nav-number">4.4.2.</span> <span class="nav-text">填充和步幅</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%A3%E7%A0%81-4"><span class="nav-number">4.4.3.</span> <span class="nav-text">代码</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BB%8F%E5%85%B8%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CLeNet"><span class="nav-number">4.5.</span> <span class="nav-text">经典卷积神经网络LeNet</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#LeNet%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">4.5.1.</span> <span class="nav-text">LeNet卷积神经网络</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%A3%E7%A0%81%E9%83%A8%E5%88%86-2"><span class="nav-number">4.5.2.</span> <span class="nav-text">代码部分</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%8E%B0%E4%BB%A3%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">5.</span> <span class="nav-text">现代卷积神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#AlexNet"><span class="nav-number">5.1.</span> <span class="nav-text">AlexNet</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8E%86%E5%8F%B2"><span class="nav-number">5.1.1.</span> <span class="nav-text">历史</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#AlexNet%E6%9E%B6%E6%9E%84"><span class="nav-number">5.1.2.</span> <span class="nav-text">AlexNet架构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%80%BB%E7%BB%93-6"><span class="nav-number">5.1.3.</span> <span class="nav-text">总结</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8%E5%9D%97%E7%9A%84%E7%BD%91%E7%BB%9CVGG"><span class="nav-number">5.2.</span> <span class="nav-text">使用块的网络VGG</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#NiNVGG%E5%9D%97"><span class="nav-number">5.2.1.</span> <span class="nav-text">NiNVGG块</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#VGG%E6%9E%B6%E6%9E%84"><span class="nav-number">5.2.2.</span> <span class="nav-text">VGG架构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%80%BB%E7%BB%93-7"><span class="nav-number">5.2.3.</span> <span class="nav-text">总结</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%A3%E7%A0%81-5"><span class="nav-number">5.2.4.</span> <span class="nav-text">代码</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#NiN-%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E7%BD%91%E7%BB%9C"><span class="nav-number">5.3.</span> <span class="nav-text">NiN-网络中的网络</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8A%A8%E6%9C%BA-1"><span class="nav-number">5.3.1.</span> <span class="nav-text">动机</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#NiN%E5%9D%97"><span class="nav-number">5.3.2.</span> <span class="nav-text">NiN块</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#NiN%E6%9E%B6%E6%9E%84"><span class="nav-number">5.3.3.</span> <span class="nav-text">NiN架构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#NiN-Networks"><span class="nav-number">5.3.4.</span> <span class="nav-text">NiN Networks</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%80%BB%E7%BB%93-8"><span class="nav-number">5.3.5.</span> <span class="nav-text">总结</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%A3%E7%A0%81-6"><span class="nav-number">5.3.6.</span> <span class="nav-text">代码</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#GoogleNet"><span class="nav-number">5.4.</span> <span class="nav-text">GoogleNet</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%90%AB%E5%B9%B6%E8%A1%8C%E8%BF%9E%E7%BB%93%E7%9A%84%E7%BD%91%E7%BB%9C"><span class="nav-number">5.4.1.</span> <span class="nav-text">含并行连结的网络</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Inception%E5%9D%97"><span class="nav-number">5.4.2.</span> <span class="nav-text">Inception块</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#GooLeNet%E6%A8%A1%E5%9E%8B"><span class="nav-number">5.4.3.</span> <span class="nav-text">GooLeNet模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%80%BB%E7%BB%93-9"><span class="nav-number">5.4.4.</span> <span class="nav-text">总结</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%89%B9%E9%87%8F%E5%BD%92%E4%B8%80%E5%8C%96"><span class="nav-number">5.5.</span> <span class="nav-text">批量归一化</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%AE%8B%E5%B7%AE%E7%BD%91%E7%BB%9CResNet"><span class="nav-number">5.6.</span> <span class="nav-text">残差网络ResNet</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">6.</span> <span class="nav-text">循环神经网络</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%8E%B0%E4%BB%A3%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">7.</span> <span class="nav-text">现代循环神经网络</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6"><span class="nav-number">8.</span> <span class="nav-text">注意力机制</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95"><span class="nav-number">9.</span> <span class="nav-text">优化算法</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%AE%A1%E7%AE%97%E6%80%A7%E8%83%BD"><span class="nav-number">10.</span> <span class="nav-text">计算性能</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86"><span class="nav-number">11.</span> <span class="nav-text">自然语言处理</span></a></li></ol>

    </div>
</div>
        </div>
    
</div>



                

            </div>

            

        </div>

        <div class="main-content-footer">
            <footer class="footer mt-5 py-5 h-auto text-base text-third-text-color relative border-t-2 border-t-border-color">
    <div class="info-container py-3 text-center">
        
        <div class="text-center">
            &copy;
            
              <span>2024</span>
              -
            
            2024&nbsp;&nbsp;<i class="fa-solid fa-heart fa-beat" style="--fa-animation-duration: 0.5s; color: #f54545"></i>&nbsp;&nbsp;<a href="/">YIWEI WU</a>
            
                
                <p class="post-count space-x-0.5">
                    <span>
                        3 posts in total
                    </span>
                    
                </p>
            
        </div>
        
            <script data-swup-reload-script src="https://cn.vercount.one/js"></script>
            <div class="relative text-center lg:absolute lg:right-[20px] lg:top-1/2 lg:-translate-y-1/2 lg:text-right">
                
                    <span id="busuanzi_container_site_uv" class="lg:!block">
                        <span class="text-sm">VISITOR COUNT</span>
                        <span id="busuanzi_value_site_uv"></span>
                    </span>
                
                
                    <span id="busuanzi_container_site_pv" class="lg:!block">
                        <span class="text-sm">TOTAL PAGE VIEWS</span>
                        <span id="busuanzi_value_site_pv"></span>
                    </span>
                
            </div>
        
        <div class="relative text-center lg:absolute lg:left-[20px] lg:top-1/2 lg:-translate-y-1/2 lg:text-left">
            <span class="lg:block text-sm">POWERED BY <?xml version="1.0" encoding="utf-8"?><!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd"><svg class="relative top-[2px] inline-block align-baseline" version="1.1" id="圖層_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" width="1rem" height="1rem" viewBox="0 0 512 512" enable-background="new 0 0 512 512" xml:space="preserve"><path fill="#0E83CD" d="M256.4,25.8l-200,115.5L56,371.5l199.6,114.7l200-115.5l0.4-230.2L256.4,25.8z M349,354.6l-18.4,10.7l-18.6-11V275H200v79.6l-18.4,10.7l-18.6-11v-197l18.5-10.6l18.5,10.8V237h112v-79.6l18.5-10.6l18.5,10.8V354.6z"/></svg><a target="_blank" class="text-base" href="https://hexo.io">Hexo</a></span>
            <span class="text-sm lg:block">THEME&nbsp;<a class="text-base" target="_blank" href="https://github.com/EvanNotFound/hexo-theme-redefine">Redefine v2.6.4</a></span>
        </div>
        
        
            <div>
                Blog up for <span class="odometer" id="runtime_days" ></span> days <span class="odometer" id="runtime_hours"></span> hrs <span class="odometer" id="runtime_minutes"></span> Min <span class="odometer" id="runtime_seconds"></span> Sec
            </div>
        
        
            <script data-swup-reload-script>
                try {
                    function odometer_init() {
                    const elements = document.querySelectorAll('.odometer');
                    elements.forEach(el => {
                        new Odometer({
                            el,
                            format: '( ddd).dd',
                            duration: 200
                        });
                    });
                    }
                    odometer_init();
                } catch (error) {}
            </script>
        
        
        
    </div>  
</footer>
        </div>
    </div>

    
        <div class="post-tools">
            <div class="post-tools-container">
    <ul class="article-tools-list">
        <!-- TOC aside toggle -->
        
            <li class="right-bottom-tools page-aside-toggle">
                <i class="fa-regular fa-outdent"></i>
            </li>
        

        <!-- go comment -->
        
            <li class="go-comment">
                <i class="fa-regular fa-comments"></i>
            </li>
        
    </ul>
</div>

        </div>
    

    <div class="right-side-tools-container">
        <div class="side-tools-container">
    <ul class="hidden-tools-list">
        <li class="right-bottom-tools tool-font-adjust-plus flex justify-center items-center">
            <i class="fa-regular fa-magnifying-glass-plus"></i>
        </li>

        <li class="right-bottom-tools tool-font-adjust-minus flex justify-center items-center">
            <i class="fa-regular fa-magnifying-glass-minus"></i>
        </li>

        <li class="right-bottom-tools tool-dark-light-toggle flex justify-center items-center">
            <i class="fa-regular fa-moon"></i>
        </li>

        <!-- rss -->
        

        

        <li class="right-bottom-tools tool-scroll-to-bottom flex justify-center items-center">
            <i class="fa-regular fa-arrow-down"></i>
        </li>
    </ul>

    <ul class="visible-tools-list">
        <li class="right-bottom-tools toggle-tools-list flex justify-center items-center">
            <i class="fa-regular fa-cog fa-spin"></i>
        </li>
        
            <li class="right-bottom-tools tool-scroll-to-top flex justify-center items-center">
                <i class="arrow-up fas fa-arrow-up"></i>
                <span class="percent"></span>
            </li>
        
        
    </ul>
</div>

    </div>

    <div class="image-viewer-container">
    <img src="">
</div>


    

</main>


    
<script src="/js/libs/Swup.min.js"></script>

<script src="/js/libs/SwupSlideTheme.min.js"></script>

<script src="/js/libs/SwupScriptsPlugin.min.js"></script>

<script src="/js/libs/SwupProgressPlugin.min.js"></script>

<script src="/js/libs/SwupScrollPlugin.min.js"></script>

<script src="/js/libs/SwupPreloadPlugin.min.js"></script>

<script>
    const swup = new Swup({
        plugins: [
            new SwupScriptsPlugin({
                optin: true,
            }),
            new SwupProgressPlugin(),
            new SwupScrollPlugin({
                offset: 80,
            }),
            new SwupSlideTheme({
                mainElement: ".main-content-body",
            }),
            new SwupPreloadPlugin(),
        ],
        containers: ["#swup"],
    });
</script>







<script src="/js/tools/imageViewer.js" type="module"></script>

<script src="/js/utils.js" type="module"></script>

<script src="/js/main.js" type="module"></script>

<script src="/js/layouts/navbarShrink.js" type="module"></script>

<script src="/js/tools/scrollTopBottom.js" type="module"></script>

<script src="/js/tools/lightDarkSwitch.js" type="module"></script>

<script src="/js/layouts/categoryList.js" type="module"></script>





    
<script src="/js/tools/codeBlock.js" type="module"></script>




    
<script src="/js/layouts/lazyload.js" type="module"></script>




    
<script src="/js/tools/runtime.js"></script>

    
<script src="/js/libs/odometer.min.js"></script>

    
<link rel="stylesheet" href="/assets/odometer-theme-minimal.css">




  
<script src="/js/libs/Typed.min.js"></script>

  
<script src="/js/plugins/typed.js" type="module"></script>









<div class="post-scripts" data-swup-reload-script>
    
        
<script src="/js/tools/tocToggle.js" type="module"></script>

<script src="/js/layouts/toc.js" type="module"></script>

<script src="/js/plugins/tabs.js" type="module"></script>

    
</div>


</body>
</html>
